{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "    <center>\n",
    "    TMA4215 Project 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "#### 1. Introduction\n",
    "#### 2. Algorithm \n",
    "#### 3. Deciding parameters\n",
    "3.1 Number of hidden layers, $K$\n",
    "\n",
    "3.2 \n",
    "\n",
    "\n",
    "#### 3. Testing on suggested functions\n",
    "3.1 $F(y) = 1-\\cos(y)$\n",
    "\n",
    "3.2 $G(y) = \\frac{1}{2}(y_1^2 + y_2^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to use a neural network to train approximations of Hamiltonian function, derive and implement formulas for computing the gradient of the trained function and use those to implement symplectic Euler and the Størmer-Verlet method for the Hamiltonian function. \n",
    "\n",
    "This report present the algorithm, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import isclose\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from project_2_data_acquisition import generate_data, concatenate\n",
    "from files import writeParams, readParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above are imported from separate files and are used to generate input data batches from comma seperated files and to write our trained values to a file as well as reading these values from a file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is divided into several smaller codeblocks with the purpose of making the code easy to follow. Principally, the algorithm's parts is to transform input data between the layers in the network, decide the gradients of the objective function, $J = \\frac{1}{2} \\|Z-c\\| $, with respect to weights, biases, $\\omega$ and $\\mu$ and optimalize the network with respect to those. Additionally, several utility functions is defined to be used in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getW(K,d):\n",
    "    w = np.random.randn(K,d,d)\n",
    "    return w\n",
    "\n",
    "def getb(K,d):\n",
    "    b = np.random.randn(K,d,1)\n",
    "    return b\n",
    "\n",
    "def getomega(d):\n",
    "    omega = np.random.randn(d,1)\n",
    "    return omega\n",
    "\n",
    "def getmu():\n",
    "    mu = np.random.randn(1)\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above generate random initial values drawn from a standard normal distribution to the weights, $W_k$, the biases, $b_k$, $\\omega$ and $\\mu$. These random values causes some marginal differences between runs of the same dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getZ(Y0, W, b, K, d, I, h, sigma):\n",
    "    #initialize Z, Z0=Y0\n",
    "    Z = np.zeros((K+1,d,I))\n",
    "    Z[0] = Y0\n",
    "\n",
    "    #finds Zk\n",
    "    for k in range(1,K+1):\n",
    "        u = W[k-1]@Z[k-1] + b[k-1]\n",
    "        Z[k] = Z[k-1] + h*sigma(u)\n",
    "\n",
    "    return Z\n",
    "\n",
    "def getP(Z, ypsilon, c, omega, mu, W, b, K, d, I, h, sigma_div, eta_div):\n",
    "    #initialize P\n",
    "    P = np.zeros((K+1,d,I))\n",
    "\n",
    "    #finds P_K\n",
    "    u = np.transpose(Z[-1])@omega + mu\n",
    "    P[-1]= omega@np.transpose((ypsilon-c)*eta_div(u))\n",
    "\n",
    "    #finds P_K-1 to P_0\n",
    "    for k in range(K-1,0,-1):\n",
    "        s = W[k]@Z[k] + b[k]\n",
    "        P[k]=P[k+1] + h*np.transpose(W[k])@(sigma_div(s)*P[k+1])\n",
    "        \n",
    "    return P\n",
    "\n",
    "\n",
    "def getYpsilon(Z, omega, mu, K, eta):\n",
    "    u = np.transpose(Z[K]) @ omega + mu\n",
    "    return eta(u)\n",
    "\n",
    "def getJ(ypsilon, c):\n",
    "    return 1/2 * np.linalg.norm(ypsilon-c)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Defines the functions getZ and getP, where getZ executes linear transformations $\\Phi_k: Z_k \\rightarrow Z_{k+1}$ based on the weights and biases and returns the matrix $Z$ and getP returns an utility matrix used in calulations of derivatives of the objective function with respect to the weight and biases. Furthermore, getYpsilon is defined and returns a vector of the function values in the last layer in the network and the objective funksjon $J$ is defined as a measure of the difference between the resulting trained values and the exact function values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getdJdmu(Z, ypsilon, c, omega, mu, K, eta_div):\n",
    "    u = np.transpose(Z[K])@omega + mu\n",
    "    dJdmu = np.transpose(eta_div(u))@(ypsilon-c)\n",
    "    return dJdmu\n",
    "\n",
    "def getdJdomega(Z, ypsilon, c, omega, mu, K, eta_div):\n",
    "    u = np.transpose(Z[K])@omega + mu\n",
    "    dJdOmega = Z[K] @ ((ypsilon - c) * eta_div(u))\n",
    "    return dJdOmega\n",
    "\n",
    "def getdJdW(P, Z, W, b, K, h, sigma_div):\n",
    "    dJdW = np.zeros_like(W)\n",
    "    for k in range(K):\n",
    "        u = W[k]@Z[k]+b[k]\n",
    "        dJdW[k] = h*(P[k+1]*sigma_div(u))@ np.transpose(Z[k])  \n",
    "    return dJdW\n",
    "\n",
    "def getdJdb(P, Z, W, b, K, h, sigma_div):\n",
    "    dJdb = np.zeros_like(b)\n",
    "    for k in range(K):\n",
    "        u = W[k]@Z[k]+b[k]\n",
    "        dJdb[k] = h*(P[k+1]*sigma_div(u))@np.ones((Z.shape[2],1))\n",
    "    return dJdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Functions to obtain the derivatives of the objective function with respect to weights and biased is defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMandV(theta):\n",
    "    return np.zeros_like(theta),np.zeros_like(theta)\n",
    "\n",
    "def plain(theta, tau, dJdtheta):\n",
    "    return theta - tau*dJdtheta\n",
    "\n",
    "def adam(theta,alpha,m,v,g,i):\n",
    "    #parameters\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 10E-8\n",
    "\n",
    "    #m,v\n",
    "    m = beta1*m + (1-beta1)*g\n",
    "    v = beta2*v + (1-beta2)*np.multiply(g,g)\n",
    "\n",
    "    m_hat = np.multiply(m,1/(1-beta1**i))\n",
    "    v_hat = np.multiply(v,1/(1-beta2**i))\n",
    "\n",
    "    #update\n",
    "    R = alpha*np.multiply(m_hat,1/(np.sqrt(v_hat)+epsilon))\n",
    "\n",
    "    return theta-R,m,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above is used to optimize the weights, biases, $\\omega$ and $\\mu$ in different ways, which will be compared later on in this report.\n",
    "\n",
    "In plain vanilla gradient descent, one follows the gradient in decreasing direction and the learning parameter $\\tau$ determine how far one shall follow it. \n",
    "\n",
    "In Adam descent method, one follows the gradient with different length depending on different parameters. The parameters $m$ and $v$ represent respectively a kind of mean value and squared mean value of the previous iterations gradients. The function getMandV initialize $m$ and $v$ to be either zer, the zerovector or the zeromatrix depending on inputdata's dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigma_div(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def eta(x):\n",
    "    return 1/2*(1+np.tanh(x/2))\n",
    "\n",
    "def eta_div(x):\n",
    "    return 1/(4*np.cosh(x/2)**2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the activating function $\\sigma$, which is used in the transformation $\\Phi_k: Z_k \\rightarrow Z_{k+1}$, and the function $\\eta$, used in projection from last layer on a scalar $z$, is defined. The derivatives of $\\sigma$ and $\\eta$ is also defined and will be used in calculations of the gradient of the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale(x, alpha=0.2, beta=0.8):\n",
    "    a = np.amin(x)\n",
    "    b = np.amax(x)\n",
    "    \n",
    "    x_tilde = 1/(b-a)*((b-x)*alpha + (x-a)*beta)\n",
    "    return x_tilde, a, b\n",
    "\n",
    "def inverseScale(x_tilde, a, b, alpha=0.2, beta=0.8):\n",
    "    return 1/(beta-alpha)*((x_tilde-alpha)*b + (beta-x_tilde)*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to scale data in terms of a min-max transformation that guaranteed that the data have all its components in the interval $[\\alpha, \\beta]$ is defined. This is desirable since some functions is acting component-wise on other functions....?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def successrate(ypsilon, c, tol):\n",
    "    sum = 0\n",
    "    for i in range(ypsilon.shape[0]):\n",
    "        if isclose(ypsilon[i][0], c[0][i], abs_tol = tol):\n",
    "            sum +=1\n",
    "    return sum/ypsilon.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si noe fornuftig her etterhver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainingAlgorithm(K, d, h, tau, Y0, c0, eta, sigma, eta_div, sigma_div, N, tol, chunksize, optimization):\n",
    "    #finds input shape\n",
    "    d0 = Y0.shape[0]\n",
    "    I0 = Y0.shape[1]\n",
    "    \n",
    "    #reshapes input to match dimension of layers\n",
    "    if d0 > d:\n",
    "        return \"d must be larger than d0\"\n",
    "    \n",
    "    if d0 < d:\n",
    "        zero = np.zeros((d-d0,I0))\n",
    "        Y0 = np.vstack((Y0,zero))\n",
    "    \n",
    "    #gets initial weigths\n",
    "    omega = getomega(d)\n",
    "    mu = getmu()\n",
    "    W = getW(K,d)\n",
    "    b = getb(K,d)\n",
    "    \n",
    "    c0 = np.transpose(c0)\n",
    "  \n",
    "    if optimization == \"adam\":\n",
    "        #initial m,v for adam descent\n",
    "        mmu,vmu = getMandV(mu)\n",
    "        momega,vomega =getMandV(omega)\n",
    "        mW,vW = getMandV(W)\n",
    "        mb,vb = getMandV(b)\n",
    "    \n",
    "    #initializes vector to store objective function values\n",
    "    J = np.zeros(N)\n",
    "    ypsilon = np.zeros_like(c0)\n",
    "    \n",
    "    for i in range(N):\n",
    "               \n",
    "        #stochastic gradient descent\n",
    "        if I0 == chunksize:\n",
    "            Y0_chunk = Y0\n",
    "            c_chunk = c0\n",
    "        elif I0 > chunksize:\n",
    "            s = np.random.randint(0,I0-chunksize)\n",
    "            Y0_chunk = Y0[:,s:(s+chunksize)]\n",
    "            c_chunk=c0[s:(s+chunksize),:]\n",
    "        else:\n",
    "            return \"chunksize must be smaller than I\"        \n",
    "        \n",
    "\n",
    "        #transformations between layers\n",
    "        Z = getZ(Y0_chunk, W, b, K, d, chunksize, h, sigma)\n",
    "        ypsilon = getYpsilon(Z, omega, mu, K, eta)\n",
    "        P = getP(Z, ypsilon, c_chunk, omega, mu, W, b, K, d, chunksize, h, sigma_div, eta_div)\n",
    "        \n",
    "        #objective function\n",
    "        J[i] = getJ(ypsilon, c_chunk)\n",
    "        \n",
    "        #finds gradients\n",
    "        dJdmu = getdJdmu(Z, ypsilon, c_chunk, omega, mu, K, eta_div)\n",
    "        dJdomega = getdJdomega(Z, ypsilon, c_chunk, omega, mu, K, eta_div)\n",
    "        dJdW = getdJdW(P, Z, W, b, K, h, sigma_div)\n",
    "        dJdb = getdJdb(P, Z, W, b, K, h, sigma_div)\n",
    "        \n",
    "        if optimization == \"plain\":\n",
    "            mu = plain(mu, tau, dJdmu)\n",
    "            omega = plain(omega, tau, dJdomega)\n",
    "            W = plain(W, tau, dJdW)\n",
    "            b = plain(b, tau, dJdb)\n",
    "        \n",
    "        elif optimization == \"adam\":\n",
    "            mu, mmu,vmu = adam(mu,tau,mmu,vmu,dJdmu,i+1)\n",
    "            omega, momega,vomega = adam(omega,tau,momega,vomega,dJdomega,i+1)\n",
    "            W, mW,vW = adam(W,tau,mW,vW,dJdW,i+1)\n",
    "            b, mb,vb = adam(b,tau,mb,vb,dJdb,i+1)\n",
    "        \n",
    "        if (J[i]/chunksize) < tol:\n",
    "            break\n",
    "        \n",
    "    return mu, omega, W, b, J, ypsilon, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testing(yTest, cTest, W, b, omega, mu, K, d, I, h, sigma, eta):\n",
    "\n",
    "    #scaling of input\n",
    "    yTilde, aY, bY = scale(yTest)\n",
    "    cTilde, ac, bc = scale(cTest)\n",
    "\n",
    "    #finds input shape\n",
    "    d0 = yTest.shape[0]\n",
    "    \n",
    "    if d > d0:\n",
    "        zero = np.zeros((d-d0,I))\n",
    "        yTilde = np.vstack((yTilde,zero))\n",
    "    \n",
    "    zTest = getZ(yTilde, W, b, K, d, I, h, sigma)\n",
    "    ypsilonTilde = getYpsilon(zTest, omega, mu, K, eta)\n",
    "\n",
    "    #rescaling of input\n",
    "    ypsilonTest = inverseScale(ypsilonTilde, ac, bc)\n",
    "\n",
    "    return ypsilonTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training algorithm returns trained values of the weights, biases, $\\omega$ and $\\mu$ as well as the objective function and $\\Upsilon$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deciding parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 2500\n",
    "tol = 1E-10 \n",
    "\n",
    "batch0 = generate_data(0)\n",
    "\n",
    "p0_tilde, ap0,bp0 = scale(batch0['P'])\n",
    "T0_tilde, aT0, bT0 = scale(batch0['T'])\n",
    "T0_tilde.resize(1,T0_tilde.shape[0])\n",
    "I_0 = p0_tilde.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, optimal choices for the parameters $K$, $\\tau$, $d$ and $h$ will be investigated. All the following tests are based on that first batch of values, enclosed to the project description, for the unknown Hamiltonian function. Furthermore, the number for training series is chosen to be 2500 in all of the cases, and the tolerance is for that reason very small for so that one can guarantee that the training algorithm does not obtain a tolerated value of the objective function within the 2500 training series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.0 Batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variables for this test\n",
    "batchsize_list = np.arange(I_0/4, I_0+1,I_0/4).astype(int) \n",
    "d_0 = p0_tilde.shape[0]    \n",
    "h = 0.1     \n",
    "tau = 0.0001  \n",
    "k = 50\n",
    "\n",
    "#define lists of filenames where the trained parameters will be stored \n",
    "filenames_batchsize_A = []\n",
    "filenames_batchsize_P = []\n",
    "for i in range(len(batchsize_list)):\n",
    "    batchsize = batchsize_list[i]\n",
    "    filenames_batchsize_A.append('trainingParams_Abatch{}'.format(batchsize))\n",
    "    filenames_batchsize_P.append('trainingParams_Pbatch{}'.format(batchsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(batchsize_list)):\n",
    "    batchsize = batchsize_list[i]\n",
    "    #train for different batchsizes with adam descent \n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_Abatch, ypsilon_A0, itr_A0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Abatch, itr_A0, filename = filenames_batchsize_A[i])\n",
    "    \n",
    "    #train for different batchsizes with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_Pbatch, ypsilon_P0, itr_P0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Pbatch, itr_P0, filename = filenames_batchsize_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "#plot adam\n",
    "ax = axs[0]\n",
    "for i in range(len(batchsize_list)):\n",
    "    batchsize = batchsize_list[i]\n",
    "    W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_A0, itr_A0 = readParams(K, d_0, batchsize, N, filename = filenames_batchsize_A[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_Abatch/batchsize, label = r\"$batchsize ={}$\".format(batchsize))\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "#plot plain\n",
    "ax = axs[1]\n",
    "for i in range(len(batchsize_list)):\n",
    "    batchsize = batchsize_list[i]\n",
    "    W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_P0, itr_P0 = readParams(K, d_0, batchsize, N, filename = filenames_batchsize_P[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_Pbatch/batchsize, label = r\"$batchsize ={}$\".format(batchsize))\n",
    "ax.set_title(r\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can observe from the plots, the value of the objective function will not converge but oscillate around the convergent value for batchsizes smaller than the size of the batch that the parameters are trained by. This implies that the use of a smaller batchsize in further testing will make the convergence plot oscillating.............. nah\n",
    "\n",
    "In investigation of the parameters $K$, $\\tau$, $d$ and $h$ we will use the whole batchsize as input values, since this will give a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OLD BELOW"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batchsize_list = np.arange(I_0/4, I_0+1,I_0/4).astype(int) \n",
    "d_0 = p0_tilde.shape[0]      #dimension of layers for F(y)\n",
    "h = 0.1      #stepsize\n",
    "tau = 0.0001   #learning parameter\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "ax = axs[0]\n",
    "#Adam\n",
    "for i in range(len(batchsize_list)):\n",
    "    #training with different K with adam\n",
    "    batchsize = batchsize_list[i]\n",
    "    mu_0, omega_0, W_0, b_0, J_batch, ypsilon_0, itr_0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_batch/batchsize, label = r\"$batchsize ={}$\".format(batchsize))\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "#Plain\n",
    "for i in range(len(batchsize_list)):\n",
    "    #training with different K with plain\n",
    "    batchsize = batchsize_list[i]\n",
    "    mu_0, omega_0, W_0, b_0, J_batch_plain, ypsilon_0, itr_0  = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N,tol, batchsize, \"plain\")\n",
    "    ax.plot(np.linspace(0,N,N), J_batch_plain/batchsize, label = r\"$batchsize ={}$\".format(batchsize))\n",
    "ax.set_title(r\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Number of hidden layers, $K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variables for this test\n",
    "batchsize = I_0\n",
    "d_0 = p0_tilde.shape[0]      \n",
    "h = 0.1      \n",
    "tau = 0.0001   \n",
    "K_list = np.arange(25,101,25) \n",
    "\n",
    "#define lists of filenames where the trained parameters will be stored\n",
    "filenames_K_A = []\n",
    "filenames_K_P = []\n",
    "for i in range(len(K_list)):\n",
    "    K = K_list[i]\n",
    "    filenames_K_A.append('trainingParams_AK_{}'.format(K))\n",
    "    filenames_K_P.append('trainingParams_PK_{}'.format(K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(K_list)):\n",
    "    K = K_list[i]\n",
    "    #train for different K values with adam descent\n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_AK, ypsilon_A0, itr_A0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_AK, itr_A0, filename = filenames_K_A[i])\n",
    "    \n",
    "    #train for different K values with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_PK, ypsilon_P0, itr_P0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_PK, itr_P0, filename = filenames_K_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "#plot adam\n",
    "ax = axs[0]\n",
    "for i in range(len(K_list)):\n",
    "    K = K_list[i]\n",
    "    W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_AK, itr_A0 = readParams(K, d_0, batchsize, N, filename = filenames_K_A[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_AK/batchsize, label = r\"$K ={}$\".format(K))\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "#plot plain\n",
    "ax = axs[1]\n",
    "for i in range(len(K_list)):\n",
    "    K = K_list[i]\n",
    "    W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_PK, itr_P0 = readParams(K, d_0, batchsize, N, filename = filenames_K_P[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_PK/batchsize, label = r\"$K ={}$\".format(K))\n",
    "ax.set_title(r\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old below "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "K_list = np.arange(25,101,25)      #number of hidden layers\n",
    "d_0 = p0_tilde.shape[0]      #dimension of layers for F(y)\n",
    "h = 0.1      #stepsize\n",
    "tau = 0.001   #learning parameter\n",
    "batchsize = I_0\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "ax = axs[0]\n",
    "#Adam\n",
    "for i in range(len(K_list)):\n",
    "    #training with different K with adam\n",
    "    K = K_list[i]\n",
    "    mu_0, omega_0, W_0, b_0, J_K, ypsilon_0, itr_0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol,batchsize, \"adam\")\n",
    "\n",
    "    ax.plot(np.linspace(0,N,N), J_K/batchsize, label = r\"$K ={}$\".format(K))\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "#Plain\n",
    "for i in range(len(K_list)):\n",
    "    #training with different K with plain\n",
    "    K = K_list[i]\n",
    "    mu_0, omega_0, W_0, b_0, J_K_plain, ypsilon_0, itr_0  = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N,tol, batchsize, \"plain\")\n",
    "    ax.plot(np.linspace(0,N,N), J_K_plain/batchsize, label = r\"$K ={}$\".format(K))\n",
    "ax.set_title(r\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Y0_F = np.linspace(-2,2,I)\n",
    "Y0_F.resize(1,I)\n",
    "c_F = F(Y0_F)\n",
    "\n",
    "Y0_F_tilde, aY0_F, bY0_F = scale(Y0_F)\n",
    "c_F_tilde, ac_F, bc_F = scale(c_F)\n",
    "\n",
    "Y0_G = np.random.uniform(-2,2,(2,I))\n",
    "c_G = G(Y0_G[0],Y0_G[1])\n",
    "c_G.resize(1,I)\n",
    "\n",
    "Y0_G_tilde, aY0_G, bY0_G = scale(Y0_G)\n",
    "c_G_tilde, ac_G, bc_G = scale(c_G)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "K_list = np.arange(25,101,25)      #number of hidden layers\n",
    "d_F = 2      #dimension of layers for F(y)\n",
    "h = 0.1      #stepsize\n",
    "tau = 0.001   #learning parameter\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "ax = axs[0]\n",
    "#Adam\n",
    "for i in range(len(K_list)):\n",
    "    #training with different K with adam\n",
    "    K = K_list[i]\n",
    "    mu_F, omega_F, W_F, b_F, J_K, ypsilon, itr = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I, \"adam\")\n",
    "                                                            \n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_K/I, label = r\"$K ={}$\".format(K))\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "#Plain\n",
    "for i in range(len(K_list)):\n",
    "    #training with different K with plain\n",
    "    K = K_list[i]\n",
    "    mu_F, omega_F, W_F, b_F, J_K_plain, ypsilon, itr = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I, \"plain\")\n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_K_plain/I, label = r\"$K ={}$\".format(K))\n",
    "ax.set_title(r\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Value of the learning parameter $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variables for this test\n",
    "batchsize = I_0\n",
    "d_0 = p0_tilde.shape[0]      #dimension of layers for F(y)\n",
    "h = 0.1      #stepsize\n",
    "tau_list = np.linspace(0.001, 0.01, 4)   #learning parameter\n",
    "K = 50 \n",
    "\n",
    "#define lists of filenames where the trained parameters will be stored\n",
    "filenames_tau_A = []\n",
    "filenames_tau_P = []\n",
    "for i in range(len(tau_list)):\n",
    "    tau = tau_list[i]\n",
    "    filenames_tau_A.append('trainingParams_Atau_{}'.format(tau))\n",
    "    filenames_tau_P.append('trainingParams_Ptau_{}'.format(tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(tau_list)):\n",
    "    tau = tau_list[i]\n",
    "\n",
    "    #train for different tau values with adam descent\n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_Atau, ypsilon_A0, itr_A0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Atau, itr_A0, filename = filenames_tau_A[i])\n",
    "    \n",
    "    #train for different tau values with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_Ptau, ypsilon_P0, itr_P0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Ptau, itr_P0, filename = filenames_tau_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "#plot adam\n",
    "ax = axs[0]\n",
    "for i in range(len(tau_list)):\n",
    "    tau = tau_list[i]\n",
    "    W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Atau, itr_A0 = readParams(K, d_0, batchsize, N, filename = filenames_tau_A[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_Atau/batchsize, label = r\"$\\tau =%.4f$\"%tau)\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "#plot plain\n",
    "ax = axs[1]\n",
    "for i in range(len(tau_list)):\n",
    "    tau = tau_list[i]\n",
    "    W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Ptau, itr_P0 = readParams(K, d_0, batchsize, N, filename = filenames_tau_P[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_Ptau/batchsize, label = r\"$\\tau =%.4f$\"%tau)\n",
    "ax.set_title(r\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OLD BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K = 50      #number of hidden layers\n",
    "d_0 = p0_tilde.shape[0]      #dimension of layers for F(y)\n",
    "h = 0.1      \n",
    "tau_list = np.linspace(0.001, 0.01, 4)   #learning parameter\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "ax = axs[0]\n",
    "#Adam\n",
    "for i in range(len(tau_list)):\n",
    "    #training with different tau with adam\n",
    "    tau = tau_list[i]\n",
    "    mu_0, omega_0, W_0, b_0, J_tau, ypsilon_0, itr_0  = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N,tol, batchsize, \"adam\")\n",
    "\n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_tau/I_0, label = r\"$\\tau =%.4f$\"%tau)\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "\n",
    "ax = axs[1]\n",
    "#Plain\n",
    "for i in range(len(tau_list)):\n",
    "    #training with different tau with plain\n",
    "    tau = tau_list[i]\n",
    "    mu_0, omega_0, W_0, b_0, J_tau_plain, ypsilon_0, itr_0  = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N,tol, batchsize, \"plain\")\n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_tau_plain/I, label = r\"$\\tau =%.4f$\"%tau)\n",
    "ax.set_title(\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "K = 50      #number of hidden layers\n",
    "d_F = 2      #dimension of layers for F(y)\n",
    "h = 0.1      #stepsize\n",
    "tau_list = np.linspace(0.001, 0.01, 4)   #learning parameter\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "ax = axs[0]\n",
    "#Adam\n",
    "for i in range(len(tau_list)):\n",
    "    #training with different tau with adam\n",
    "    tau = tau_list[i]\n",
    "    mu_F, omega_F, W_F, b_F, J_tau, ypsilon, itr = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I, \"adam\")\n",
    "\n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_tau/I, label = r\"$\\tau =%.4f$\"%tau)\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "\n",
    "ax = axs[1]\n",
    "#Plain\n",
    "for i in range(len(tau_list)):\n",
    "    #training with different tau with plain\n",
    "    tau = tau_list[i]\n",
    "    mu_F, omega_F, W_F, b_F, J_tau_plain, yspilon, itr = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I, \"plain\")\n",
    "    \n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_tau_plain/I, label = r\"$\\tau =%.4f$\"%tau)\n",
    "ax.set_title(\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 The dimension of the layers $d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defines variable for this test\n",
    "batchsize = I_0\n",
    "d_0_list = np.array([1,2,3,4])  \n",
    "h = 0.1      \n",
    "tau = 0.001 \n",
    "K = 50 \n",
    "\n",
    "#define lists of filenames where the trained parameters will be stored\n",
    "filenames_d_A = []\n",
    "filenames_d_P = []\n",
    "for i in range(len(d_0_list)):\n",
    "    d_0 = d_0_list[i]\n",
    "    filenames_d_A.append('trainingParams_Ad_{}'.format(d_0))\n",
    "    filenames_d_P.append('trainingParams_Pd_{}'.format(d_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(d_0_list)):\n",
    "    d_0 = d_0_list[i]\n",
    "    \n",
    "    #train for different values of d with adam descent\n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_Ad0, ypsilon_A0, itr_A0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Ad0, itr_A0, filename = filenames_d_A[i])\n",
    "    \n",
    "    #train for different values of d with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_Pd0, ypsilon_P0, itr_P0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Pd0, itr_P0, filename = filenames_d_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "#plot adam\n",
    "ax = axs[0]\n",
    "for i in range(len(d_0_list)):\n",
    "    d_0 = d_0_list[i]\n",
    "    W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Ad, itr_A0 = readParams(K, d_0, batchsize, N, filename = filename_d_A[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_Ad/batchsize, label = r\"$\\d ={}$\".format(d))\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "#plot plain\n",
    "ax = axs[1]\n",
    "for i in range(len(d_0_list)):\n",
    "    d_0 = d_0_list[i]\n",
    "    W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Pd, itr_P0 = readParams(K, d_0, batchsize, N, filename = filename_d_P[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_Pd/batchsize, label = r\"$\\d ={}$\".format(d))\n",
    "ax.set_title(r\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OLD BELOW"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "K = 50      #number of hidden layers\n",
    "d0_list = np.array([1,2,3,4])      #dimension of layers for F(y)\n",
    "h = 0.1      #stepsize\n",
    "tau = 0.001   #learning parameter\n",
    "\n",
    "\n",
    "for i in range(len(d0_list)):\n",
    "    #training with different d_F with adam\n",
    "    d0 = d0_list[i]\n",
    "    mu_0, omega_0, W_0, b_0, J_d0, ypsilon_0, itr_0  = trainingAlgorithm(K, d0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N,tol, batchsize, \"adam\")\n",
    "    plt.plot(np.linspace(0,N,N), J_d0/I, label = r\"$d ={}$\".format(d_F))\n",
    "    \n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "K = 50      #number of hidden layers\n",
    "d_F_list = np.array([1,2,3,4])      #dimension of layers for F(y)\n",
    "h = 0.1      #stepsize\n",
    "tau = 0.001   #learning parameter\n",
    "\n",
    "\n",
    "for i in range(len(d_F_list)):\n",
    "    #training with different d_F with adam\n",
    "    d_F = d_F_list[i]\n",
    "    mu_F, omega_F, W_F, b_F, J_dF, ypsilon, itr = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I, \"adam\")\n",
    "\n",
    "    plt.plot(np.linspace(0,N,N), J_dF/I, label = r\"$d ={}$\".format(d_F))\n",
    "    \n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "K = 50      #number of hidden layers\n",
    "d_G_list = np.array([2,4,6,8])      #dimension of layers for G(y)\n",
    "h = 0.1      #stepsize\n",
    "tau = 0.001   #learning parameter\n",
    "\n",
    "for i in range(len(d_G_list)):\n",
    "    #training with different d_G with adam\n",
    "    d_G = d_G_list[i]\n",
    "    mu_G, omega_G, W_G, b_G, J_dG, ypsilon, itr = trainingAlgorithm(K, d_G, h, tau, Y0_G_tilde, c_G_tilde, eta, sigma, eta_div, sigma_div, N, tol, I, \"adam\")\n",
    "      \n",
    "    plt.plot(np.linspace(0,N,N), J_dG/I, label = r\"$d ={}$\".format(d_G))\n",
    "    \n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 The stepsize $h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variables for this test\n",
    "batchsize = I_0\n",
    "d_0 = p0_tilde.shape[0]\n",
    "h_list = np.array([0.05, 0.1, 0.15, 0.2])\n",
    "tau = 0.001 \n",
    "K = 50 \n",
    "\n",
    "#define lists of filenames where the trained parameters will be stored\n",
    "filenames_h_A = []\n",
    "filenames_h_P = []\n",
    "for i in range(len(h_list)):\n",
    "    h = h_list[i]\n",
    "    filenames_h_A.append('trainingParams_Ah_{}'.format(h))\n",
    "    filenames_h_P.append('trainingParams_Ph_{}'.format(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(h_list)):\n",
    "    h = h_list[i]\n",
    "    #train for different stepsizes h with adam descent\n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_Ah, ypsilon_A0, itr_A0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Ah, itr_A0, filename = filenames_h_A[i])\n",
    "    \n",
    "    #train for different stepsizes h with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_Ph, ypsilon_P0, itr_P0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Ph, itr_P0, filename = filenames_h_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "ax = axs[0]\n",
    "\n",
    "#plot adam\n",
    "for i in range(len(h_list)):\n",
    "    h = h_list[i]\n",
    "    W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Ah, itr_A0 = readParams(K, d_0, batchsize, N, filename = filenames_h_A[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_Ah/batchsize, label = r\"$\\h ={}$\".format(h))\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "#plot plain\n",
    "for i in range(len(h_list)):\n",
    "    h = h_list[i]\n",
    "    W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Ph, itr_P0 = readParams(K, d_0, batchsize, N, filename = filenames_h_P[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_Ph/batchsize, label = r\"$\\h ={}$\".format(h))\n",
    "ax.set_title(r\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OLD BELOW"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "K = 50      #number of hidden layers\n",
    "d0 = p0_tilde.shape[0]      #dimension of layers for F(y)\n",
    "h_list = np.array([0.05, 0.1, 0.15, 0.2])      #stepsize\n",
    "tau = 0.001   #learning parameter\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "ax = axs[0]\n",
    "#Adam\n",
    "for i in range(len(h_list)):\n",
    "    #training with different h with adam\n",
    "    h = h_list[i]\n",
    "    mu_0, omega_0, W_0, b_0, J_h, ypsilon_0, itr_0  = trainingAlgorithm(K, d0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N,tol, batchsize, \"adam\")\n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_h/I, label = r\"$h ={}$\".format(h))\n",
    "\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "ax = axs[1]\n",
    "#Plain\n",
    "for i in range(len(h_list)):\n",
    "    #training with different h with plain\n",
    "    h = h_list[i]\n",
    "    mu_0, omega_0, W_0, b_0, J_h_plain, ypsilon_0, itr_0  = trainingAlgorithm(K, d0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N,tol, batchsize, \"plain\")\n",
    "\n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_h_plain/I, label = r\"$h ={}$\".format(h))\n",
    "\n",
    "ax.set_title(\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "K = 50      #number of hidden layers\n",
    "d_F = 2      #dimension of layers for F(y)\n",
    "h_list = np.array([0.05, 0.1, 0.15, 0.2])      #stepsize\n",
    "tau = 0.001   #learning parameter\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "ax = axs[0]\n",
    "#Adam\n",
    "for i in range(len(h_list)):\n",
    "    #training with different h with adam\n",
    "    h = h_list[i]\n",
    "    mu_F, omega_F, W_F, b_F, J_h, ypsilon, itr = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I, \"adam\")\n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_h/I, label = r\"$h ={}$\".format(h))\n",
    "\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "ax = axs[1]\n",
    "#Plain\n",
    "for i in range(len(h_list)):\n",
    "    #training with different h with plain\n",
    "    h = h_list[i]\n",
    "    mu_F, omega_F, W_F, b_F, J_h_plain, ypsilon, itr = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I, \"plain\")\n",
    "\n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_h_plain/I, label = r\"$h ={}$\".format(h))\n",
    "\n",
    "ax.set_title(\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Testing with suggested functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defines global variables\n",
    "h = 0.1       #stepsize\n",
    "tau = 0.0001   #learning parameter\n",
    "tol = 1e-5  #tolerance\n",
    "N = 5000      #number of training series\n",
    "K = 50        #number of hidden layers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TAU VAR NÅ VELDIG LITEN? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1  F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F(y):\n",
    "    return 1-np.cos(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_F = 4        #dimension of layers for F(y)\n",
    "I_F = 1000     #input size \n",
    "\n",
    "#training with F(y)\n",
    "Y0_F = np.random.uniform(-2,2,(1,I_F))\n",
    "c_F = F(Y0_F)\n",
    "\n",
    "Y0_F_tilde, aY0_F, bY0_F = scale(Y0_F)\n",
    "c_F_tilde, ac_F, bc_F = scale(c_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_F, omega_F, W_F, b_F, J_F, ypsilon_F, itr_F = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_F, \"adam\")\n",
    "#writeParams(W_F, b_F, omega_F, mu_F,ypsilon_F, J_F, itr_F, filename = 'trainingParams_F.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_Fr,b_Fr,omega_Fr,mu_Fr,ypsilon_Fr, J_F, itr_F = readParams(K, d_F, I_F, N, filename='trainingParams_F.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,itr_F,itr_F), J_F[:itr_F]/I_F)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing with F(y)\n",
    "YTest_F = np.random.uniform(-2,2,(1,I_F))\n",
    "cTest_F = F(YTest_F)\n",
    "\n",
    "\n",
    "ypsilonTest_F = testing(YTest_F, cTest_F, W_F, b_F, omega_F, mu_F, K, d_F, I_F, h, sigma, eta)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(YTest_F[0], ypsilonTest_F, \"*\", label = \"test\")\n",
    "plt.plot(YTest_F[0], cTest_F[0], \"*\", label = \"fasit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def G(y1, y2):\n",
    "    return 1/2 *(y1**2 + y2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_G = 4        #dimension of layers for F(y)\n",
    "I_G = 1000     #input size \n",
    "\n",
    "#training with G(y)\n",
    "Y0_G = np.random.uniform(-2,2,(2,I_G))\n",
    "c_G = G(Y0_G[0],Y0_G[1])\n",
    "c_G.resize(1,I_G)\n",
    "\n",
    "Y0_G_tilde, aY0_G, bY0_G = scale(Y0_G)\n",
    "c_G_tilde, ac_G, bc_G = scale(c_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_G, omega_G, W_G, b_G, J_G, ypsilon_G, itr_G = trainingAlgorithm(K, d_G, h, tau, Y0_G_tilde, c_G_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_G, \"adam\")\n",
    "writeParams(W_G, b_G, omega_G, mu_G, ypsilon_G, J_G, itr_G, filename = 'trainingParams_G.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_Gr,b_Gr,omega_Gr,mu_Gr, ypsilon_Gr, J_G, itr_G = readParams(K, d_G, I_G, N, filename='trainingParams_G.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plotting of objective function\n",
    "plt.plot(np.linspace(0,itr_G,itr_G), J_G[:itr_G]/I_G)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing with G(y)\n",
    "YTest_G = np.random.uniform(-2,2,(2,I_G))\n",
    "cTest_G = G(YTest_G[0], YTest_G[1])\n",
    "cTest_G.resize(1,I_G)\n",
    "\n",
    "ypsilonTest_G = testing(YTest_G, cTest_G, W_G, b_G, omega_G, mu_G, K, d_G, I_G, h, sigma, eta)\n",
    "\n",
    "\n",
    "#plotting of G(y)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(YTest_G[0], YTest_G[1], ypsilonTest_G, label = \"test\", depthshade = True)\n",
    "ax.scatter(YTest_G[0], YTest_G[1], cTest_G[0], label = \"test\" , c=\"red\", depthshade = False)\n",
    "plt.show()\n",
    "\n",
    "x = np.arange(0, I_G)\n",
    "plt.plot(x, ypsilonTest_G, label =\"test\")\n",
    "plt.plot(x, cTest_G[0], label = \"fasit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4. Training with data given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_0 = 3\n",
    "N_0 = 3000\n",
    "K_0= 50\n",
    "\n",
    "\n",
    "batch0 = generate_data()\n",
    "\n",
    "p0_tilde, ap0,bp0 = scale(batch0['P'])\n",
    "T0_tilde, aT0, bT0 = scale(batch0['T'])\n",
    "T0_tilde.resize(1,T0_tilde.shape[0])\n",
    "\n",
    "I_0 = p0_tilde.shape[1]\n",
    "\n",
    "mu_0, omega_0, W_0, b_0, J_0, ypsilon_0, J_0 = trainingAlgorithm(K_0, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N_0,500, \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writeParams(W_0, b_0, omega_0, mu_0, ypsilon_0, filename = 'trainingParams_0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_0r,b_0r,omega_0r,mu_0r,ypsilon_0r = readParams(K_0, d_0, I_0, filename='trainingParams_0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plotting of objective function\n",
    "plt.plot(np.linspace(0,N_0,N_0), J_0)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(0, I_0)\n",
    "ypsilon0_scaled = inverseScale(ypsilon_0, aT0, bT0)\n",
    "T0 = batch0['T']\n",
    "\n",
    "print(successrate(ypsilon0_scaled, T0, 0.0035))\n",
    "\n",
    "plt.plot(x, ypsilon0_scaled, label =\"test\")\n",
    "plt.plot(x, T0[0], label = \"fasit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing with batch 1\n",
    "\n",
    "batch1 = generate_data(1)\n",
    "\n",
    "p1 = batch1['P']\n",
    "T1 =batch1['T']\n",
    "T1.resize(1,T1.shape[0])\n",
    "\n",
    "\n",
    "ypsilonTest_1 = testing(p1, T1, W_0, b_0, omega_0, mu_0, K_0, d_0, I_0, h, sigma, eta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(0, I_0)\n",
    "plt.plot(x, ypsilonTest_1, label =\"test\")\n",
    "plt.plot(x, T1[0], label = \"fasit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingBatch = concatenate(batchmin=0, batchmax=25)\n",
    "I_T = trainingBatch[\"P\"].shape[1]\n",
    "d_T = trainingBatch[\"P\"].shape[0]\n",
    "batchsize = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pT_tilde, apT, bpT = scale(trainingBatch[\"P\"])\n",
    "TT_tilde, aTT, bTT = scale(trainingBatch[\"T\"])\n",
    "TT_tilde.resize(1,TT_tilde.shape[0])\n",
    "\n",
    "mu_T, omega_T, W_T, b_T, J_T, ypsilon_T, itr_T = trainingAlgorithm(K, d_T, h, tau, pT_tilde, TT_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_T,b_T,omega_T,mu_T,ypsilon_T, J_T, itr_T = readParams(K, d_T, batchsize, N, filename='trainingParams_T.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,itr_T,itr_T), J_T[:itr_T]/batchsize)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### larger batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingBatch = concatenate(0,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dT = trainingBatch['P'].shape[0]\n",
    "IT = trainingBatch['P'].shape[1]\n",
    "batchsize = 50000\n",
    "\n",
    "pT_tilde = trainingBatch['P']\n",
    "TT_tilde, aTT, bTT = scale(trainingBatch['T'])\n",
    "TT_tilde.resize(1,TT_tilde.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_T, omega_T, W_T, b_T, J_T, ypsilon_T, itr_T = trainingAlgorithm(K, dT, h, tau, pT_tilde, TT_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "writeParams(W_T, b_T, omega_T, mu_T, ypsilon_T, J_T, itr_T, filename = 'trainingParams_T.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_T, b_T, omega_T, mu_T, ypsilon_T, J_T, itr_T = readParams(K, dT, batchsize, N, filename='trainingParams_T.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting of objective function\n",
    "plt.plot(np.linspace(0,itr_T,itr_T), J_T[:itr_T]/batchsize)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Computing the gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getZ_oneData(y,W,b,K,h,sigma):\n",
    "    d = y.shape[0]\n",
    "    I = y.shape[1]\n",
    "    Z = np.zeros((K,d,I))\n",
    "    Z[0] = y\n",
    "    \n",
    "    for k in range(1,K):\n",
    "        u = W[k-1]@Z[k-1] + b[k-1]\n",
    "        Z[k] = Z[k-1] + h*sigma(u)\n",
    "\n",
    "    return Z\n",
    "\n",
    "def gradF(y, omega, mu, W, b, K, sigma, eta_div, sigma_div):\n",
    "    Z = getZ_oneData(y, W, b, K, h, sigma)\n",
    "    A = eta_div(np.transpose(Z[K-1])@omega + mu)*omega \n",
    "    for k in range(K-1,0,-1): \n",
    "        B = h*sigma_div(W[k]@y+b[k])\n",
    "        u = B*A\n",
    "        A = A + W[k]@u\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Numerical methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def symplecticEuler(gradF, t, q0, p0, omega_V, mu_V, W_V, b_V, omega_T, mu_T, W_T, b_T, K, sigma, eta_div, sigma_div): \n",
    "\n",
    "    q = np.zeros((len(t), q0.shape[0], q0.shape[1]))\n",
    "    p = np.zeros((len(t), p0.shape[0], p0.shape[1]))\n",
    "    \n",
    "    q[0] = q0\n",
    "    p[0] = p0\n",
    "    \n",
    "    for n in range(len(t)-1):\n",
    "        delta_t = t[n+1]-t[n] \n",
    "\n",
    "        q[n+1] = q[n] + delta_t*gradF(p[n], omega_T, mu_T, W_T, b_T, K, sigma, eta_div, sigma_div) #dTdp\n",
    "        p[n+1] = p[n] - delta_t*gradF(q[n+1], omega_V, mu_V, W_V, b_V, K, sigma, eta_div, sigma_div) #dVdq\n",
    "    \n",
    "    return q,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Stromer_Verlet(gradF, t, q0, p0, omega_V, mu_V, W_V, b_V, omega_T, mu_T, W_T, b_T, K, sigma, eta_div, sigma_div): \n",
    "    \n",
    "    q = np.zeros((len(t), q0.shape[0], q0.shape[1]))\n",
    "    p = np.zeros((len(t), p0.shape[0], p0.shape[1]))\n",
    "    \n",
    "    q[0] = q0\n",
    "    p[0] = p0\n",
    "    \n",
    "    for n in range(len(t)-1):\n",
    "        delta_t = t[n+1]-t[n] \n",
    "        u = p[n] - delta_t/2*gradF(q[n], omega_V, mu_V, W_V, b_V, K, sigma, eta_div, sigma_div) #u = p_{n+1/2}, dVdq\n",
    "        q[n+1] = q[n] + delta_t*gradF(u, omega_T, mu_T, W_T, b_T, K, sigma, eta_div, sigma_div) #dTdp\n",
    "        p[n+1] = u - delta_t/2*gradF(q[n+1], omega_V, mu_V, W_V, b_V, K, sigma, eta_div, sigma_div) #dVdq\n",
    "    \n",
    "    return q,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Nonlinear pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = 10E-3\n",
    "l = 0.5\n",
    "g = 9.81\n",
    "\n",
    "def T_pend(p):\n",
    "    return 0.5*p**2\n",
    "\n",
    "def V_pend(q):\n",
    "    return m*g*l*(1-np.cos(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I_p = 1000\n",
    "d_p = 2\n",
    "\n",
    "q_p = np.random.uniform(0,2*np.pi,(1,I_p))\n",
    "V_p = V_pend(q_p)\n",
    "V_p_tilde, aVp, bVp = scale(V_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_Tpend,b_Tpend,omega_Tpend,mu_Tpend,ypsilon_Tpend, J_Tpend, itr_Tpend = readParams(K, d_p, I_p, N, filename='trainingParams_Tp.txt')\n",
    "W_Vpend,b_Vpend,omega_Vpend,mu_Vpend,ypsilon_Vpend, J_Vpend, itr_Vpend = readParams(K, d_p, I_p, N, filename='trainingParams_Vp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q0 = np.zeros((d_p,1))\n",
    "p0 = np.zeros((d_p,1))\n",
    "t = np.linspace(0,10,100)\n",
    "\n",
    "q_sympEuler, p_sympEuler = symplecticEuler(gradF, t, q0, p0, omega_Vpend, mu_Vpend, W_Vpend, b_Vpend, omega_Tpend, mu_Tpend, W_Tpend, b_Tpend, K, sigma, eta_div, sigma_div)\n",
    "\n",
    "q_StromerVerlet, p_StromerVerlet = Stromer_Verlet(gradF, t, q0, p0, omega_Vpend, mu_Vpend, W_Vpend, b_Vpend, omega_Tpend, mu_Tpend, W_Tpend, b_Tpend, K, sigma, eta_div, sigma_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_plot = np.append(np.flip(q_sympEuler[:,0,:]),q_sympEuler[:,1,:])\n",
    "\n",
    "q = np.linspace(-2,2,100)\n",
    "\n",
    "plt.plot(q, V_pend(q), label = \"fasit\")\n",
    "plt.plot(q_plot, V_pend(q_plot), \"--\", label = \"Symplectic Euler\")\n",
    "\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_plot = np.append(np.flip(q_StromerVerlet[:,0,:]),q_StromerVerlet[:,1,:])\n",
    "\n",
    "q = np.linspace(-2,2,100)\n",
    "\n",
    "plt.plot(q, V_pend(q), label = \"fasit\")\n",
    "plt.plot(q_plot, V_pend(q_plot), \"--\", label = \"Stromer Verlet\")\n",
    "\n",
    "\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
