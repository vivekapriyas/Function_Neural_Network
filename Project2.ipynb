{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "    <center>\n",
    "    TMA4215 Project 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "#### 1. Introduction\n",
    "#### 2. Algorithm \n",
    "\n",
    "2.1 Functions used in the algorithms\n",
    "\n",
    "2.2 The training algorithm \n",
    "\n",
    "2.3 The testing algorithm and functions for analysis\n",
    "\n",
    "#### 3. Deciding parameters\n",
    "\n",
    "3.1 Batchsize \n",
    "\n",
    "3.2 Number of hidden layers, $K$\n",
    "\n",
    "3.3 Learning parameter, $\\tau$\n",
    "\n",
    "3.4 Dimension of the layers $d$\n",
    "\n",
    "3.5 Stepsize $h$\n",
    "\n",
    "3.6 Conclusion\n",
    "\n",
    "\n",
    "#### 4. Training and testing with suggested functions\n",
    "4.1 $F(y) = 1-\\cos(y)$\n",
    "\n",
    "4.2 $G(y_1, y_2) = \\frac{1}{2}(y_1^2 + y_2^2)$\n",
    "\n",
    "4.3 Known Hamiltonians\n",
    "   \n",
    "    4.3.1 Nonlinar pendulum\n",
    "    \n",
    "    4.3.2 Kepler two-body problem\n",
    "\n",
    "4.4 Unknown Hamiltonian\n",
    "\n",
    "#### 5. Computing the gradient \n",
    "#### 6. Implementation of the numerical methods sympletic Euler and Størmer-Verlet\n",
    "6.1 Nonlinear pendelum\n",
    "\n",
    "6.2 Kepler two-body problem\n",
    "\n",
    "6.3 The unknown Hamiltonian function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to implement a  neural network for estimating a seperable Hamiltonian function $H(p,q) = T(p) + V(p)$ based on data. This is achieved by training the network for $T(p)$ and $V(q)$, computing the gradient of the trained function and use the gradient in the numerical methods symplectic Euler and Størmer-Verlet to approximate the trajectories.\n",
    "\n",
    "This report first presents the training algorithm and argues for the choices of the parameters $K$, $\\tau$, $d$ and $h$ used as well as the batchsize used in the stochastic gradient method. Then the model is trained for know functions and Hamiltonians and the results are compared to exact values. This is done to evaluate the accuracy of the network, before it is trained on data given from an unknown Hamiltonian function. Finally, the formulas for computing the gradient are presented and the numerical methods implemented and used, both on known Hamiltonians and the unknown function.\n",
    "\n",
    "NB: We have chosen to use the inputdata batches from the new trajectories. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import isclose\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from project_2_data_acquisition import generate_data, concatenate\n",
    "from files import writeParams, readParams, writeScale, readScale, plotObjFnc, plotTrajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above functions are imported from separate files and are used to generate the input data from the unknown Hamiltonian function, as well as save the values of a trained network to textfiles and reaccess them at a later time. Additionally functions that plot reoccuring graphs are imported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training algorithm is separated into several smaller codeblocks to simply its structure and make it more readable. It is mainly split into transforming the input data between layers, determining the gradient of the objective function with respect to the weights  and biases, $W$, $b$, $\\omega$ and $\\mu$, and optimising the network with respect to them. Additionally there are various smaller functions for use in the main functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Functions used in the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getW(K,d):\n",
    "    w = np.random.randn(K,d,d)\n",
    "    return w\n",
    "\n",
    "def getb(K,d):\n",
    "    b = np.random.randn(K,d,1)\n",
    "    return b\n",
    "\n",
    "def getomega(d):\n",
    "    omega = np.random.randn(d,1)\n",
    "    return omega\n",
    "\n",
    "def getmu():\n",
    "    mu = np.random.randn(1)\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above generate random initial values, drawn from a standard normal distribution, for the weights and biases, $W_k$, $b_k$, $\\omega$ and $\\mu$. These random values lead to some slight differences when training of the network multiple times for the same dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigma_div(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def eta(x):\n",
    "    return 1/2*(1+np.tanh(x/2))\n",
    "\n",
    "def eta_div(x):\n",
    "    return 1/(4*np.cosh(x/2)**2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the activating function $\\sigma$, which is used in the transformation $\\Phi_k: Z_k \\rightarrow Z_{k+1}$, and the hypothesis function $\\eta$, used in projection from last layer on a scalar $z$, as well as their derivatives, are defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getZ(Y0, W, b, K, d, I, h, sigma):\n",
    "    #initialize Z, Z0=Y0\n",
    "    Z = np.zeros((K+1,d,I))\n",
    "    Z[0] = Y0\n",
    "\n",
    "    #finds Zk\n",
    "    for k in range(1,K+1):\n",
    "        u = W[k-1]@Z[k-1] + b[k-1]\n",
    "        Z[k] = Z[k-1] + h*sigma(u)\n",
    "\n",
    "    return Z\n",
    "\n",
    "def getP(Z, ypsilon, c, omega, mu, W, b, K, d, I, h, sigma_div, eta_div):\n",
    "    #initialize P\n",
    "    P = np.zeros((K+1,d,I))\n",
    "\n",
    "    #finds P_K\n",
    "    u = np.transpose(Z[-1])@omega + mu\n",
    "    P[-1]= omega@np.transpose((ypsilon-c)*eta_div(u))\n",
    "\n",
    "    #finds P_K-1 to P_0\n",
    "    for k in range(K-1,0,-1):\n",
    "        s = W[k]@Z[k] + b[k]\n",
    "        P[k]=P[k+1] + h*np.transpose(W[k])@(sigma_div(s)*P[k+1])\n",
    "        \n",
    "    return P\n",
    "\n",
    "\n",
    "def getYpsilon(Z, omega, mu, K, eta):\n",
    "    u = np.transpose(Z[K]) @ omega + mu\n",
    "    return eta(u)\n",
    "\n",
    "def getJ(ypsilon, c):\n",
    "    return 1/2 * np.linalg.norm(ypsilon-c)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here the functions $getZ$, $getP$, $getYpsilon$ and $getJ$ are defined. $getZ$ executes the transfomation $\\Phi_k: Z_k \\rightarrow Z_{k+1}$ between layers, and $getP$ returns an utility matrix used in calulations of derivatives of the objective function with respect to the weight and biases.\n",
    "$getYpsilon$ returns a vector of the approximated function values and $getJ$ returns the objective function which is a measure of the difference between the approximate and exact function values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getdJdmu(Z, ypsilon, c, omega, mu, K, eta_div):\n",
    "    u = np.transpose(Z[K])@omega + mu\n",
    "    dJdmu = np.transpose(eta_div(u))@(ypsilon-c)\n",
    "    return dJdmu\n",
    "\n",
    "def getdJdomega(Z, ypsilon, c, omega, mu, K, eta_div):\n",
    "    u = np.transpose(Z[K])@omega + mu\n",
    "    dJdOmega = Z[K] @ ((ypsilon - c) * eta_div(u))\n",
    "    return dJdOmega\n",
    "\n",
    "def getdJdW(P, Z, W, b, K, h, sigma_div):\n",
    "    dJdW = np.zeros_like(W)\n",
    "    for k in range(K):\n",
    "        u = W[k]@Z[k]+b[k]\n",
    "        dJdW[k] = h*(P[k+1]*sigma_div(u))@ np.transpose(Z[k])  \n",
    "    return dJdW\n",
    "\n",
    "def getdJdb(P, Z, W, b, K, h, sigma_div):\n",
    "    dJdb = np.zeros_like(b)\n",
    "    for k in range(K):\n",
    "        u = W[k]@Z[k]+b[k]\n",
    "        dJdb[k] = h*(P[k+1]*sigma_div(u))@np.ones((Z.shape[2],1))\n",
    "    return dJdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Above the functions to obtain the derivatives of the objective function with respect to weights and biased are defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plain(theta, tau, dJdtheta):\n",
    "    return theta - tau*dJdtheta\n",
    "\n",
    "def adam(theta,alpha,m,v,g,i):\n",
    "    #parameters\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 10E-8\n",
    "\n",
    "    #m,v\n",
    "    m = beta1*m + (1-beta1)*g\n",
    "    v = beta2*v + (1-beta2)*np.multiply(g,g)\n",
    "\n",
    "    m_hat = np.multiply(m,1/(1-beta1**i))\n",
    "    v_hat = np.multiply(v,1/(1-beta2**i))\n",
    "\n",
    "    #update\n",
    "    R = alpha*np.multiply(m_hat,1/(np.sqrt(v_hat)+epsilon))\n",
    "\n",
    "    return theta-R,m,v\n",
    "\n",
    "def getMandV(theta):\n",
    "    return np.zeros_like(theta),np.zeros_like(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two functions above implement two different methods of optimization which both are implemented in the training algorithm.\n",
    "\n",
    "The plain vanilla gradient descent simply follows the gradient in decreasing direction for a distance given by the learning parameter $\\tau$, while the Adam descent method, follows the gradient with different length depending on different parameters. The parameters $m$ and $v$ represent respectively a kind of mean value and squared mean value of the previous iterations gradients. The function $getMandV$ initializes $m$ and $v$ to be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale(x, alpha=0.2, beta=0.8):\n",
    "    a = np.amin(x)\n",
    "    b = np.amax(x)\n",
    "    \n",
    "    x_tilde = 1/(b-a)*((b-x)*alpha + (x-a)*beta)\n",
    "    return x_tilde, a, b\n",
    "\n",
    "def inverseScale(x_tilde, a, b, alpha=0.2, beta=0.8):\n",
    "    return 1/(beta-alpha)*((x_tilde-alpha)*b + (beta-x_tilde)*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to scale the exact function values, $c$, such that they are in the same interval as the output data of $\\eta$, used in $getYpsilon$. We want these to be in the same interval, so that we are able to accurately compare them in $getJ$. To do this we impliment a function to scale inputdata in terms of a min-max transformation, which guarantees that all datapoints lie in the interval $[\\alpha, \\beta]$. Since we are scaling the exact function values in the training algorithm, we have to inverse scale the $\\Upsilon$ in the testing algorithm to get back to the original interval of the exact function values. In $inverseScale$ $a$ and $b$ therefore are the min and max values returned from the scaling of the exact function values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 The training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainingAlgorithm(K, d, h, tau, Y0, c0, eta, sigma, eta_div, sigma_div, N, tol, chunksize, optimization):\n",
    "    \n",
    "    #finds input shape\n",
    "    d0 = Y0.shape[0]\n",
    "    I0 = Y0.shape[1]\n",
    "    \n",
    "    #reshapes input to match dimension of layers\n",
    "    if d0 > d:\n",
    "        print(\"d must be larger than d0\")\n",
    "        return\n",
    "        \n",
    "    elif d0 < d:\n",
    "        zero = np.zeros((d-d0,I0))\n",
    "        Y0 = np.vstack((Y0,zero))\n",
    "    \n",
    "    #gets initial weigths\n",
    "    omega = getomega(d)\n",
    "    mu = getmu()\n",
    "    W = getW(K,d)\n",
    "    b = getb(K,d)\n",
    "    \n",
    "    c0 = np.transpose(c0)\n",
    "  \n",
    "    if optimization == \"adam\":\n",
    "        #initial m,v for adam descent\n",
    "        mmu,vmu = getMandV(mu)\n",
    "        momega,vomega =getMandV(omega)\n",
    "        mW,vW = getMandV(W)\n",
    "        mb,vb = getMandV(b)\n",
    "    \n",
    "    #initializes vector to store objective function values\n",
    "    J = np.zeros(N)\n",
    "    ypsilon = np.zeros_like(c0)\n",
    "    \n",
    "    for i in range(N):\n",
    "               \n",
    "        #stochastic gradient descent\n",
    "        if I0 == chunksize:\n",
    "            Y0_chunk = Y0\n",
    "            c_chunk = c0\n",
    "        elif I0 > chunksize:\n",
    "            s = np.random.randint(0,I0-chunksize)\n",
    "            Y0_chunk = Y0[:,s:(s+chunksize)]\n",
    "            c_chunk=c0[s:(s+chunksize),:]\n",
    "        else:\n",
    "            print(\"chunksize must be smaller than I\")\n",
    "            return\n",
    "        \n",
    "\n",
    "        #transformations between layers\n",
    "        Z = getZ(Y0_chunk, W, b, K, d, chunksize, h, sigma)\n",
    "        ypsilon = getYpsilon(Z, omega, mu, K, eta)\n",
    "        P = getP(Z, ypsilon, c_chunk, omega, mu, W, b, K, d, chunksize, h, sigma_div, eta_div)\n",
    "        \n",
    "        #objective function\n",
    "        J[i] = getJ(ypsilon, c_chunk)\n",
    "        \n",
    "        #finds gradients\n",
    "        dJdmu = getdJdmu(Z, ypsilon, c_chunk, omega, mu, K, eta_div)\n",
    "        dJdomega = getdJdomega(Z, ypsilon, c_chunk, omega, mu, K, eta_div)\n",
    "        dJdW = getdJdW(P, Z, W, b, K, h, sigma_div)\n",
    "        dJdb = getdJdb(P, Z, W, b, K, h, sigma_div)\n",
    "        \n",
    "        if optimization == \"plain\":\n",
    "            mu = plain(mu, tau, dJdmu)\n",
    "            omega = plain(omega, tau, dJdomega)\n",
    "            W = plain(W, tau, dJdW)\n",
    "            b = plain(b, tau, dJdb)\n",
    "        \n",
    "        elif optimization == \"adam\":\n",
    "            mu, mmu,vmu = adam(mu,tau,mmu,vmu,dJdmu,i+1)\n",
    "            omega, momega,vomega = adam(omega,tau,momega,vomega,dJdomega,i+1)\n",
    "            W, mW,vW = adam(W,tau,mW,vW,dJdW,i+1)\n",
    "            b, mb,vb = adam(b,tau,mb,vb,dJdb,i+1)\n",
    "        \n",
    "        else:\n",
    "            print(\"This training algorithm only permits the use of either adam descent or plain vanilla gradient descent\")\n",
    "            print(\"Insert \\\"adam\\\" or \\\"plain\\\" as the optimization parameter\")\n",
    "            return\n",
    "            \n",
    "        \n",
    "        if (J[i]/chunksize) < tol:\n",
    "            break\n",
    "        \n",
    "    return mu, omega, W, b, J, ypsilon, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning of the training algorithm the input data is embedded into the network. We allow the input data to have a lower dimension $d_0$ than the dimension of the layers $d$ and then fill the extra dimension with zeros. The algorithm also allows for stochastic gradient descent, where a different randomly chosen subset of datapoints is used for each iteration. It also permits us to chose between the two different methods of optimization as an input parameter.\n",
    "\n",
    "Additionally, we chose to implement an evaluation of the objective function $J$ in the training algorithm which terminates when a given tolerance is reached. As the objective function is defined using the Fröbenius norm which sums the squared distances, we divide by the number of input data point in this evaluation. Terminating when the tolerance is reached both improves the runtime of the algorithm, as well as prevents possible overfitting of the network.\n",
    "\n",
    "The training algorithm returns trained values of the weights $W$, biases $b$, $\\omega$ and $\\mu$ as well as the objective function $J$, the trained function values $\\Upsilon$ and the number of iterations required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 The testing algorithms and functions used in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testingAlgorithm(yTest, W, b, omega, mu, K, d, I, h, sigma, eta, a_scale, b_scale):\n",
    "\n",
    "    #finds input shape\n",
    "    d0 = yTest.shape[0]\n",
    "    \n",
    "    if d > d0:\n",
    "        zero = np.zeros((d-d0,I))\n",
    "        yTest = np.vstack((yTest,zero))\n",
    "    elif d0 > d:\n",
    "        print(\"d must be larger than d0\")\n",
    "        return\n",
    "    \n",
    "    zTest = getZ(yTest, W, b, K, d, I, h, sigma)\n",
    "    ypsilonTilde = getYpsilon(zTest, omega, mu, K, eta)\n",
    "\n",
    "    #rescaling of input\n",
    "    ypsilonTest = inverseScale(ypsilonTilde, a_scale, b_scale)\n",
    "\n",
    "    return ypsilonTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing algorithm return the approximated function values $\\Upsilon$, using the trained weights $W$, biases $b$, $\\omega$ and $\\mu$. Again, we allow the input data to have a different dimension and then embed it into the dimension of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scaleYpsilon(ypsilon, a, b, alpha=0.2, beta=0.8):\n",
    "    return 1/(b-a)*((b-ypsilon)*alpha + (ypsilon-a)*beta)\n",
    "\n",
    "def successrate(ypsilon, c, tol,I):\n",
    "    ypsilon.resize((1,I))\n",
    "    c.resize((1,I))\n",
    "    \n",
    "    s = 0\n",
    "    acc = 0\n",
    "    \n",
    "    for i in range(I):\n",
    "        d = np.abs(ypsilon[0,i]-c[0,i])\n",
    "        acc += d\n",
    "        if d <= tol:\n",
    "            s +=1\n",
    "    return s/I, acc/I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $successrate$ is used to determine the successrate of the network and the mean deviation of the approximated from the exact values.  We define the successrate as the proportion of approximated values within a given distance of the exact function values.  \n",
    "\n",
    "When using the function $successrate$ we want to compare $\\Upsilon$ and $c$ scaled to the interval $[\\alpha ,\\beta]$ analogous to how they are compared in the loss function during the training. To do this we have to scale the approximated function values, $\\Upsilon$, according to the exact function values of the testing. Therefore we define $scaleYpsilon$, which uses the minimum $a$ and maximum $b$ of the exact function values to scale $\\Upsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deciding parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, optimal choices for the parameters $K$, $\\tau$, $d$ and $h$, as well as the batchsize used in stochastic gradient descent, will be investigated. All testing is done on $T(p)$ in batch 1 of the given data from the unknown Hamiltonian. To best illustrate the development of each parameter's impact, the number of iterations is set to 5000 and the tolerance set so low that the algorithm will not terminate before it has run through all iterations.\n",
    "\n",
    "The initial values of the parameters are set to  $K = 50$, $\\tau = 0.001$, $d = 3$ and $h =0.1$. After each investigation the current parameter will be updated to the chosen value which is then used in the remaining tests.\n",
    "\n",
    "The investigations include graphs for both implementation of the adam descent and plain vanilla gradient descent, but our choices of parameters will in all cases be made based on Adam descent, knowing this method is more efficient than plain vanilla gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from decidingParameters import filenameList, plotParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch0 = generate_data(0)\n",
    "\n",
    "p0_tilde = batch0['P']\n",
    "T0_tilde, aT0, bT0 = scale(batch0['T'])\n",
    "T0_tilde.resize(1,T0_tilde.shape[0])\n",
    "\n",
    "N = 5000\n",
    "tol = 1E-10 \n",
    "\n",
    "I_0 = p0_tilde.shape[1] #I_0 = 2048\n",
    "\n",
    "#initial values\n",
    "batchsize = I_0 \n",
    "K = 50\n",
    "tau = 0.001\n",
    "d_0 = p0_tilde.shape[0]   #in this case 3  \n",
    "h = 0.1     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 The batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variables for this test\n",
    "batchsize_list = np.arange(I_0/4, I_0+1,I_0/4).astype(int) \n",
    "\n",
    "filenames_batchsize_A, filenames_batchsize_P = filenameList(batchsize_list, \"batch\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(len(batchsize_list)):\n",
    "    batchsize = batchsize_list[i]\n",
    "    #train for different batchsizes with adam descent \n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_Abatch, ypsilon_A0, itr_A0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Abatch, itr_A0, filename = filenames_batchsize_A[i])\n",
    "    \n",
    "    #train for different batchsizes with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_Pbatch, ypsilon_P0, itr_P0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Pbatch, itr_P0, filename = filenames_batchsize_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotParams(batchsize_list, \"batch\", K = K , d = d_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs illustrate that the use of stochastic gradient descent causes the objective function to oscillate around a minimum instead of converging smoothly and that decrasing the batchsize increases the range of the oscillations.  \n",
    "\n",
    "However, decreasing the batchsize also decreases the runtime which is especially relevant when training for large amounts of input data. From the graphs it can be seen that using half the input data causes relatively regular oscillations and does converges, so that this will be a reasonable choice when using stochastic gradient descent.\n",
    "\n",
    "When using smaller amounts of input data it is not necessary to use stochastic gradient descent to decrease the runtime, so we have chosen not to do so for the known functions in this project. We will also not use stochastic gradient descent when investigating further parameter choices as it is easier to evaluate the graphs without oscillations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 The number of hidden layers, $K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variables for this test   \n",
    "K_list = np.arange(25,101,25) \n",
    "\n",
    "#define lists of filenames where the trained parameters will be stored\n",
    "filenames_K_A, filenames_K_P = filenameList(K_list, \"K\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(len(K_list)):\n",
    "    K = K_list[i]\n",
    "    #train for different K values with adam descent\n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_AK, ypsilon_A0, itr_A0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_AK, itr_A0, filename = filenames_K_A[i])\n",
    "    \n",
    "    #train for different K values with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_PK, ypsilon_P0, itr_P0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_PK, itr_P0, filename = filenames_K_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotParams(K_list, \"K\", d = d_0, batchsize = batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs show that the loss function decreases similarly for $K=25$ and $K=75$, although it is somewhat lower for $K=75$. However, a higher number of hidden layers will also increase the runtime considerably so, to balance accuracy and runtime, we will use $K=25$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 The value of the learning parameter $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variables for this test\n",
    "tau_list = np.array([0.001,0.004,0.007,0.01])   \n",
    "K_tau = 25\n",
    "\n",
    "#define lists of filenames where the trained parameters will be stored\n",
    "filenames_tau_A, filenames_tau_P = filenameList(tau_list, \"tau\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(len(tau_list)):\n",
    "    tau = tau_list[i]\n",
    "\n",
    "    #train for different tau values with adam descent\n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_Atau, ypsilon_A0, itr_A0 = trainingAlgorithm(K_tau, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Atau, itr_A0, filename = filenames_tau_A[i])\n",
    "    \n",
    "    #train for different tau values with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_Ptau, ypsilon_P0, itr_P0 = trainingAlgorithm(K_tau, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Ptau, itr_P0, filename = filenames_tau_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotParams(tau_list, \"tau\", K = K_tau, d = d_0, batchsize = batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here we can see that the objective function seems to spike intermittently for $\\tau=0.004$, $\\tau = 0.007$ and $\\tau = 0.01$. While it is smooth for $\\tau = 0.001$, it also decreases more slowly and converges to a larger value, so that it is not optimal. For $\\tau = 0.004$ the spikes start after a higher number of iterations than for the other two and are smaller in magnitude, which makes it the optimal choice for the learning parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 The dimension of the input data in the hidden layers $d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variables for this test\n",
    "d_0_list = np.array([3,4,5,6])  \n",
    "K_d = 25\n",
    "tau_d = 0.004\n",
    "\n",
    "#define lists of filenames where the trained parameters will be stored\n",
    "filenames_d_A, filenames_d_P = filenameList(d_0_list, \"d\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(len(d_0_list)):\n",
    "    d0 = d_0_list[i]\n",
    "    \n",
    "    #train for different values of d with adam descent\n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_Ad0, ypsilon_A0, itr_A0 = trainingAlgorithm(K_d, d0, h, tau_d, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Ad0, itr_A0, filename = filenames_d_A[i])\n",
    "    \n",
    "    #train for different values of d with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_Pd0, ypsilon_P0, itr_P0 = trainingAlgorithm(K_d, d0, h, tau_d, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Pd0, itr_P0, filename = filenames_d_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotParams(d_0_list, \"d\", K = K_d, batchsize = batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dimension of the input data is $d_0=3$. The graphs illustrate that having a higher dimension in the hidden layers improves the decrease of the loss function even though it also causes some spikes. We choose to use $d=5$ as the dimension of our layers as the spikes in its corresponding loss function are considerably smaller in magnitude than for $d=4$ and $d=6$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 The stepsize $h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variables for this test\n",
    "h_list = np.array([0.05, 0.1, 0.15, 0.2])\n",
    "K_h = 25\n",
    "tau_h = 0.004\n",
    "d_h = 5\n",
    "\n",
    "#define lists of filenames where the trained parameters will be stored\n",
    "filenames_h_A, filenames_h_P= filenameList(h_list, \"h\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(len(h_list)):\n",
    "    h = h_list[i]\n",
    "    #train for different stepsizes h with adam descent\n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_Ah, ypsilon_A0, itr_A0 = trainingAlgorithm(K_h, d_h, h, tau_h, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Ah, itr_A0, filename = filenames_h_A[i])\n",
    "    \n",
    "    #train for different stepsizes h with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_Ph, ypsilon_P0, itr_P0 = trainingAlgorithm(K_h, d_h, h, tau_h, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Ph, itr_P0, filename = filenames_h_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotParams(h_list, \"h\", K = K_h, d = d_h, batchsize = batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs above we can see that the loss function behaves similarly and is smooth for stepsizes $h=0.1$, $h=0.15$ and $h=0.2$ during the first $N=2000$ iterations. Thereafter however all three develop some occasional spikes. The spiking is a lot more frequent for $h=0.1$ and $h=0.2$, so therefore we choose to use $h=0.15$ as the optimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that the best parameters, taking both accuracy and runtime into account, were $K = 25$, $\\tau = 0.004$, $d=5$ and $h = 0.15$ and will use those in the training. When training for other functions than the unknown Hamiltonian which we used for paramter testing, we will not use $d=5$, but simply $d=2d_0$ as suggested in the Project description.\n",
    "\n",
    "We have also seen that there might occur some spikes in the loss function, however the tolerance checkpoint implemented in the training algorithm  will prevent that the training terminates during one of the spikes.\n",
    "\n",
    "Throughout all the tests, it is also clear that Adam descent is a more efficient optimization method than plain vanilla gradient descent. Therefore we will simply use the Adam descent for all training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training and testing with suggested functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global variables for all the following tests\n",
    "h = 0.15                #stepsize\n",
    "tau = 0.004             #learning parameter\n",
    "K = 25                  #number of hidden layers\n",
    "\n",
    "tol = 1e-5              #tolerance for J/batchsize\n",
    "N = 5000                #number of iterations\n",
    "tol_dev = (2*tol)**0.5  #tolerated deviation from exact solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above the global parameters determined in section 3 are defined. The dimension of the hidden layers $d$ varies for the different functions and will therefore be defined in the relevant subsections.\n",
    "\n",
    "Additionally tolerances for the loss function in the training algorithm and the successrate are defined. The tolerance for the loss function is set to $tol = 10^{-5}$ as this gave good results during early runs. The tolerance for the successrate determines the distance deemed acceptable between the exact and approximated function values. The evaluation of the loss function terminates when $\\frac{J}{batchsize} \\leq tol$ and since $J = \\frac12 \\|\\Upsilon-c\\|_2^2$, this tolerance is set to $\\sqrt{2 \\cdot tol} \\approx 4.5\\cdot 10^{-3}$. When determining the successrate for $H(p,q) = T(p) + V(q)$, we will double the accepted distance as the networks for $T(p)$ and $V(q)$ are trained seperately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1  $F(y) = 1-\\cos{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F(y):\n",
    "    return 1-np.cos(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variable used for training with F(y)\n",
    "d_F = 2        \n",
    "I_F = 1500 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Y0_F = np.random.uniform(-2,2,(1,I_F))\n",
    "c_F = F(Y0_F)\n",
    "Y0_F_tilde = Y0_F\n",
    "c_F_tilde, ac_F, bc_F = scale(c_F)\n",
    "\n",
    "#training for F(y)\n",
    "mu_F, omega_F, W_F, b_F, J_F, ypsilon_F, itr_F = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_F, \"adam\")\n",
    "\n",
    "#stores the trained parameters in file\n",
    "writeParams(W_F, b_F, omega_F, mu_F,ypsilon_F, J_F, itr_F, filename = 'trainingParams_F.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read the stored trained parameters\n",
    "W_F,b_F,omega_F,mu_F,ypsilon_F, J_F, itr_F = readParams(K, d_F, I_F, N, filename='trainingParams_F.txt')\n",
    "ac_F,bc_F,ac_G,bc_G = readScale('scaleParamsFandG.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotObjFnc(J_F, itr_F, I_F, \"$F(y) = 1-\\cos(y)$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above shows that the loss function decreased to $10^{-5}$ within $N=1854$ iterations upon which the training processs for $F(y) = 1-\\cos(y)$ terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with F(y)\n",
    "YTest_F = np.sort(np.random.uniform(-2,2,(1,I_F)))\n",
    "cTest_F = F(YTest_F)\n",
    "\n",
    "ypsilonTest_F = testingAlgorithm(YTest_F, W_F, b_F, omega_F, mu_F, K, d_F, I_F, h, sigma, eta, ac_F, bc_F)\n",
    "\n",
    "\n",
    "#plotting\n",
    "plt.figure()\n",
    "plt.plot(YTest_F[0], ypsilonTest_F, label = \"Approximated\")\n",
    "plt.plot(YTest_F[0], cTest_F[0], label = \"Exact\")\n",
    "plt.xlabel(r\"$y$\")\n",
    "plt.ylabel(r\"$F(y)$\")\n",
    "plt.title(r\"Testing on $F(y) = 1-\\cos(y)$\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "cTest_Ftilde, aftest, bftest = scale(cTest_F)\n",
    "ypsilonTilde_F = scaleYpsilon(ypsilonTest_F, aftest, bftest)\n",
    "\n",
    "\n",
    "succF, accF = successrate(ypsilonTilde_F, cTest_Ftilde, tol_dev, I_F)\n",
    "print(\"The successrate is\",round(succF,3))\n",
    "print(\"The mean deviation is\",round(accF,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated in the graph, the appoximated values are close to the exact values and deviate mostly around the intervals endpoints and the minimum the function in the trained interval. To improve the network's ability to recognize the values at the endpoints it is possible to train for a bigger interval than one wants to test for. \n",
    "\n",
    "We see that the the successrate is only $70.1\\%$. However, the mean deviation is $3.45\\cdot 10^{-3}$ which actually lies within the tolerated distance $4.5\\cdot10^{-3}$. This indicates that most of the datapoints corresponds well with the exact values, while the rest deviate slightly too much to be classified as successful approximants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 $G(y_1,y_2) = \\frac{1}{2} (y_1^2 + y_2^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def G(y1, y2):\n",
    "    return 1/2 *(y1**2 + y2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variable used for training with G(y)\n",
    "d_G = 4       \n",
    "I_G = 1500    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#training for G(y)\n",
    "Y0_G = np.random.uniform(-2,2,(2,I_G))\n",
    "c_G = G(Y0_G[0],Y0_G[1])\n",
    "c_G.resize(1,I_G)\n",
    "\n",
    "Y0_G_tilde = Y0_G\n",
    "c_G_tilde, ac_G, bc_G = scale(c_G)\n",
    "\n",
    "mu_G, omega_G, W_G, b_G, J_G, ypsilon_G, itr_G = trainingAlgorithm(K, d_G, h, tau, Y0_G_tilde, c_G_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_G, \"adam\")\n",
    "writeParams(W_G, b_G, omega_G, mu_G, ypsilon_G, J_G, itr_G, filename = 'trainingParams_G.txt')\n",
    "writeScale(ac_F,bc_F,ac_G,bc_G,'scaleParamsFandG.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_G,b_G,omega_G,mu_G, ypsilon_G, J_G, itr_G = readParams(K, d_G, I_G, N, filename='trainingParams_G.txt')\n",
    "ac_F,bc_F,ac_G,bc_G = readScale('scaleParamsFandG.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotObjFnc(J_G, itr_G, I_G, \"$G(y_1,y_2) = 0.5 (y_1^2 + y_2^2)$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process for  $G(y_1,y_2) = \\frac{1}{2} (y_1^2 + y_2^2)$ terminated after $N=1292$ when the loss function had decreased to $10^{-5}$ as shown in the graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with G(y)\n",
    "YTest_G = np.random.uniform(-2,2,(2,I_G))\n",
    "cTest_G = G(YTest_G[0], YTest_G[1])\n",
    "cTest_G.resize(1,I_G)\n",
    "\n",
    "ypsilonTest_G = testingAlgorithm(YTest_G, W_G, b_G, omega_G, mu_G, K, d_G, I_G, h, sigma, eta, ac_G, bc_G)\n",
    "\n",
    "\n",
    "#plotting of G(y)\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(6.5)\n",
    "fig.set_figwidth(7.5)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(YTest_G[0], YTest_G[1], cTest_G[0], label = \"Exact\" , c=\"red\", depthshade = True)\n",
    "ax.scatter(YTest_G[0], YTest_G[1], ypsilonTest_G, label = \"Approximated\", depthshade = False)\n",
    "ax.set_title(r\"Testing on $G(y_1, y_2)= \\frac{1}{2} (y_1^2 + y_2^2)$\")\n",
    "plt.legend(loc = 'lower left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#determine successrate and accuracy\n",
    "cTest_Gtilde, aGtest, bGtest = scale(cTest_G)\n",
    "ypsilonTilde_G = scaleYpsilon(ypsilonTest_G, aGtest, bGtest)\n",
    "\n",
    "succG, accG = successrate(ypsilonTilde_G, cTest_Gtilde, tol_dev, I_G)\n",
    "print(\"Successrate:\",round(succG,3))\n",
    "print(\"Mean deviation:\",round(accG,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph illustrate that the appoximated values lie relativly close to the exact values, but it is difficult to ascertain the exact magnitude of the deviation or in which areas it deviates the most from the graph. We have however determined that the successrate is $74.9\\%$ and the mean deviation $3.25\\cdot10^{-3}$, so that we know that the network approximates the function well and without any large outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 4.3 Known Hamiltonian "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Nonlinear pendel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = 1E-2\n",
    "l = 0.5\n",
    "g = 9.81\n",
    "\n",
    "def T_pend(p):\n",
    "    return 0.5*p**2\n",
    "\n",
    "def V_pend(q):\n",
    "    return m*g*l*(1-np.cos(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I_p = 1500\n",
    "d_p = 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Training for nonlinear pendel\n",
    "q_p = np.random.uniform(-2*np.pi,2*np.pi,(1,I_p))\n",
    "V_p = V_pend(q_p)\n",
    "V_p_tilde, aVp, bVp = scale(V_p)\n",
    "\n",
    "p_p = np.random.uniform(-10,10,(1,I_p))\n",
    "T_p = T_pend(p_p)\n",
    "T_p_tilde, aTp, bTp = scale(T_p)\n",
    "\n",
    "#Training for V\n",
    "mu_p, omega_p, W_p, b_p, J_p, ypsilon_p, itr_p = trainingAlgorithm(K, d_p, h, tau, q_p, V_p_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_p, \"adam\")\n",
    "writeParams(W_p, b_p, omega_p, mu_p, ypsilon_p, J_p, itr_p, filename = 'trainingParams_Vpend.txt')\n",
    "\n",
    "#Training for T\n",
    "mu_pp, omega_pp, W_pp, b_pp, J_pp, ypsilon_pp, itr_pp = trainingAlgorithm(K, d_p, h, tau, p_p, T_p_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_p, \"adam\")\n",
    "writeParams(W_pp, b_pp, omega_pp, mu_pp, ypsilon_pp, J_pp, itr_pp, filename = 'trainingParams_Tpend.txt')\n",
    "\n",
    "writeScale(aVp,bVp,aTp,bTp,'scaleParamsPend.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_Tpend,b_Tpend,omega_Tpend,mu_Tpend, ypsilon_Tpend, J_Tpend, itr_Tpend = readParams(K, d_p, I_p, N, filename='trainingParams_Tpend.txt')\n",
    "W_Vpend,b_Vpend,omega_Vpend,mu_Vpend, ypsilon_Vpend, J_Vpend, itr_Vpend = readParams(K, d_p, I_p, N, filename='trainingParams_Vpend.txt')\n",
    "aVp,bVp,aTp,bTp = readScale('scaleParamsPend.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotObjFnc(J_Tpend, itr_Tpend, I_p, \"$T(p) = 0.5p^2$\")\n",
    "plotObjFnc(J_Vpend, itr_Vpend, I_p, \"$V(q) = mgl\\cdot(1-\\cos(q))$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network obtained the desired value $10^{-5}$ for the loss function after about $N=2500$ iterations for both $T(p)$ and $V(q)$ for the nonlinear pendulum function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with V_pend(q) and T_pend(p)\n",
    "q_pTest = np.random.uniform(-np.pi,np.pi,(1,I_p))\n",
    "V_pTest = V_pend(q_pTest)\n",
    "p_pTest = np.random.uniform(-5,5,(1,I_p))\n",
    "T_pTest = T_pend(p_pTest)\n",
    "\n",
    "\n",
    "ypsilonTest_Vpend = testingAlgorithm(q_pTest, W_Vpend, b_Vpend, omega_Vpend, mu_Vpend, K, d_p, I_p, h, sigma, eta, aVp, bVp)\n",
    "ypsilonTest_Tpend = testingAlgorithm(p_pTest, W_Tpend, b_Tpend, omega_Tpend, mu_Tpend, K, d_p, I_p, h, sigma, eta, aTp, bTp)\n",
    "\n",
    "\n",
    "#plotting \n",
    "fig = plt.figure()\n",
    "fig.set_figheight(6.5)\n",
    "fig.set_figwidth(7.5)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(q_pTest[0], p_pTest[0], V_pTest[0]+T_pTest[0], label = \"Exact\" , c=\"red\", depthshade = False)\n",
    "ax.scatter(q_pTest[0], p_pTest[0], ypsilonTest_Vpend[:,0]+ypsilonTest_Tpend[:,0], label = \"Approximated\", depthshade = True)\n",
    "ax.set_title(r\"Testing on nonlinear pendel\")\n",
    "ax.set_xlabel(\"q\")\n",
    "ax.set_ylabel(\"p\")\n",
    "ax.set_zlabel(\"H\")\n",
    "plt.legend(loc = 'lower left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#determine the successrates and accuracies \n",
    "V_pTilde, aVpend_test, bVpend_test = scale(V_pTest)\n",
    "ypsilonTilde_Vpend = scaleYpsilon(ypsilonTest_Vpend, aVpend_test, bVpend_test)\n",
    "\n",
    "T_pTilde, aTpend_test, bTpend_test = scale(T_pTest)\n",
    "ypsilonTilde_Tpend = scaleYpsilon(ypsilonTest_Tpend, aTpend_test, bTpend_test)\n",
    "\n",
    "succVpend, accVpend = successrate(ypsilonTilde_Vpend, V_pTilde, tol_dev, I_p)\n",
    "succTpend, accTpend = successrate(ypsilonTilde_Tpend, T_pTilde, tol_dev, I_p)\n",
    "\n",
    "succHpend, accHpend = successrate(ypsilonTilde_Vpend+ypsilonTilde_Tpend, V_pTilde+T_pTilde, 2*tol_dev, I_p)\n",
    "print(\"Successrate for V(q):\",round(succVpend,3))\n",
    "print(\"Mean deviation for V(q):\",round(accVpend,5),'\\n')\n",
    "\n",
    "print(\"Successrate for T(p):\", round(succTpend,3))\n",
    "print(\"Mean deviation for T(p):\",round(accTpend,5),'\\n')\n",
    "\n",
    "print(\"Successrate for H(p,q)\", round(succHpend,3))\n",
    "print(\"Mean deviation for H(p,q)\", round(accTpend,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the graph shows that the approximated values mimic the original function, however it is also here difficult to determine how the magnitude of deviation, although we can see some clear deviation around the minimum at $p=0$\n",
    "\n",
    "The calculated successrates and mean deviations show that the trained network for $V$ is slightly better than that for $T$. Still both networks have a mean deviation well within $4.5\\cdot 10^{-3}$, which shows that most approximated values are close to the exact ones. Moreover, the successrate of the complete Hamiltonian function found by adding $T$ and $V$ is $93.5\\%$ and the mean deviation is also within the acceptable range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Kepler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def T_Kepler(p):\n",
    "    T_p = np.zeros((1,p.shape[1]))\n",
    "    for i in range(p.shape[1]):\n",
    "        T_p[:,i] = 1/2*np.transpose(p[:,i])@p[:,i]\n",
    "    return T_p\n",
    "\n",
    "def V_Kepler(q1,q2):\n",
    "    return -1/np.sqrt(q1**2+q2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I_K = 1500\n",
    "d_K = 4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p_K = np.random.uniform(-10,10, (2,I_K))\n",
    "T_K = T_Kepler(p_K)\n",
    "T_K_tilde, aTK, bTK = scale(T_K)\n",
    "\n",
    "q_K = np.random.uniform(-10,10, (2,I_K))\n",
    "V_K = V_Kepler(q_K[0],q_K[1])\n",
    "V_K.resize(1,I_K)\n",
    "V_K_tilde, aVK, bVK = scale(V_K)\n",
    "\n",
    "#Training T\n",
    "mu_TK, omega_TK, W_TK, b_TK, J_TK, ypsilon_TK, itr_TK = trainingAlgorithm(K, d_K, h, tau, p_K, T_K_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_K, \"adam\")\n",
    "writeParams(W_TK, b_TK, omega_TK, mu_TK, ypsilon_TK, J_TK, itr_TK, filename = 'trainingParams_TKepler.txt')\n",
    "\n",
    "#Training V\n",
    "mu_VK, omega_VK, W_VK, b_VK, J_VK, ypsilon_VK, itr_VK = trainingAlgorithm(K, d_K, h, tau, q_K, V_K_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_K, \"adam\")\n",
    "writeParams(W_VK, b_VK, omega_VK, mu_VK, ypsilon_VK, J_VK, itr_VK, filename = 'trainingParams_VKepler.txt')\n",
    "\n",
    "#saving scale parameters\n",
    "writeScale(aVK,bVK,aTK,bTK,'scaleParamsKepler.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_TK,b_TK,omega_TK,mu_TK, ypsilon_TK, J_TK, itr_TK = readParams(K, d_K, I_K, N, filename='trainingParams_TKepler.txt')\n",
    "W_VK,b_VK,omega_VK,mu_VK, ypsilon_VK, J_VK, itr_VK = readParams(K, d_K, I_K, N, filename='trainingParams_VKepler.txt')\n",
    "aVK,bVK,aTK,bTK = readScale('scaleParamsKepler.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting of objective function\n",
    "plotObjFnc(J_TK, itr_TK, I_K, '$T(p) = 0.5 p^T p$')\n",
    "plotObjFnc(J_VK, itr_VK, I_K, '$V(q_1, q_2)= - 1 / (q_1^2 + q_2^2)^{1/2}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network obtained the desired value $10^{-5}$ for the loss function after about $N=3000$ iterations for both $T(p)$ and $V(q)$ for the Kepler two-body problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing with V_Kepler(q) and T_Kepler(p)\n",
    "q_KTest = np.random.uniform(-10,10,(2,I_K))\n",
    "V_KTest = V_Kepler(q_KTest[0], q_KTest[1])\n",
    "V_KTest.resize(1,I_K)\n",
    "p_KTest = np.random.uniform(-10,10,(2,I_K))\n",
    "T_KTest = T_Kepler(p_KTest)\n",
    "T_KTest.resize(1,I_K)\n",
    "\n",
    "ypsilonTest_VKepler = testingAlgorithm(q_KTest, W_VK, b_VK, omega_VK, mu_VK, K, d_K, I_K, h, sigma, eta, aVK, bVK)\n",
    "ypsilonTest_TKepler = testingAlgorithm(p_KTest, W_TK, b_TK, omega_TK, mu_TK, K, d_K, I_K, h, sigma, eta, aTK, bTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine the succeccrates and accuracies for Kepler two-body. \n",
    "V_KTilde, aVKepler_test, bVKepler_test = scale(V_KTest)\n",
    "ypsilonTilde_VKepler = scaleYpsilon(ypsilonTest_VKepler, aVKepler_test, bVKepler_test)\n",
    "\n",
    "succVKepler, accVKepler = successrate(ypsilonTilde_VKepler, V_KTilde, tol_dev, I_K)\n",
    "\n",
    "T_KTilde, aTKepler_test, bTKepler_test = scale(T_KTest)\n",
    "ypsilonTilde_TKepler = scaleYpsilon(ypsilonTest_TKepler, aTKepler_test, bTKepler_test)\n",
    "\n",
    "succTKepler, accTKepler = successrate(ypsilonTilde_TKepler, T_KTilde, tol_dev, I_K)\n",
    "\n",
    "succHKepler, accHKepler = successrate(ypsilonTilde_TKepler + ypsilonTilde_VKepler, T_KTilde + V_KTilde, 2*tol_dev, I_K)\n",
    "\n",
    "print(\"Testing on the Kepler two-body problem\")\n",
    "print(\"Successrate for V(q):\",round(succVKepler,3))\n",
    "print(\"Mean deviation for V(q):\",round(accVKepler,5),'\\n')\n",
    "\n",
    "print(\"Successrate for T(p):\",round(succTKepler,3))\n",
    "print(\"Mean deviation for T(p):\",round(accTKepler,5),'\\n')\n",
    "\n",
    "print(\"Successrate for H(p,q):\",round(succHKepler,3))\n",
    "print(\"Mean deviation for H(p,q):\",round(accHKepler,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the testing for the Kepler two-body problem are not illustrated graphically as it would be difficult to illustrate the whole truth because of its high dimension. However the calculated successrates and mean deviations show that the trained networks again seem to approximate $V$ more accurately than $T$. Although the mean deviation $8.19\\cdot10^{-3}$ for $T$ is slightly higher than the accepted distance $4.5\\cdot10^{-3}$, it is still relatively low which shows that the values on average do not deviate greatly and that there seem to be no large outliers, and for $V$ the mean deviation lies within the accepted distance.\n",
    "\n",
    "The successrate for the complete Hamiltonian fuction is only $67.2\\%$, but we see also here that the mean deviation of $8.43\\cdot10^{-3}$ still is comparably low, though outside the accepted distance, and indicates that the function is approximated relatively well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Unknown Hamiltonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingBatch = concatenate(0,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I_T = trainingBatch[\"P\"].shape[1]\n",
    "d_T = 5\n",
    "batchsize = int(I_T/2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pT_tilde = trainingBatch[\"P\"]\n",
    "TT_tilde, aTT, bTT = scale(trainingBatch[\"T\"])\n",
    "TT_tilde.resize(1,TT_tilde.shape[0])\n",
    "\n",
    "qT_tilde = trainingBatch[\"Q\"]\n",
    "VT_tilde, aVT, bVT = scale(trainingBatch[\"V\"])\n",
    "VT_tilde.resize(1,VT_tilde.shape[0])\n",
    "\n",
    "#training T\n",
    "mu_TT, omega_TT, W_TT, b_TT, J_TT, ypsilon_TT, itr_TT = trainingAlgorithm(K, d_T, h, tau, pT_tilde, TT_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "writeParams(W_TT, b_TT, omega_TT, mu_TT, ypsilon_TT, J_TT, itr_TT, \"trainingParams_TUnknown.txt\")\n",
    "\n",
    "#training V \n",
    "mu_VT, omega_VT, W_VT, b_VT, J_VT, ypsilon_VT, itr_VT = trainingAlgorithm(K, d_T, h, tau, qT_tilde, VT_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "writeParams(W_VT, b_VT, omega_VT, mu_VT, ypsilon_VT, J_VT, itr_VT, \"trainingParams_VUnknown.txt\")\n",
    "\n",
    "writeScale(aVT, bVT, aTT, bTT, \"scaleParamsUnknown.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_TT,b_TT,omega_TT,mu_TT, ypsilon_TT, J_TT, itr_TT = readParams(K, d_T, batchsize, N, filename='trainingParams_TUnknown.txt')\n",
    "\n",
    "W_VT,b_VT,omega_VT,mu_VT, ypsilon_VT, J_VT, itr_VT = readParams(K, d_T, batchsize, N, filename='trainingParams_VUnknown.txt')\n",
    "\n",
    "aVT, bVT, aTT, bTT = readScale(\"scaleParamsUnknown.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting of objective function\n",
    "plotObjFnc(J_TT, itr_TT, batchsize, \"the unknown Hamiltonians T(p)\")\n",
    "plotObjFnc(J_VT, itr_VT, batchsize, \"the unknown Hamiltonians V(q)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs show that the training process for $T(p)$ and $V(p)$ terminates after respectively $N=809$ and $N=2087$ iterations using data from the first $25$ batches with data from the unknown Hamiltonian function.\n",
    "We also see that the graphs of both loss functions oscillate, due to the use of stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_unknown = generate_data(27)\n",
    "\n",
    "p_unknown = batch_unknown[\"P\"][:1,:]\n",
    "q_unknown = batch_unknown[\"Q\"][:1,:]\n",
    "\n",
    "I_unknown = p_unknown.shape[1]\n",
    "\n",
    "p_unknown0 = p_unknown\n",
    "q_unknown0 = q_unknown\n",
    "\n",
    "ones = np.ones((2,I_unknown))\n",
    "p_unknown1 = np.vstack((p_unknown,ones))\n",
    "q_unknown1 = np.vstack((q_unknown,ones))\n",
    "\n",
    "\n",
    "ypsilonT_unknown0 = testingAlgorithm(p_unknown0,W_TT,b_TT,omega_TT,mu_TT, K, d_T, I_unknown, h, sigma, eta, aTT, bTT)\n",
    "ypsilonT_unknown1 = testingAlgorithm(p_unknown1,W_TT,b_TT,omega_TT,mu_TT, K, d_T, I_unknown, h, sigma, eta, aTT, bTT)\n",
    "\n",
    "ypsilonV_unknown0 = testingAlgorithm(q_unknown0,W_VT,b_VT,omega_VT,mu_VT, K, d_T, I_unknown, h, sigma, eta, aVT, bVT)\n",
    "ypsilonV_unknown1 = testingAlgorithm(q_unknown1,W_VT,b_VT,omega_VT,mu_VT, K, d_T, I_unknown, h, sigma, eta, aVT, bVT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(11)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(q_unknown0[0], p_unknown0[0], ypsilonV_unknown0+ypsilonT_unknown0, label = \"p2,p3,q2,q3 = 0\", depthshade = False)\n",
    "ax.scatter(q_unknown1[0], p_unknown1[0], ypsilonV_unknown1+ypsilonT_unknown1, label = \"p2,p3,q2,q3 = 1\", depthshade = False)\n",
    "ax.set_xlabel(\"q\")\n",
    "ax.set_ylabel(\"p\")\n",
    "ax.set_zlabel(r\"H\")\n",
    "ax.set_title(\"Unknown Hamiltonian function with one free variable\")\n",
    "\n",
    "ax.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows how the trained network approximates the unknown Hamiltonian $H(p,q)=T(p)+V(q)$ with the the variables $p2$,$p3$,$q2$ and $q3$ fixed at $0$ for the blue and at $1$ for the orange graph. We can see that they have a similar shape although they are different, which is what we would expect as they are given by the same function but for different fixed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing with unknown V(q) and T(p) from batch 27\n",
    "batch_unknown = generate_data(27)\n",
    "\n",
    "p_testUnknown = batch_unknown[\"P\"]\n",
    "q_testUnknown = batch_unknown[\"Q\"]\n",
    "V_testUnknown = batch_unknown[\"V\"]\n",
    "T_testUnknown = batch_unknown[\"T\"]\n",
    "\n",
    "I_unknown = p_testUnknown.shape[1]\n",
    "\n",
    "ypsilonTest_Tunknown = testingAlgorithm(p_testUnknown,W_TT,b_TT,omega_TT,mu_TT, K, d_T, I_unknown, h, sigma, eta, aTT, bTT)\n",
    "ypsilonTest_Vunknown = testingAlgorithm(q_testUnknown,W_VT,b_VT,omega_VT,mu_VT, K, d_T, I_unknown, h, sigma, eta, aVT, bVT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine the succeccrates and accuracies for the unknown Hamiltonian\n",
    "V_tildeUnknown, aVUnknown_test, bVUnknown_test = scale(V_testUnknown)\n",
    "ypsilonTilde_Vunknown = scaleYpsilon(ypsilonTest_Vunknown, aVUnknown_test, bVUnknown_test)\n",
    "\n",
    "succVUnknown, accVUnknown = successrate(ypsilonTilde_Vunknown, V_tildeUnknown, tol_dev, I_unknown)\n",
    "\n",
    "\n",
    "T_tildeUnknown, aTUnknown_test, bTUnknown_test = scale(T_testUnknown)\n",
    "ypsilonTilde_Tunknown = scaleYpsilon(ypsilonTest_Tunknown, aTUnknown_test, bTUnknown_test)\n",
    "\n",
    "succTUnknown, accTUnknown = successrate(ypsilonTilde_Tunknown, T_tildeUnknown, tol_dev, I_unknown)\n",
    "\n",
    "succHUnknown, accHUnknown = successrate(ypsilonTilde_Tunknown + ypsilonTilde_Vunknown, T_tildeUnknown + V_tildeUnknown, 2*tol_dev, I_unknown)\n",
    "\n",
    "print(\"Testing on the unknown Hamiltonian\")\n",
    "print(\"Successrate for V(q):\",round(succVUnknown,3))\n",
    "print(\"Mean deviation for V(q):\",round(accVUnknown,5),'\\n')\n",
    "\n",
    "print(\"Successrate for T(p):\",round(succTUnknown,3))\n",
    "print(\"Mean deviation for T(p):\",round(accTUnknown,5),'\\n')\n",
    "\n",
    "print(\"Successrate for H(p,q):\",round(succHUnknown,3))\n",
    "print(\"Mean deviation for H(p,q):\",round(accHUnknown,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without having some of the input dimensions at a fixed value, it is difficult to illustrate the unknown Hamiltonian function. The result of the testing are therefore not shown as graphs.\n",
    "\n",
    "We see that the successrates of the trained network for the unknown functions are low, however the mean deviations are still comparably small although one order of magnitude larger than the accepted distance. Still the networks seem to be able to approximate the function relatively well.\n",
    "\n",
    "The successrate for the complete Hamiltononian $H(p,q)=T(p)+V(q)$ is very low at only $6.9\\%$, however also here the mean deviation is still comparatively small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Computing the gradient "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section presents the formulas for computing an approximated gradient of a function using the trained network for the function. In the next section this gradient is then used in the implementation of the symplectic Euler and Størmer-Verlet numerical methods to find the trajectories of the Hamiltonian functions presented earlier.\n",
    "\n",
    "Based on the appendix \"Hints for evaluating the gradient\", the gradient formula is derived to be \n",
    "\n",
    "$$\\nabla F(y) = \\left(D\\Psi_{K-1}(y)\\right)^T\\nabla G(Z^{(K)}) $$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\nabla G(Z^{(K)}) = \\eta'(\\omega^TZ^{(K)}+\\mu)\\omega,$$ \n",
    "\n",
    "$$(D\\Psi_{K-1}(y))^T = (D\\Psi_{K-2}(y))^T\\left(\\Phi_{K-1}(Z^{(K-1)}) \\right)^T,$$\n",
    "\n",
    "$$D\\Phi(Z^{(K-1)})^TA = A + W^T(h\\sigma'(WZ^{(K-1)}+b) \\odot A).$$\n",
    "\n",
    "\n",
    "This is implemented in the function $gradF$ below, using backward propagation. \n",
    "\n",
    "Since $gradF$ uses the weights and biases from the training with scaled exact function values, we have to inverse scale the gradient with $a$ and $b$ as the min and max values returned from the scaling of the exact function values. Hence, the function $inverseScaleGrad$ is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradF(y, omega, mu, W, b, K, d, h, sigma, eta_div, sigma_div):\n",
    "    d0 = y.shape[0]\n",
    "    I = y.shape[1]\n",
    "    \n",
    "    #reshapes input to match dimension of layers\n",
    "    if d0 > d:\n",
    "        print(\"d must be larger than d0\")\n",
    "        return\n",
    "    \n",
    "    elif d0 < d:\n",
    "        zero = np.zeros((d-d0,I))\n",
    "        y = np.vstack((y,zero))\n",
    "    \n",
    "    Z = getZ(y, W, b, K, d, I, h, sigma)\n",
    "    A = eta_div(np.transpose(omega)@Z[K] + mu)*omega \n",
    "    for k in range(K,0,-1): \n",
    "        B = h*sigma_div(W[k-1]@Z[k-1]+b[k-1])\n",
    "        u = B*A\n",
    "        A = A + np.transpose(W[k-1])@u\n",
    "    \n",
    "    return A[0:d0,:]\n",
    "\n",
    "def inverseScaleGrad(x_tilde, a, b, alpha = 0.2, beta = 0.8):\n",
    "    return 1/(beta-alpha)*(x_tilde*b - x_tilde*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Implementation of the numerical methods symplectic Euler and Størmer-Verlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def symplecticEuler(gradF, t, q0, p0, omega_V, mu_V, W_V, b_V, a_Vgrad, b_Vgrad, omega_T, mu_T, W_T, b_T, a_Tgrad, b_Tgrad, K, d, h, sigma, eta_div, sigma_div): \n",
    "\n",
    "    q = np.zeros((q0.shape[0], q0.shape[1],len(t)))\n",
    "    p = np.zeros((p0.shape[0], p0.shape[1],len(t)))\n",
    "    \n",
    "    q[:,:,0] = q0\n",
    "    p[:,:,0] = p0\n",
    "    \n",
    "    for n in range(len(t)-1):\n",
    "        delta_t = t[n+1]-t[n] \n",
    "        \n",
    "        dTdp = gradF(p[:,:,n], omega_T, mu_T, W_T, b_T, K, d, h, sigma, eta_div, sigma_div)\n",
    "        q[:,:,n+1] = q[:,:,n] + delta_t*inverseScaleGrad(dTdp,a_Tgrad,b_Tgrad)\n",
    "        \n",
    "        dVdq = gradF(q[:,:,n+1], omega_V, mu_V, W_V, b_V, K, d, h, sigma, eta_div, sigma_div)\n",
    "        p[:,:,n+1] = p[:,:,n] - delta_t*inverseScaleGrad(dVdq,a_Vgrad,b_Vgrad)\n",
    "    \n",
    "    return q,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Stromer_Verlet(gradF, t, q0, p0, omega_V, mu_V, W_V, b_V, a_Vgrad, b_Vgrad, omega_T, mu_T, W_T, b_T, a_Tgrad, b_Tgrad, K, d, h, sigma, eta_div, sigma_div): \n",
    "    \n",
    "    q = np.zeros((q0.shape[0], q0.shape[1], len(t)))\n",
    "    p = np.zeros((p0.shape[0], p0.shape[1], len(t)))\n",
    "    \n",
    "    q[:,:,0] = q0\n",
    "    p[:,:,0] = p0\n",
    "    \n",
    "    for n in range(len(t)-1):\n",
    "        delta_t = t[n+1]-t[n]\n",
    "        \n",
    "        dVdq_n = gradF(q[:,:,n], omega_V, mu_V, W_V, b_V, K, d, h, sigma, eta_div, sigma_div)\n",
    "        u = p[:,:,n] - delta_t/2*inverseScaleGrad(dVdq_n, a_Vgrad, b_Vgrad) #u = p_{n+1/2}\n",
    "        dTdp = gradF(u, omega_T, mu_T, W_T, b_T, K, d, h, sigma, eta_div, sigma_div)\n",
    "        q[:,:,n+1] = q[:,:,n] + delta_t*inverseScaleGrad(dTdp, a_Tgrad, b_Tgrad)\n",
    "        dVdq = gradF(q[:,:,n+1], omega_V, mu_V, W_V, b_V, K, d, h, sigma, eta_div, sigma_div)\n",
    "        p[:,:,n+1] = u - delta_t/2*inverseScaleGrad(dVdq, a_Vgrad, b_Vgrad)\n",
    "    \n",
    "    return q,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above the numerical methods symplectic Euler and Størmer-Verlet are implented, using the gradient computed from the trained neural networks. Both will be used to compute the trajectories of the Hamiltonian functions, however, since the sympletic Euler method only has order 1 while the Strømer-Verlet method has order 2, we expect the Strømer-Verlet method to achieve better results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Nonlinear pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I_p = 1000\n",
    "d_p = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = 1E-2\n",
    "l = 0.5\n",
    "g = 9.81\n",
    "\n",
    "def T_pend(p):\n",
    "    return 0.5*p**2\n",
    "\n",
    "def V_pend(q):\n",
    "    return m*g*l*(1-np.cos(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_Tpend,b_Tpend,omega_Tpend,mu_Tpend,ypsilon_Tpend, J_Tpend, itr_Tpend = readParams(K, d_p, I_p, N, filename='trainingParams_Tpend.txt')\n",
    "W_Vpend,b_Vpend,omega_Vpend,mu_Vpend,ypsilon_Vpend, J_Vpend, itr_Vpend = readParams(K, d_p, I_p, N, filename='trainingParams_Vpend.txt')\n",
    "\n",
    "aVp, bVp, aTp, bTp = readScale(\"scaleParamsPend.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the gradient of V(q) for the nonlinear pendelum using both the gradF and numpy.gradient\n",
    "q_p = np.linspace(-2,2,100)\n",
    "V_p = V_pend(q_p)\n",
    "\n",
    "gradV_numpy = np.gradient(V_p)\n",
    "\n",
    "q_p.resize((1,100))\n",
    "\n",
    "gradV = gradF(q_p, omega_Vpend, mu_Vpend, W_Vpend, b_Vpend, K, d_p, h, sigma, eta_div, sigma_div)\n",
    "gradV = inverseScaleGrad(gradV, aVp, bVp)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.set_figheight(4)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(q_p[0], gradV[0], label = \"Approximated gradient\")\n",
    "ax.plot(q_p[0], gradV_numpy, label = \"Numpy approximated gradient\")\n",
    "ax.set_xlabel(\"q\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(q_p[0], gradV_numpy, label = \"Numpy approximated gradient\")\n",
    "ax.set_xlabel(\"q\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "fig.suptitle(\"Comparison of approximated gradient with trained network and exact gradient of nonlinear pendulum\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figures above show a comparison of the approximated gradient calculated with the trained network, to the gradient found using the numpy gradient function for the nonlinear pendulum. As we can see on the left, the network's approximated gradient is wrong. On the left however, we can see that its shape seems to be correct.\n",
    "\n",
    "We were not able to correctly compute the gradient, so this error will propagate into the calculation of the trajectories. We chose to still find approximations of the trajectories to see whether the Hamiltonian properties were preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q0 = np.zeros((1,1))\n",
    "p0 = np.zeros((1,1))\n",
    "t = np.linspace(0,10,100)\n",
    "\n",
    "q_sympEuler_pend, p_sympEuler_pend = symplecticEuler(gradF, t, q0, p0, omega_Vpend, mu_Vpend, W_Vpend, b_Vpend, aVp, bVp, omega_Tpend, mu_Tpend, W_Tpend, b_Tpend, aTp, bTp, K, d_p, h, sigma, eta_div, sigma_div)\n",
    "\n",
    "q_StromerVerlet_pend, p_StromerVerlet_pend = Stromer_Verlet(gradF, t, q0, p0, omega_Vpend, mu_Vpend, W_Vpend, b_Vpend, aVp, bVp, omega_Tpend, mu_Tpend, W_Tpend, b_Tpend, aTp, bTp, K, d_p, h, sigma, eta_div, sigma_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing of T(p) and V(q) with p and q from the numerical methods\n",
    "p_sympEuler_pend.resize((1,100))\n",
    "q_sympEuler_pend.resize((1,100))\n",
    "T_net_sympEuler_pend = testingAlgorithm(p_sympEuler_pend, W_Tpend,b_Tpend,omega_Tpend,mu_Tpend, K, d_p, 100, h, sigma, eta, aTp, bTp)\n",
    "V_net_sympEuler_pend = testingAlgorithm(q_sympEuler_pend, W_Vpend,b_Vpend,omega_Vpend,mu_Vpend, K, d_p, 100, h, sigma, eta, aVp, bVp)\n",
    "\n",
    "p_StromerVerlet_pend.resize((1,100))\n",
    "q_StromerVerlet_pend.resize((1,100))\n",
    "T_net_StromerVerlet_pend = testingAlgorithm(p_StromerVerlet_pend, W_Tpend,b_Tpend,omega_Tpend,mu_Tpend, K, d_p, 100, h, sigma, eta, aTp, bTp)\n",
    "V_net_StromerVerlet_pend = testingAlgorithm(q_StromerVerlet_pend, W_Vpend,b_Vpend,omega_Vpend,mu_Vpend, K, d_p, 100, h, sigma, eta, aVp, bVp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTrajectories(t, V_net_sympEuler_pend, T_net_sympEuler_pend, V_net_StromerVerlet_pend, T_net_StromerVerlet_pend,\"nonlinear pendelum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to check whether the numerical solution preserves the Hamiltonian property of being constant along the trajectories. From the graphs we can see that the numerical solutions are not completely constant, however they are approximately constant seeing as the magnitude of the variation is of order $10^{-4}$ for the solution computed using the Symplectic Euler method and of order $10^{-6}$ for the solution computed using the Strømer-Verlet method. As expected the solution with the Strømer-Verlet method has a smaller variation. Additionally both numerical solutions take similar values which also is as expected, as the order of the method should not affect the value of the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Kepler two body "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I_K = 1500\n",
    "d_K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def T_Kepler(p):\n",
    "    T_p = np.zeros((1,p.shape[1]))\n",
    "    for i in range(p.shape[1]):\n",
    "        T_p[:,i] = 1/2*np.transpose(p[:,i])@p[:,i]\n",
    "    return T_p\n",
    "\n",
    "def V_Kepler(q1,q2):\n",
    "    return -1/np.sqrt(q1**2+q2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_TK,b_TK,omega_TK,mu_TK, ypsilon_TK, J_TK, itr_TK = readParams(K, d_K, I_K, N, filename='trainingParams_TKepler.txt')\n",
    "W_VK,b_VK,omega_VK,mu_VK, ypsilon_VK, J_VK, itr_VK = readParams(K, d_K, I_K, N, filename='trainingParams_VKepler.txt')\n",
    "\n",
    "aVK, bVK, aTK, bTK = readScale(\"scaleParamsKepler.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q0 = np.array([0.2,0.2])\n",
    "q0.resize((2,1))\n",
    "p0 = np.zeros((2,1))\n",
    "t = np.linspace(0,100,100)\n",
    "\n",
    "q_sympEulerK, p_sympEulerK = symplecticEuler(gradF, t, q0, p0, omega_VK, mu_VK, W_VK, b_VK, aVK, bVK, omega_TK, mu_TK, W_TK, b_TK, aTK, bTK, K, d_K, h, sigma, eta_div, sigma_div)\n",
    "\n",
    "q_StromerVerletK, p_StromerVerletK = Stromer_Verlet(gradF, t, q0, p0, omega_VK, mu_VK, W_VK, b_VK, aVK, bVK, omega_TK, mu_TK, W_TK, b_TK, aTK, bTK, K, d_K, h, sigma, eta_div, sigma_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing of T(p) and V(q) with p and q from the numerical methods\n",
    "p_sympEulerK.resize((2,100))\n",
    "q_sympEulerK.resize((2,100))\n",
    "T_net_sympEulerK = testingAlgorithm(p_sympEulerK, W_TK,b_TK,omega_TK,mu_TK, K, d_K, 100, h, sigma, eta, aTK, bTK)\n",
    "V_net_sympEulerK = testingAlgorithm(q_sympEulerK, W_VK,b_VK,omega_VK,mu_VK, K, d_K, 100, h, sigma, eta, aVK, bVK)\n",
    "\n",
    "p_StromerVerletK.resize((2,100))\n",
    "q_StromerVerletK.resize((2,100))\n",
    "T_net_StromerVerletK = testingAlgorithm(p_StromerVerletK, W_TK,b_TK,omega_TK,mu_TK, K, d_K, 100, h, sigma, eta, aTK, bTK)\n",
    "V_net_StromerVerletK = testingAlgorithm(q_StromerVerletK, W_VK,b_VK,omega_VK,mu_VK, K, d_K, 100, h, sigma, eta, aVK, bVK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTrajectories(t, V_net_sympEulerK, T_net_sympEulerK, V_net_StromerVerletK, T_net_StromerVerletK,\"Kepler two body problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graphs shown above we see that both numerical solutions to some extent approximate constancy along most of the trajectory, the solutions computed using the symplectic Euler and Størmer-Verlet methods vary with a magnitude of order $2\\cdot10^{-1}$ and $10^{-2}$ respectively. However, for both solutions there occurs a significant jump in the first timesteps which we cannot explain. Additionally we also see that the solutions take very different values, with the solution computed with symplectic Euler taking values between $-0.05$ and $0.25$ where it is approximately constant, and the solution computed with Størmer-Verlet taking values around $6.48$. This is unexpected since the choice of method should not affect the value of the solution and we are not able to explain why this happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchGrad = generate_data(40)\n",
    "q0 = batchGrad[\"Q\"][:,0]\n",
    "p0 = batchGrad[\"P\"][:,0]\n",
    "\n",
    "d0 = q0.shape[0]\n",
    "\n",
    "q0.resize((d0,1))\n",
    "p0.resize((d0,1))\n",
    "\n",
    "tU = batchGrad[\"t\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_TT,b_TT,omega_TT,mu_TT, ypsilon_TT, J_TT, itr_TT = readParams(K, d_T, batchsize, N, filename='trainingParams_TUnknown.txt')\n",
    "W_VT,b_VT,omega_VT,mu_VT, ypsilon_VT, J_VT, itr_VT = readParams(K, d_T, batchsize, N, filename='trainingParams_VUnknown.txt')\n",
    "\n",
    "aVT, bVT, aTT, bTT = readScale(\"scaleParamsUnknown.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_sympEulerU, p_sympEulerU = symplecticEuler(gradF, tU, q0, p0, omega_VT, mu_VT, W_VT, b_VT, aVT, bVT, omega_TT, mu_TT, W_TT, b_TT, aTT, bTT, K, d_T, h, sigma, eta_div, sigma_div)\n",
    "\n",
    "q_StromerVerletU, p_StromerVerletU = Stromer_Verlet(gradF, tU, q0, p0, omega_VT, mu_VT, W_VT, b_VT, aVT, bVT, omega_TT, mu_TT, W_TT, b_TT, aTT, bTT, K, d_T,h, sigma, eta_div, sigma_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing of T(p) and V(q) with p and q from the numerical methods\n",
    "p_sympEulerU.resize((d0,len(tU)))\n",
    "q_sympEulerU.resize((d0,len(tU)))\n",
    "T_net_sympEulerU = testingAlgorithm(p_sympEulerU, W_TT,b_TT,omega_TT,mu_TT, K, d_T, len(tU), h, sigma, eta, aTT, bTT)\n",
    "V_net_sympEulerU = testingAlgorithm(q_sympEulerU, W_VT,b_VT,omega_VT,mu_VT, K, d_T, len(tU), h, sigma, eta, aVT, bVT)\n",
    "\n",
    "p_StromerVerletU.resize((d0,len(tU)))\n",
    "q_StromerVerletU.resize((d0,len(tU)))\n",
    "T_net_StromerVerletU = testingAlgorithm(p_StromerVerletU, W_TT,b_TT,omega_TT,mu_TT, K, d_T, len(tU), h, sigma, eta, aTT, bTT)\n",
    "V_net_StromerVerletU = testingAlgorithm(q_StromerVerletU, W_VT,b_VT,omega_VT,mu_VT, K, d_T, len(tU), h, sigma, eta, aVT, bVT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTrajectories(tU, V_net_sympEulerU, T_net_sympEulerU, V_net_StromerVerletU, T_net_StromerVerletU,\"unknown Hamiltonian function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The graphs above again show that the Hamiltonian property is not exactly preserved as the graphs are only approximately constant, however the magnitude of variation is realtively small here as well. The solutions computed using the symplectic Euler and Størmer-Verlet methods vary with a magnitude of order $10^{-3}$ and $10^{-6}$ respectively. Again we see that the magnitude of the variation is smaller for the Strømer-Verlet method than for the sympletic Euler method. Here we also again see that both numerical solutions take approximately the same values as we would expect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
