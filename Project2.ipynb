{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "    <center>\n",
    "    TMA4215 Project 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "#### 1. Introduction\n",
    "#### 2. Algorithm \n",
    "#### 3. Deciding parameters\n",
    "\n",
    "3.1 The batchsize \n",
    "\n",
    "3.2 The number of hidden layers, $K$\n",
    "\n",
    "3.3 The value of the learning parameter, $\\tau$\n",
    "\n",
    "3.4 The dimension of the input data in the hidden layers $d$\n",
    "\n",
    "3.5 The stepsize $h$\n",
    "\n",
    "3.6 Conclusion\n",
    "\n",
    "\n",
    "#### 4. Training and testing with suggested functions\n",
    "4.1 $F(y) = 1-\\cos(y)$\n",
    "\n",
    "4.2 $G(y_1, y_2) = \\frac{1}{2}(y_1^2 + y_2^2)$\n",
    "\n",
    "4.3 Known Hamiltonian\n",
    "\n",
    "4.3 Unknown Hamiltonian\n",
    "\n",
    "#### 5. Determine the gradient \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to use a neural network to train approximations of Hamiltonian function, derive and implement formulas for computing the gradient of the trained function and use those to implement symplectic Euler and the Størmer-Verlet method for the Hamiltonian function. \n",
    "\n",
    "This report starts with presenting the training algorithm and arguing for the choices of the parameters $K$, $\\tau$, $d$ ang $h$ used in this algorithm. Furthermore, the training prosedure is implemented for known functions and Hamiltonians and the efficienty of this training is determined by comparing the values obtained by using the trained weights and biases to the accurate function values. For unknown Hamiltonian.... The report brifly determine a formula for computing of gradients and define the symplectic Euler and Størmer-Verlet method. ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import isclose\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_2_data_acquisition import generate_data, concatenate\n",
    "from files import writeParams, readParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above are imported from separate files and are used to generate input data batches from comma seperated files and to write our trained values to a file as well as reading these values from a file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is divided into several smaller codeblocks with the purpose of making the code easy to follow. Principally, the algorithm's parts is to transform input data between the layers in the network, decide the gradients of the objective function, $J = \\frac{1}{2} \\|Z-c\\| $, with respect to weights, biases, $\\omega$ and $\\mu$ and optimalize the network with respect to those. Additionally, several utility functions is defined to be used in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getW(K,d):\n",
    "    w = np.random.randn(K,d,d)\n",
    "    return w\n",
    "\n",
    "def getb(K,d):\n",
    "    b = np.random.randn(K,d,1)\n",
    "    return b\n",
    "\n",
    "def getomega(d):\n",
    "    omega = np.random.randn(d,1)\n",
    "    return omega\n",
    "\n",
    "def getmu():\n",
    "    mu = np.random.randn(1)\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above generate random initial values, drawn from a standard normal distribution, to the weights, $W_k$, the biases, $b_k$, $\\omega$ and $\\mu$. These random values causes some marginal differences between runs of the same dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigma_div(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def eta(x):\n",
    "    return 1/2*(1+np.tanh(x/2))\n",
    "\n",
    "def eta_div(x):\n",
    "    return 1/(4*np.cosh(x/2)**2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the activating function $\\sigma$, which is used in the transformation $\\Phi_k: Z_k \\rightarrow Z_{k+1}$, and the function $\\eta$, used in projection from last layer on a scalar $z$, is defined. The derivatives of $\\sigma$ and $\\eta$ is also defined and will be used in calculations of the gradient of the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getZ(Y0, W, b, K, d, I, h, sigma):\n",
    "    #initialize Z, Z0=Y0\n",
    "    Z = np.zeros((K+1,d,I))\n",
    "    Z[0] = Y0\n",
    "\n",
    "    #finds Zk\n",
    "    for k in range(1,K+1):\n",
    "        u = W[k-1]@Z[k-1] + b[k-1]\n",
    "        Z[k] = Z[k-1] + h*sigma(u)\n",
    "\n",
    "    return Z\n",
    "\n",
    "def getP(Z, ypsilon, c, omega, mu, W, b, K, d, I, h, sigma_div, eta_div):\n",
    "    #initialize P\n",
    "    P = np.zeros((K+1,d,I))\n",
    "\n",
    "    #finds P_K\n",
    "    u = np.transpose(Z[-1])@omega + mu\n",
    "    P[-1]= omega@np.transpose((ypsilon-c)*eta_div(u))\n",
    "\n",
    "    #finds P_K-1 to P_0\n",
    "    for k in range(K-1,0,-1):\n",
    "        s = W[k]@Z[k] + b[k]\n",
    "        P[k]=P[k+1] + h*np.transpose(W[k])@(sigma_div(s)*P[k+1])\n",
    "        \n",
    "    return P\n",
    "\n",
    "\n",
    "def getYpsilon(Z, omega, mu, K, eta):\n",
    "    u = np.transpose(Z[K]) @ omega + mu\n",
    "    return eta(u)\n",
    "\n",
    "def getJ(ypsilon, c):\n",
    "    return 1/2 * np.linalg.norm(ypsilon-c)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Defines the functions getZ and getP, where getZ executes linear transformations $\\Phi_k: Z_k \\rightarrow Z_{k+1}$ based on the weights and biases and returns the matrix $Z$ and getP returns an utility matrix used in calulations of derivatives of the objective function with respect to the weight and biases. Furthermore, getYpsilon is defined and returns a vector of the function values in the last layer of the network and the objective funksjon $J$ is defined as a measure of the difference between the resulting trained values and the exact function values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdJdmu(Z, ypsilon, c, omega, mu, K, eta_div):\n",
    "    u = np.transpose(Z[K])@omega + mu\n",
    "    dJdmu = np.transpose(eta_div(u))@(ypsilon-c)\n",
    "    return dJdmu\n",
    "\n",
    "def getdJdomega(Z, ypsilon, c, omega, mu, K, eta_div):\n",
    "    u = np.transpose(Z[K])@omega + mu\n",
    "    dJdOmega = Z[K] @ ((ypsilon - c) * eta_div(u))\n",
    "    return dJdOmega\n",
    "\n",
    "def getdJdW(P, Z, W, b, K, h, sigma_div):\n",
    "    dJdW = np.zeros_like(W)\n",
    "    for k in range(K):\n",
    "        u = W[k]@Z[k]+b[k]\n",
    "        dJdW[k] = h*(P[k+1]*sigma_div(u))@ np.transpose(Z[k])  \n",
    "    return dJdW\n",
    "\n",
    "def getdJdb(P, Z, W, b, K, h, sigma_div):\n",
    "    dJdb = np.zeros_like(b)\n",
    "    for k in range(K):\n",
    "        u = W[k]@Z[k]+b[k]\n",
    "        dJdb[k] = h*(P[k+1]*sigma_div(u))@np.ones((Z.shape[2],1))\n",
    "    return dJdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Functions to obtain the derivatives of the objective function with respect to weights and biased is defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plain(theta, tau, dJdtheta):\n",
    "    return theta - tau*dJdtheta\n",
    "\n",
    "def adam(theta,alpha,m,v,g,i):\n",
    "    #parameters\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 10E-8\n",
    "\n",
    "    #m,v\n",
    "    m = beta1*m + (1-beta1)*g\n",
    "    v = beta2*v + (1-beta2)*np.multiply(g,g)\n",
    "\n",
    "    m_hat = np.multiply(m,1/(1-beta1**i))\n",
    "    v_hat = np.multiply(v,1/(1-beta2**i))\n",
    "\n",
    "    #update\n",
    "    R = alpha*np.multiply(m_hat,1/(np.sqrt(v_hat)+epsilon))\n",
    "\n",
    "    return theta-R,m,v\n",
    "\n",
    "def getMandV(theta):\n",
    "    return np.zeros_like(theta),np.zeros_like(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above is used to optimize the weights, biases, $\\omega$ and $\\mu$ in different ways, which will be compared later on in this report.\n",
    "\n",
    "In plain vanilla gradient descent, one follows the gradient in decreasing direction and the learning parameter $\\tau$ determine how far one shall follow it. \n",
    "\n",
    "In Adam descent method, one follows the gradient with different length depending on different parameters. The parameters $m$ and $v$ represent respectively a kind of mean value and squared mean value of the previous iterations gradients. The function getMandV initialize $m$ and $v$ to be either zero, the zerovector or the zeromatrix depending on inputdata's dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(x, alpha=0.2, beta=0.8):\n",
    "    a = np.amin(x)\n",
    "    b = np.amax(x)\n",
    "    \n",
    "    x_tilde = 1/(b-a)*((b-x)*alpha + (x-a)*beta)\n",
    "    return x_tilde, a, b\n",
    "\n",
    "def inverseScale(x_tilde, a, b, alpha=0.2, beta=0.8):\n",
    "    return 1/(beta-alpha)*((x_tilde-alpha)*b + (beta-x_tilde)*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to scale inputdata in terms of a min-max transformation, which guarantees that the data have all its components in the interval $[\\alpha, \\beta]$, is defined. This is desirable since some functions is acting component-wise on other functions....?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def successrate(ypsilon, c, tol):\n",
    "    sum = 0\n",
    "    for i in range(ypsilon.shape[0]):\n",
    "        if isclose(ypsilon[i][0], c[0][i], abs_tol = tol):\n",
    "            sum +=1\n",
    "    return sum/ypsilon.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si noe fornuftig her etterhver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingAlgorithm(K, d, h, tau, Y0, c0, eta, sigma, eta_div, sigma_div, N, tol, chunksize, optimization):\n",
    "    #finds input shape\n",
    "    d0 = Y0.shape[0]\n",
    "    I0 = Y0.shape[1]\n",
    "    \n",
    "    #reshapes input to match dimension of layers\n",
    "    if d0 > d:\n",
    "        print(\"d must be larger than d0\")\n",
    "    \n",
    "    if d0 < d:\n",
    "        zero = np.zeros((d-d0,I0))\n",
    "        Y0 = np.vstack((Y0,zero))\n",
    "    \n",
    "    #gets initial weigths\n",
    "    omega = getomega(d)\n",
    "    mu = getmu()\n",
    "    W = getW(K,d)\n",
    "    b = getb(K,d)\n",
    "    \n",
    "    c0 = np.transpose(c0)\n",
    "  \n",
    "    if optimization == \"adam\":\n",
    "        #initial m,v for adam descent\n",
    "        mmu,vmu = getMandV(mu)\n",
    "        momega,vomega =getMandV(omega)\n",
    "        mW,vW = getMandV(W)\n",
    "        mb,vb = getMandV(b)\n",
    "    \n",
    "    #initializes vector to store objective function values\n",
    "    J = np.zeros(N)\n",
    "    ypsilon = np.zeros_like(c0)\n",
    "    \n",
    "    for i in range(N):\n",
    "               \n",
    "        #stochastic gradient descent\n",
    "        if I0 == chunksize:\n",
    "            Y0_chunk = Y0\n",
    "            c_chunk = c0\n",
    "        elif I0 > chunksize:\n",
    "            s = np.random.randint(0,I0-chunksize)\n",
    "            Y0_chunk = Y0[:,s:(s+chunksize)]\n",
    "            c_chunk=c0[s:(s+chunksize),:]\n",
    "        else:\n",
    "            print(\"chunksize must be smaller than I\")       \n",
    "        \n",
    "\n",
    "        #transformations between layers\n",
    "        Z = getZ(Y0_chunk, W, b, K, d, chunksize, h, sigma)\n",
    "        ypsilon = getYpsilon(Z, omega, mu, K, eta)\n",
    "        P = getP(Z, ypsilon, c_chunk, omega, mu, W, b, K, d, chunksize, h, sigma_div, eta_div)\n",
    "        \n",
    "        #objective function\n",
    "        J[i] = getJ(ypsilon, c_chunk)\n",
    "        \n",
    "        #finds gradients\n",
    "        dJdmu = getdJdmu(Z, ypsilon, c_chunk, omega, mu, K, eta_div)\n",
    "        dJdomega = getdJdomega(Z, ypsilon, c_chunk, omega, mu, K, eta_div)\n",
    "        dJdW = getdJdW(P, Z, W, b, K, h, sigma_div)\n",
    "        dJdb = getdJdb(P, Z, W, b, K, h, sigma_div)\n",
    "        \n",
    "        if optimization == \"plain\":\n",
    "            mu = plain(mu, tau, dJdmu)\n",
    "            omega = plain(omega, tau, dJdomega)\n",
    "            W = plain(W, tau, dJdW)\n",
    "            b = plain(b, tau, dJdb)\n",
    "        \n",
    "        elif optimization == \"adam\":\n",
    "            mu, mmu,vmu = adam(mu,tau,mmu,vmu,dJdmu,i+1)\n",
    "            omega, momega,vomega = adam(omega,tau,momega,vomega,dJdomega,i+1)\n",
    "            W, mW,vW = adam(W,tau,mW,vW,dJdW,i+1)\n",
    "            b, mb,vb = adam(b,tau,mb,vb,dJdb,i+1)\n",
    "        \n",
    "        if (J[i]/chunksize) < tol:\n",
    "            break\n",
    "        \n",
    "    return mu, omega, W, b, J, ypsilon, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training algorithm returns trained values of the weights, biases, $\\omega$ and $\\mu$ as well as the objective function and $\\Upsilon$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingAlgorithm(yTest, W, b, omega, mu, K, d, I, h, sigma, eta, a_scale, b_scale):\n",
    "    \n",
    "    #scaling y\n",
    "    yTilde, ay, by = scale(yTest)\n",
    "    \n",
    "    #finds input shape\n",
    "    d0 = yTest.shape[0]\n",
    "    \n",
    "    if d > d0:\n",
    "        zero = np.zeros((d-d0,I))\n",
    "        yTilde = np.vstack((yTilde,zero))\n",
    "    \n",
    "    zTest = getZ(yTilde, W, b, K, d, I, h, sigma)\n",
    "    ypsilonTilde = getYpsilon(zTest, omega, mu, K, eta)\n",
    "\n",
    "    #rescaling of input\n",
    "    ypsilonTest = inverseScale(ypsilonTilde, a_scale, b_scale)\n",
    "\n",
    "    return ypsilonTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si litt om testingsalgoritma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deciding parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decidingParameters import filenameList, plotParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch0 = generate_data(0)\n",
    "\n",
    "p0_tilde ap0, bp0 = scale(batch0['P'])\n",
    "T0_tilde, aT0, bT0 = scale(batch0['T'])\n",
    "T0_tilde.resize(1,T0_tilde.shape[0])\n",
    "\n",
    "N = 5000\n",
    "tol = 1E-10 \n",
    "I_0 = p0_tilde.shape[1]\n",
    "batchsize = I_0\n",
    "d_0 = p0_tilde.shape[0]    \n",
    "h = 0.1     \n",
    "tau = 0.001  \n",
    "K = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, optimal choices for the parameters $K$, $\\tau$, $d$ and $h$ will be investigated. All the following tests are based on the first batch of values, enclosed to the project description, for the unknown Hamiltonian function. The number for training series is chosen to be 5000 for all the tests, as it makes it possible to illustrate the development of each parameter's impact on the training process. The tolerance is set sufficiently small so that one can guarantee that the training algorithm does not obtain a tolerated value for the objective function within the 5000 training series. \n",
    "\n",
    "The initial values of the parameters is set to be $K = 50$, $\\tau = 0.001$, $d = 3$ and $h =0.1$. After each investigation the value of the current parameter will be updated to the investigated optimal choice and the updated value is used in the remaining tests.\n",
    "\n",
    "The investigations include graphs for both implementation of the adam descent and plain vanilla gradient descent, but our choices of parameters will in all cases be made based on Adam descent, knowing this method is more efficient than plain vanilla gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 The batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variables for this test\n",
    "batchsize_list = np.arange(I_0/4, I_0+1,I_0/4).astype(int) \n",
    "\n",
    "filenames_batchsize_A, filenames_batchsize_P = filenameList(batchsize_list, \"batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(batchsize_list)):\n",
    "    batchsize = batchsize_list[i]\n",
    "    #train for different batchsizes with adam descent \n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_Abatch, ypsilon_A0, itr_A0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Abatch, itr_A0, filename = filenames_batchsize_A[i])\n",
    "    \n",
    "    #train for different batchsizes with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_Pbatch, ypsilon_P0, itr_P0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Pbatch, itr_P0, filename = filenames_batchsize_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotParams(batchsize_list, \"batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs illustrate that the use of stochastic gradient descent makes the objective function oscillates around a minimum. The smaller the batchsize, the more oscillations?? For half the batchsize, in this case $2048$, these oscillations are about one order of magnitude and after $N=4000$ training series the value of the objective function is of magnitude $10^{-4}$. Since a smaller batchsize will reduce the run-time, will we use the stochastic gradient descent in cases with large original inputsize and use about half of the total inputdatasize as inputdata for the training process. For full batchsize, the objective function converges to its minimum. This is favourable for the investigations of the parameters $K$, $\\tau$ and $d$ (since ??) and full batchsize will therefore be used in the investigations in this section. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "#plot adam\n",
    "ax = axs[0]\n",
    "for i in range(len(batchsize_list)):\n",
    "    batchsize = batchsize_list[i]\n",
    "    W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Abatch, itr_A0 = readParams(K, d_0, batchsize, N, filename = filenames_batchsize_A[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_Abatch/batchsize, label = r\"$batchsize ={}$\".format(batchsize))\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "#plot plain\n",
    "ax = axs[1]\n",
    "for i in range(len(batchsize_list)):\n",
    "    batchsize = batchsize_list[i]\n",
    "    W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Pbatch, itr_P0 = readParams(K, d_0, batchsize, N, filename = filenames_batchsize_P[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_Pbatch/batchsize, label = r\"$batchsize ={}$\".format(batchsize))\n",
    "ax.set_title(r\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#define lists of filenames where the trained parameters will be stored \n",
    "filenames_batchsize_A = []\n",
    "filenames_batchsize_P = []\n",
    "for i in range(len(batchsize_list)):\n",
    "    batchsize = batchsize_list[i]\n",
    "    filenames_batchsize_A.append('trainingParams_Abatch{}'.format(batchsize))\n",
    "    filenames_batchsize_P.append('trainingParams_Pbatch{}'.format(batchsize))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 The number of hidden layers, $K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variables for this test   \n",
    "K_list = np.arange(25,101,25) \n",
    "\n",
    "#define lists of filenames where the trained parameters will be stored\n",
    "filenames_K_A, filenames_K_P = filenameList(K_list, \"K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(K_list)):\n",
    "    K = K_list[i]\n",
    "    #train for different K values with adam descent\n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_AK, ypsilon_A0, itr_A0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_AK, itr_A0, filename = filenames_K_A[i])\n",
    "    \n",
    "    #train for different K values with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_PK, ypsilon_P0, itr_P0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_PK, itr_P0, filename = filenames_K_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotParams(K_list, \"K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "#plot adam\n",
    "ax = axs[0]\n",
    "for i in range(len(K_list)):\n",
    "    K = K_list[i]\n",
    "    W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_AK, itr_A0 = readParams(K, d_0, batchsize, N, filename = filenames_K_A[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_AK/batchsize, label = r\"$K ={}$\".format(K))\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "#plot plain\n",
    "ax = axs[1]\n",
    "for i in range(len(K_list)):\n",
    "    K = K_list[i]\n",
    "    W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_PK, itr_P0 = readParams(K, d_0, batchsize, N, filename = filenames_K_P[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_PK/batchsize, label = r\"$K ={}$\".format(K))\n",
    "ax.set_title(r\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated above, the different number of hidden layers seems to behave relatively similar for training on this inputdata. Therefore $K=50$ is chosen as an optimal value, both with respect to the fact that it looks like the best option from the graph and due to the small differences between the number of layers it is not desireable to choose a larger numbers since it will increase the running time substantially. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 The value of the learning parameter $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variables for this test\n",
    "batchsize = I_0\n",
    "d_0 = p0_tilde.shape[0]      #dimension of layers for F(y)\n",
    "h = 0.1      #stepsize\n",
    "tau_list = np.linspace(0.001, 0.01, 4)   #learning parameter\n",
    "K = 50 \n",
    "\n",
    "#define lists of filenames where the trained parameters will be stored\n",
    "filenames_tau_A = []\n",
    "filenames_tau_P = []\n",
    "for i in range(len(tau_list)):\n",
    "    tau = tau_list[i]\n",
    "    filenames_tau_A.append('trainingParams_Atau_{}'.format(tau))\n",
    "    filenames_tau_P.append('trainingParams_Ptau_{}'.format(tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tau_list)):\n",
    "    tau = tau_list[i]\n",
    "\n",
    "    #train for different tau values with adam descent\n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_Atau, ypsilon_A0, itr_A0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Atau, itr_A0, filename = filenames_tau_A[i])\n",
    "    \n",
    "    #train for different tau values with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_Ptau, ypsilon_P0, itr_P0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Ptau, itr_P0, filename = filenames_tau_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "#plot adam\n",
    "ax = axs[0]\n",
    "for i in range(len(tau_list)):\n",
    "    tau = tau_list[i]\n",
    "    W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Atau, itr_A0 = readParams(K, d_0, batchsize, N, filename = filenames_tau_A[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_Atau/batchsize, label = r\"$\\tau =%.4f$\"%tau)\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "#plot plain\n",
    "ax = axs[1]\n",
    "for i in range(len(tau_list)):\n",
    "    tau = tau_list[i]\n",
    "    W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Ptau, itr_P0 = readParams(K, d_0, batchsize, N, filename = filenames_tau_P[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_Ptau/batchsize, label = r\"$\\tau =%.4f$\"%tau)\n",
    "ax.set_title(r\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "By observing the graph, the optimal value of the learning parameter is chosen to be $\\tau = 0.004$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 The dimension of the input data in the hidden layers $d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines variable for this test\n",
    "batchsize = I_0\n",
    "d_0_list = np.array([3,4,5,6])  \n",
    "h = 0.1      \n",
    "tau = 0.004 \n",
    "K = 50 \n",
    "\n",
    "#define lists of filenames where the trained parameters will be stored\n",
    "filenames_d_A = []\n",
    "filenames_d_P = []\n",
    "for i in range(len(d_0_list)):\n",
    "    d0 = d_0_list[i]\n",
    "    filenames_d_A.append('trainingParams_Ad_{}'.format(d0))\n",
    "    filenames_d_P.append('trainingParams_Pd_{}'.format(d0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(d_0_list)):\n",
    "    d0 = d_0_list[i]\n",
    "    \n",
    "    #train for different values of d with adam descent\n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_Ad0, ypsilon_A0, itr_A0 = trainingAlgorithm(K, d0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Ad0, itr_A0, filename = filenames_d_A[i])\n",
    "    \n",
    "    #train for different values of d with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_Pd0, ypsilon_P0, itr_P0 = trainingAlgorithm(K, d0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Pd0, itr_P0, filename = filenames_d_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "#plot adam\n",
    "ax = axs[0]\n",
    "for i in range(len(d_0_list)):\n",
    "    d0 = d_0_list[i]\n",
    "    W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Ad, itr_A0 = readParams(K, d0, batchsize, N, filename = filenames_d_A[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_Ad/batchsize, label = r\"$d ={}$\".format(d0))\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "#plot plain\n",
    "ax = axs[1]\n",
    "for i in range(len(d_0_list)):\n",
    "    d0 = d_0_list[i]\n",
    "    W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Pd, itr_P0 = readParams(K, d0, batchsize, N, filename = filenames_d_P[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_Pd/batchsize, label = r\"$d ={}$\".format(d0))\n",
    "ax.set_title(r\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs illustrate that the value of the objective function will have spikes in the cases where the dimension of the inputdata in the hidden layers is $d=3$, $d=5$ and $d=6$, and these spikes which can give values greater than the value for $d=4$. The dimension of the input data in the hidden layers is therefore chosen to be $d=4$ (where the input data originally has the dimension $d_0 = 3$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 The stepsize $h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variables for this test\n",
    "batchsize1 = I_0\n",
    "batchsize2 = int(I_0*0.5)\n",
    "d_0 = 4\n",
    "h_list = np.array([0.05, 0.1, 0.15, 0.2])\n",
    "tau = 0.004 \n",
    "K = 50 \n",
    "\n",
    "#define lists of filenames where the trained parameters will be stored\n",
    "filenames_h_A1 = []\n",
    "filenames_h_A2 = []\n",
    "filenames_h_P1 = []\n",
    "filenames_h_P2 = []\n",
    "for i in range(len(h_list)):\n",
    "    h = h_list[i]\n",
    "    filenames_h_A1.append('trainingParams_Ah1_{}'.format(h))\n",
    "    filenames_h_A2.append('trainingParams_Ah2_{}'.format(h))\n",
    "    filenames_h_P1.append('trainingParams_Ph1_{}'.format(h))\n",
    "    filenames_h_P2.append('trainingParams_Ph2_{}'.format(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(h_list)):\n",
    "    h = h_list[i]\n",
    "    #train for different stepsizes h with adam descent\n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_Ah1, ypsilon_A0, itr_A0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize1, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Ah1, itr_A0, filename = filenames_h_A1[i])\n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_Ah2, ypsilon_A0, itr_A0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize2, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Ah2, itr_A0, filename = filenames_h_A2[i])\n",
    "    \n",
    "    \n",
    "    #train for different stepsizes h with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_Ph1, ypsilon_P0, itr_P0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize1, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Ph1, itr_P0, filename = filenames_h_P1[i])\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_Ph2, ypsilon_P0, itr_P0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize2, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Ph2, itr_P0, filename = filenames_h_P2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "ax = axs[0]\n",
    "\n",
    "#plot adam\n",
    "for i in range(len(h_list)):\n",
    "    h = h_list[i]\n",
    "    W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Ah1, itr_A0 = readParams(K, d_0, batchsize1, N, filename = filenames_h_A1[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_Ah1/batchsize1, label = r\"$h ={}$\".format(h))\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "#plot plain\n",
    "for i in range(len(h_list)):\n",
    "    h = h_list[i]\n",
    "    W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Ph1, itr_P0 = readParams(K, d_0, batchsize1, N, filename = filenames_h_P1[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_Ph1/batchsize1, label = r\"$h ={}$\".format(h))\n",
    "ax.set_title(r\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "\n",
    "#half the batchsize\n",
    "fig2, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig2.set_figheight(7)\n",
    "fig2.set_figwidth(10)\n",
    "ax1 = axs[0]\n",
    "\n",
    "#plot adam\n",
    "for i in range(len(h_list)):\n",
    "    h = h_list[i]\n",
    "    W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Ah2, itr_A0 = readParams(K, d_0, batchsize2, N, filename = filenames_h_A2[i])\n",
    "    ax1.plot(np.linspace(0,N,N), J_Ah2/batchsize2, label = r\"$h ={}$\".format(h))\n",
    "ax1.set_title(\"Adam\")\n",
    "ax1.set_yscale(\"log\")\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "ax1 = axs[1]\n",
    "#plot plain\n",
    "for i in range(len(h_list)):\n",
    "    h = h_list[i]\n",
    "    W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Ph2, itr_P0 = readParams(K, d_0, batchsize2, N, filename = filenames_h_P2[i])\n",
    "    ax1.plot(np.linspace(0,N,N), J_Ph2/batchsize2, label = r\"$h ={}$\".format(h))\n",
    "ax1.set_title(r\"Plain\")\n",
    "ax1.set_yscale(\"log\")\n",
    "ax1.legend()\n",
    "      \n",
    "    \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the different values of the stepsize seems to behave relatively similar for the first 2500 training series, if the number of training series exceeds this number, its seems to be spikes that can be due to overfitting?? The optimal value of the stepsize is chosen to be $h=0.1$, since ... hopefully the training will obtain a tolerated value of the objective function within the first 3000 training series, and the overfitting problem will not occur??????????? feilen er veldig lav, er det bra eller dårliG??"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "HAR IKKE NEVNT NOE MED OVERFITTING NOEN STEDER PER NÅ, HVOR BØR DET NEVNES? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-grafene viser at adam er bedre enn plain overalt\n",
    "\n",
    "-valg av verdier oppsumert\n",
    "\n",
    "-om man skulle vist den endelige objective function grafen med disse verdiene??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training and testing with suggested functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines global variables for all the following tests\n",
    "h = 0.2       #stepsize\n",
    "tau = 0.004   #learning parameter\n",
    "tol = 1e-5  #tolerance\n",
    "N = 5000      #number of training series\n",
    "K = 50        #number of hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1  $F(y) = 1-\\cos{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(y):\n",
    "    return 1-np.cos(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variable used for training with F(y)\n",
    "d_F = 4        \n",
    "I_F = 1000 \n",
    "\n",
    "#define and scale input data ....AND FASIT\n",
    "Y0_F = np.random.uniform(-2,2,(1,I_F))\n",
    "c_F = F(Y0_F)\n",
    "Y0_F_tilde aY_f, bY_F =scale(Y0_F)\n",
    "c_F_tilde, ac_F, bc_F = scale(c_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training for F(y)\n",
    "mu_F, omega_F, W_F, b_F, J_F, ypsilon_F, itr_F = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_F, \"adam\")\n",
    "\n",
    "#stores the trained parameters in file\n",
    "writeParams(W_F, b_F, omega_F, mu_F,ypsilon_F, J_F, itr_F, filename = 'trainingParams_F.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the stored trained parameters\n",
    "W_Fr,b_Fr,omega_Fr,mu_Fr,ypsilon_Fr, J_F, itr_F = readParams(K, d_F, I_F, N, filename='trainingParams_F.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "plt.plot(np.linspace(0,itr_F,itr_F), J_F[:itr_F]/I_F)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cellen over gjøres om til en funkjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with F(y)\n",
    "YTest_F = np.random.uniform(-2,2,(1,I_F))\n",
    "cTest_F = F(YTest_F)\n",
    "\n",
    "\n",
    "ypsilonTest_F = testingAlgorithm(YTest_F, cTest_F, W_F, b_F, omega_F, mu_F, K, d_F, I_F, h, sigma, eta)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(YTest_F[0], ypsilonTest_F, \"*\", label = \"test\")\n",
    "plt.plot(YTest_F[0], cTest_F[0], \"*\", label = \"fasit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 $G(y_1,y_2) = \\frac{1}{2} (y_1^2 + y_2^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G(y1, y2):\n",
    "    return 1/2 *(y1**2 + y2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variable used for training with G(y)\n",
    "d_G = 4       \n",
    "I_G = 1000    \n",
    "\n",
    "#training with G(y)\n",
    "Y0_G = np.random.uniform(-2,2,(2,I_G))\n",
    "c_G = G(Y0_G[0],Y0_G[1])\n",
    "c_G.resize(1,I_G)\n",
    "\n",
    "Y0_G_tilde = Y0_G\n",
    "c_G_tilde, ac_G, bc_G = scale(c_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training for G(y)\n",
    "mu_G, omega_G, W_G, b_G, J_G, ypsilon_G, itr_G = trainingAlgorithm(K, d_G, h, tau, Y0_G_tilde, c_G_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_G, \"adam\")\n",
    "writeParams(W_G, b_G, omega_G, mu_G, ypsilon_G, J_G, itr_G, filename = 'trainingParams_G.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Gr,b_Gr,omega_Gr,mu_Gr, ypsilon_Gr, J_G, itr_G = readParams(K, d_G, I_G, N, filename='trainingParams_G.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting of objective function\n",
    "plt.plot(np.linspace(0,itr_G,itr_G), J_G[:itr_G]/I_G)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with G(y)\n",
    "YTest_G = np.random.uniform(-2,2,(2,I_G))\n",
    "cTest_G = G(YTest_G[0], YTest_G[1])\n",
    "cTest_G.resize(1,I_G)\n",
    "\n",
    "ypsilonTest_G = testing(YTest_G, cTest_G, W_G, b_G, omega_G, mu_G, K, d_G, I_G, h, sigma, eta)\n",
    "\n",
    "\n",
    "#plotting of G(y)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(YTest_G[0], YTest_G[1], ypsilonTest_G, label = \"test\", depthshade = True)\n",
    "ax.scatter(YTest_G[0], YTest_G[1], cTest_G[0], label = \"test\" , c=\"red\", depthshade = False)\n",
    "plt.show()\n",
    "\n",
    "x = np.arange(0, I_G)\n",
    "plt.plot(x, ypsilonTest_G, label =\"test\")\n",
    "plt.plot(x, cTest_G[0], label = \"fasit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 4.3 Known Hamiltonian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T_Kepler(p):\n",
    "    T_p = np.zeros((1,p.shape[1]))\n",
    "    for i in range(p.shape[1]):\n",
    "        T_p[:,i] = 1/2*np.transpose(p[:,i])@p[:,i]\n",
    "    return T_p\n",
    "\n",
    "def V_Kepler(q1,q2):\n",
    "    return -1/np.sqrt(q1**2+q2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_K = 1500\n",
    "d_K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_K = np.random.uniform(0,10, (2,I_K))\n",
    "T_K = T_Kepler(p_K)\n",
    "T_K_tilde, aTK, bTK = scale(T_K)\n",
    "\n",
    "q_K = np.random.uniform(0.01,10, (2,I_K))\n",
    "V_K = V_Kepler(q_K[0],q_K[1])\n",
    "V_K.resize(1,I_K)\n",
    "V_K_tilde, aVK, bVK = scale(V_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training T\n",
    "mu_TK, omega_TK, W_TK, b_TK, J_TK, ypsilon_TK, itr_TK = trainingAlgorithm(K, d_K, h, tau, p_K, T_K_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_K, \"adam\")\n",
    "writeParams(W_TK, b_TK, omega_TK, mu_TK, ypsilon_TK, J_TK, itr_TK, filename = 'trainingParams_TKepler.txt')\n",
    "\n",
    "#Training V\n",
    "mu_VK, omega_VK, W_VK, b_VK, J_VK, ypsilon_VK, itr_VK = trainingAlgorithm(K, d_K, h, tau, q_K, V_K_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_K, \"adam\")\n",
    "writeParams(W_VK, b_VK, omega_VK, mu_VK, ypsilon_VK, J_VK, itr_VK, filename = 'trainingParams_VKepler.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_TK,b_TK,omega_TK,mu_TK, ypsilon_TK, J_TK, itr_TK = readParams(K, d_K, I_K, N, filename='trainingParams_TKepler.txt')\n",
    "W_VK,b_VK,omega_VK,mu_VK, ypsilon_VK, J_VK, itr_VK = readParams(K, d_K, I_K, N, filename='trainingParams_VKepler.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Unknown Hamiltonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingBatch = concatenate(0,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dT = trainingBatch['P'].shape[0]\n",
    "IT = trainingBatch['P'].shape[1]\n",
    "batchsize = 50000\n",
    "\n",
    "pT_tilde = trainingBatch['P']\n",
    "TT_tilde, aTT, bTT = scale(trainingBatch['T'])\n",
    "TT_tilde.resize(1,TT_tilde.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_T, omega_T, W_T, b_T, J_T, ypsilon_T, itr_T = trainingAlgorithm(K, dT, h, tau, pT_tilde, TT_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "writeParams(W_T, b_T, omega_T, mu_T, ypsilon_T, J_T, itr_T, filename = 'trainingParams_T.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_T, b_T, omega_T, mu_T, ypsilon_T, J_T, itr_T = readParams(K, dT, batchsize, N, filename='trainingParams_T.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting of objective function\n",
    "plt.plot(np.linspace(0,itr_T,itr_T), J_T[:itr_T]/batchsize)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_T = trainingBatch[\"Q\"].shape[1]\n",
    "d_T = 4\n",
    "batchsize = 50000\n",
    "\n",
    "qT_tilde = trainingBatch[\"Q\"]\n",
    "VT_tilde, aVT, bVT = scale(trainingBatch[\"V\"])\n",
    "VT_tilde.resize(1,VT_tilde.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_VT, omega_VT, W_VT, b_VT, J_VT, ypsilon_VT, itr_VT = trainingAlgorithm(K, d_T, h, tau, qT_tilde, VT_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "writeParams(W_VT, b_VT, omega_VT, mu_VT, ypsilon_VT, J_VT, itr_VT, \"trainingParams_Vd4riktigtau.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_VT,b_VT,omega_VT,mu_VT, ypsilon_VT, J_VT, itr_VT = readParams(K, d_T, batchsize, N, filename='trainingParams_Vd4riktigtau.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Computing the gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getZ_oneData(y,W,b,K,h,sigma):\n",
    "    d = y.shape[0]\n",
    "    I = y.shape[1]\n",
    "    Z = np.zeros((K,d,I))\n",
    "    Z[0] = y\n",
    "    \n",
    "    for k in range(1,K):\n",
    "        u = W[k-1]@Z[k-1] + b[k-1]\n",
    "        Z[k] = Z[k-1] + h*sigma(u)\n",
    "\n",
    "    return Z\n",
    "\n",
    "def gradF(y, omega, mu, W, b, K, sigma, eta_div, sigma_div):\n",
    "    Z = getZ_oneData(y, W, b, K, h, sigma)\n",
    "    A = eta_div(np.transpose(Z[K-1])@omega + mu)*omega \n",
    "    for k in range(K-1,0,-1): \n",
    "        B = h*sigma_div(W[k]@y+b[k])\n",
    "        u = B*A\n",
    "        A = A + W[k]@u\n",
    "    return A"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sjekk om det er forskjeller mellom getZ_oneData og getZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Numerical methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symplecticEuler(gradF, t, q0, p0, omega_V, mu_V, W_V, b_V, omega_T, mu_T, W_T, b_T, K, sigma, eta_div, sigma_div): \n",
    "\n",
    "    q = np.zeros((len(t), q0.shape[0], q0.shape[1]))\n",
    "    p = np.zeros((len(t), p0.shape[0], p0.shape[1]))\n",
    "    \n",
    "    q[0] = q0\n",
    "    p[0] = p0\n",
    "    \n",
    "    for n in range(len(t)-1):\n",
    "        delta_t = t[n+1]-t[n] \n",
    "\n",
    "        q[n+1] = q[n] + delta_t*gradF(p[n], omega_T, mu_T, W_T, b_T, K, sigma, eta_div, sigma_div) #dTdp\n",
    "        p[n+1] = p[n] - delta_t*gradF(q[n+1], omega_V, mu_V, W_V, b_V, K, sigma, eta_div, sigma_div) #dVdq\n",
    "    \n",
    "    return q,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stromer_Verlet(gradF, t, q0, p0, omega_V, mu_V, W_V, b_V, omega_T, mu_T, W_T, b_T, K, sigma, eta_div, sigma_div): \n",
    "    \n",
    "    q = np.zeros((len(t), q0.shape[0], q0.shape[1]))\n",
    "    p = np.zeros((len(t), p0.shape[0], p0.shape[1]))\n",
    "    \n",
    "    q[0] = q0\n",
    "    p[0] = p0\n",
    "    \n",
    "    for n in range(len(t)-1):\n",
    "        delta_t = t[n+1]-t[n]\n",
    "        \n",
    "        u = p[n] - delta_t/2*gradF(q[n], omega_V, mu_V, W_V, b_V, K, sigma, eta_div, sigma_div) #u = p_{n+1/2}, dVdq\n",
    "        q[n+1] = q[n] + delta_t*gradF(u, omega_T, mu_T, W_T, b_T, K, sigma, eta_div, sigma_div) #dTdp\n",
    "        p[n+1] = u - delta_t/2*gradF(q[n+1], omega_V, mu_V, W_V, b_V, K, sigma, eta_div, sigma_div) #dVdq\n",
    "    \n",
    "    return q,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Nonlinear pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_p = 1000\n",
    "d_p = 2\n",
    "\n",
    "q_p = np.random.uniform(0,2*np.pi,(1,I_p))\n",
    "V_p = V_pend(q_p)\n",
    "V_p_tilde, aVp, bVp = scale(V_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Tpend,b_Tpend,omega_Tpend,mu_Tpend,ypsilon_Tpend, J_Tpend, itr_Tpend = readParams(K, d_p, I_p, N, filename='trainingParams_Tp.txt')\n",
    "W_Vpend,b_Vpend,omega_Vpend,mu_Vpend,ypsilon_Vpend, J_Vpend, itr_Vpend = readParams(K, d_p, I_p, N, filename='trainingParams_Vp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = np.zeros((d_p,1))\n",
    "p0 = np.zeros((d_p,1))\n",
    "t = np.linspace(0,10,100)\n",
    "\n",
    "q_sympEuler, p_sympEuler = symplecticEuler(gradF, t, q0, p0, omega_Vpend, mu_Vpend, W_Vpend, b_Vpend, omega_Tpend, mu_Tpend, W_Tpend, b_Tpend, K, sigma, eta_div, sigma_div)\n",
    "\n",
    "q_StromerVerlet, p_StromerVerlet = Stromer_Verlet(gradF, t, q0, p0, omega_Vpend, mu_Vpend, W_Vpend, b_Vpend, omega_Tpend, mu_Tpend, W_Tpend, b_Tpend, K, sigma, eta_div, sigma_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_plot = np.append(np.flip(q_sympEuler[:,0,:]),q_sympEuler[:,1,:])\n",
    "\n",
    "q = np.linspace(-2,2,100)\n",
    "\n",
    "plt.plot(q, V_pend(q), label = \"fasit\")\n",
    "plt.plot(q_plot, V_pend(q_plot), \"--\", label = \"Symplectic Euler\")\n",
    "\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_plot = np.append(np.flip(q_StromerVerlet[:,0,:]),q_StromerVerlet[:,1,:])\n",
    "\n",
    "q = np.linspace(-2,2,100)\n",
    "\n",
    "plt.plot(q, V_pend(q), label = \"fasit\")\n",
    "plt.plot(q_plot, V_pend(q_plot), \"--\", label = \"Stromer Verlet\")\n",
    "\n",
    "\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
