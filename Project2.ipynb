{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "    <center>\n",
    "    TMA4215 Project 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "#### 1. Introduction\n",
    "#### 2. Algorithm \n",
    "#### 3. Deciding parameters\n",
    "3.1 Number of hidden layers, $K$\n",
    "\n",
    "3.2 \n",
    "\n",
    "\n",
    "#### 3. Testing on suggested functions\n",
    "3.1 $F(y) = 1-\\cos(y)$\n",
    "\n",
    "3.2 $G(y) = \\frac{1}{2}(y_1^2 + y_2^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to use a neural network to train approximations of Hamiltonian function, derive and implement formulas for computing the gradient of the trained function and use those to implement symplectic Euler and the St√∏rmer-Verlet method for the Hamiltonian function. \n",
    "\n",
    "This report present the algorithm, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import isclose\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_2_data_acquisition import generate_data, concatenate\n",
    "from files import writeParams, readParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above are imported from separate files and are used to generate input data batches from comma seperated files and to write our trained values to a file as well as reading these values from a file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is divided into several smaller codeblocks with the purpose of making the code easy to follow. Principally, the algorithm's parts is to transform input data between the layers in the network, decide the gradients of the objective function, $J = \\frac{1}{2} \\|Z-c\\| $, with respect to weights, biases, $\\omega$ and $\\mu$ and optimalize the network with respect to those. Additionally, several utility functions is defined to be used in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getW(K,d):\n",
    "    w = np.random.randn(K,d,d)\n",
    "    return w\n",
    "\n",
    "def getb(K,d):\n",
    "    b = np.random.randn(K,d,1)\n",
    "    return b\n",
    "\n",
    "def getomega(d):\n",
    "    omega = np.random.randn(d,1)\n",
    "    return omega\n",
    "\n",
    "def getmu():\n",
    "    mu = np.random.randn(1)\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above generate random initial values drawn from a standard normal distribution to the weights, $W_k$, the biases, $b_k$, $\\omega$ and $\\mu$. These random values causes some marginal differences between runs of the same dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getZ(Y0, W, b, K, d, I, h, sigma):\n",
    "    #initialize Z, Z0=Y0\n",
    "    Z = np.zeros((K+1,d,I))\n",
    "    Z[0] = Y0\n",
    "\n",
    "    #finds Zk\n",
    "    for k in range(1,K+1):\n",
    "        u = W[k-1]@Z[k-1] + b[k-1]\n",
    "        Z[k] = Z[k-1] + h*sigma(u)\n",
    "\n",
    "    return Z\n",
    "\n",
    "def getP(Z, ypsilon, c, omega, mu, W, b, K, d, I, h, sigma_div, eta_div):\n",
    "    #initialize P\n",
    "    P = np.zeros((K+1,d,I))\n",
    "\n",
    "    #finds P_K\n",
    "    u = np.transpose(Z[-1])@omega + mu\n",
    "    P[-1]= omega@np.transpose((ypsilon-c)*eta_div(u))\n",
    "\n",
    "    #finds P_K-1 to P_0\n",
    "    for k in range(K-1,0,-1):\n",
    "        s = W[k]@Z[k] + b[k]\n",
    "        P[k]=P[k+1] + h*np.transpose(W[k])@(sigma_div(s)*P[k+1])\n",
    "        \n",
    "    return P\n",
    "\n",
    "\n",
    "def getYpsilon(Z, omega, mu, K, eta):\n",
    "    u = np.transpose(Z[K]) @ omega + mu\n",
    "    return eta(u)\n",
    "\n",
    "def getJ(ypsilon, c):\n",
    "    return 1/2 * np.linalg.norm(ypsilon-c)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Defines the functions getZ and getP, where getZ executes linear transformations $\\Phi_k: Z_k \\rightarrow Z_{k+1}$ based on the weights and biases and returns the matrix $Z$ and getP returns an utility matrix used in calulations of derivatives of the objective function with respect to the weight and biases. Furthermore, getYpsilon is defined and returns a vector of the function values in the last layer in the network and the objective funksjon $J$ is defined as a measure of the difference between the resulting trained values and the exact function values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdJdmu(Z, ypsilon, c, omega, mu, K, eta_div):\n",
    "    u = np.transpose(Z[K])@omega + mu\n",
    "    dJdmu = np.transpose(eta_div(u))@(ypsilon-c)\n",
    "    return dJdmu\n",
    "\n",
    "def getdJdomega(Z, ypsilon, c, omega, mu, K, eta_div):\n",
    "    u = np.transpose(Z[K])@omega + mu\n",
    "    dJdOmega = Z[K] @ ((ypsilon - c) * eta_div(u))\n",
    "    return dJdOmega\n",
    "\n",
    "def getdJdW(P, Z, W, b, K, h, sigma_div):\n",
    "    dJdW = np.zeros_like(W)\n",
    "    for k in range(K):\n",
    "        u = W[k]@Z[k]+b[k]\n",
    "        dJdW[k] = h*(P[k+1]*sigma_div(u))@ np.transpose(Z[k])  \n",
    "    return dJdW\n",
    "\n",
    "def getdJdb(P, Z, W, b, K, h, sigma_div):\n",
    "    dJdb = np.zeros_like(b)\n",
    "    for k in range(K):\n",
    "        u = W[k]@Z[k]+b[k]\n",
    "        dJdb[k] = h*(P[k+1]*sigma_div(u))@np.ones((Z.shape[2],1))\n",
    "    return dJdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Functions to obtain the derivatives of the objective function with respect to weights and biased is defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMandV(theta):\n",
    "    return np.zeros_like(theta),np.zeros_like(theta)\n",
    "\n",
    "def plain(theta, tau, dJdtheta):\n",
    "    return theta - tau*dJdtheta\n",
    "\n",
    "def adam(theta,alpha,m,v,g,i):\n",
    "    #parameters\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 10E-8\n",
    "\n",
    "    #m,v\n",
    "    m = beta1*m + (1-beta1)*g\n",
    "    v = beta2*v + (1-beta2)*np.multiply(g,g)\n",
    "\n",
    "    m_hat = np.multiply(m,1/(1-beta1**i))\n",
    "    v_hat = np.multiply(v,1/(1-beta2**i))\n",
    "\n",
    "    #update\n",
    "    R = alpha*np.multiply(m_hat,1/(np.sqrt(v_hat)+epsilon))\n",
    "\n",
    "    return theta-R,m,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above is used to optimize the weights, biases, $\\omega$ and $\\mu$ in different ways, which will be compared later on in this report.\n",
    "\n",
    "In plain vanilla gradient descent, one follows the gradient in decreasing direction and the learning parameter $\\tau$ determine how far one shall follow it. \n",
    "\n",
    "In Adam descent method, one follows the gradient with different length depending on different parameters. The parameters $m$ and $v$ represent respectively a kind of mean value and squared mean value of the previous iterations gradients. The function getMandV initialize $m$ and $v$ to be either zer, the zerovector or the zeromatrix depending on inputdata's dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigma_div(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def eta(x):\n",
    "    return 1/2*(1+np.tanh(x/2))\n",
    "\n",
    "def eta_div(x):\n",
    "    return 1/(4*np.cosh(x/2)**2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the activating function $\\sigma$, which is used in the transformation $\\Phi_k: Z_k \\rightarrow Z_{k+1}$, and the function $\\eta$, used in projection from last layer on a scalar $z$, is defined. The derivatives of $\\sigma$ and $\\eta$ is also defined and will be used in calculations of the gradient of the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(x, alpha=0.2, beta=0.8):\n",
    "    a = np.amin(x)\n",
    "    b = np.amax(x)\n",
    "    \n",
    "    x_tilde = 1/(b-a)*((b-x)*alpha + (x-a)*beta)\n",
    "    return x_tilde, a, b\n",
    "\n",
    "def inverseScale(x_tilde, a, b, alpha=0.2, beta=0.8):\n",
    "    return 1/(beta-alpha)*((x_tilde-alpha)*b + (beta-x_tilde)*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to scale data in terms of a min-max transformation that guaranteed that the data have all its components in the interval $[\\alpha, \\beta]$ is defined. This is desirable since some functions is acting component-wise on other functions....?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def successrate(ypsilon, c, tol):\n",
    "    sum = 0\n",
    "    for i in range(ypsilon.shape[0]):\n",
    "        if isclose(ypsilon[i][0], c[0][i], abs_tol = tol):\n",
    "            sum +=1\n",
    "    return sum/ypsilon.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si noe fornuftig her etterhver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingAlgorithm(K, d, h, tau, Y0, c0, eta, sigma, eta_div, sigma_div, N, tol, chunksize, optimization):\n",
    "    #finds input shape\n",
    "    d0 = Y0.shape[0]\n",
    "    I0 = Y0.shape[1]\n",
    "    \n",
    "    #reshapes input to match dimension of layers\n",
    "    if d0 > d:\n",
    "        return \"d must be larger than d0\"\n",
    "    \n",
    "    if d0 < d:\n",
    "        zero = np.zeros((d-d0,I0))\n",
    "        Y0 = np.vstack((Y0,zero))\n",
    "    \n",
    "    #gets initial weigths\n",
    "    omega = getomega(d)\n",
    "    mu = getmu()\n",
    "    W = getW(K,d)\n",
    "    b = getb(K,d)\n",
    "    \n",
    "    c0 = np.transpose(c0)\n",
    "  \n",
    "    if optimization == \"adam\":\n",
    "        #initial m,v for adam descent\n",
    "        mmu,vmu = getMandV(mu)\n",
    "        momega,vomega =getMandV(omega)\n",
    "        mW,vW = getMandV(W)\n",
    "        mb,vb = getMandV(b)\n",
    "    \n",
    "    #initializes vector to store objective function values\n",
    "    J = np.zeros(N)\n",
    "    ypsilon = np.zeros_like(c0)\n",
    "    \n",
    "    for i in range(N):\n",
    "               \n",
    "        #stochastic gradient descent\n",
    "        if I0 == chunksize:\n",
    "            Y0_chunk = Y0\n",
    "            c_chunk = c0\n",
    "        elif I0 > chunksize:\n",
    "            s = np.random.randint(0,I0-chunksize)\n",
    "            Y0_chunk = Y0[:,s:(s+chunksize)]\n",
    "            c_chunk=c0[s:(s+chunksize),:]\n",
    "        else:\n",
    "            return \"chunksize must be smaller than I\"        \n",
    "        \n",
    "\n",
    "        #transformations between layers\n",
    "        Z = getZ(Y0_chunk, W, b, K, d, chunksize, h, sigma)\n",
    "        ypsilon = getYpsilon(Z, omega, mu, K, eta)\n",
    "        P = getP(Z, ypsilon, c_chunk, omega, mu, W, b, K, d, chunksize, h, sigma_div, eta_div)\n",
    "        \n",
    "        #objective function\n",
    "        J[i] = getJ(ypsilon, c_chunk)\n",
    "        \n",
    "        #finds gradients\n",
    "        dJdmu = getdJdmu(Z, ypsilon, c_chunk, omega, mu, K, eta_div)\n",
    "        dJdomega = getdJdomega(Z, ypsilon, c_chunk, omega, mu, K, eta_div)\n",
    "        dJdW = getdJdW(P, Z, W, b, K, h, sigma_div)\n",
    "        dJdb = getdJdb(P, Z, W, b, K, h, sigma_div)\n",
    "        \n",
    "        if optimization == \"plain\":\n",
    "            mu = plain(mu, tau, dJdmu)\n",
    "            omega = plain(omega, tau, dJdomega)\n",
    "            W = plain(W, tau, dJdW)\n",
    "            b = plain(b, tau, dJdb)\n",
    "        \n",
    "        elif optimization == \"adam\":\n",
    "            mu, mmu,vmu = adam(mu,tau,mmu,vmu,dJdmu,i+1)\n",
    "            omega, momega,vomega = adam(omega,tau,momega,vomega,dJdomega,i+1)\n",
    "            W, mW,vW = adam(W,tau,mW,vW,dJdW,i+1)\n",
    "            b, mb,vb = adam(b,tau,mb,vb,dJdb,i+1)\n",
    "        \n",
    "        if (J[i]/chunksize) < tol:\n",
    "            break\n",
    "        \n",
    "    return mu, omega, W, b, J, ypsilon, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(yTest, cTest, W, b, omega, mu, K, d, I, h, sigma, eta):\n",
    "\n",
    "    #scaling of input\n",
    "    yTilde, aY, bY = scale(yTest)\n",
    "    cTilde, ac, bc = scale(cTest)\n",
    "\n",
    "    #finds input shape\n",
    "    d0 = yTest.shape[0]\n",
    "    \n",
    "    if d > d0:\n",
    "        zero = np.zeros((d-d0,I))\n",
    "        yTilde = np.vstack((yTilde,zero))\n",
    "    \n",
    "    zTest = getZ(yTilde, W, b, K, d, I, h, sigma)\n",
    "    ypsilonTilde = getYpsilon(zTest, omega, mu, K, eta)\n",
    "\n",
    "    #rescaling of input\n",
    "    ypsilonTest = inverseScale(ypsilonTilde, ac, bc)\n",
    "\n",
    "    return ypsilonTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training algorithm returns trained values of the weights, biases, $\\omega$ and $\\mu$ as well as the objective function and $\\Upsilon$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deciding parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I = 1000      \n",
    "N = 2500\n",
    "tol = 1E-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.0 Batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General for all test\n",
    "N = 2500\n",
    "tol = 1E-10\n",
    "K = 50\n",
    "batch0 = generate_data(0)\n",
    "\n",
    "p0_tilde, ap0,bp0 = scale(batch0['P'])\n",
    "T0_tilde, aT0, bT0 = scale(batch0['T'])\n",
    "T0_tilde.resize(1,T0_tilde.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize_list = np.arange(I_0/4, I_0+1,I_0/4).astype(int) \n",
    "d_0 = p0_tilde.shape[0]      #dimension of layers for F(y)\n",
    "h = 0.1      #stepsize\n",
    "tau = 0.0001   #learning parameter\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "ax = axs[0]\n",
    "#Adam\n",
    "for i in range(len(batchsize_list)):\n",
    "    #training with different K with adam\n",
    "    batchsize = batchsize_list[i]\n",
    "    mu_0, omega_0, W_0, b_0, J_batch, ypsilon_0, itr_0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_batch/batchsize, label = r\"$batchsize ={}$\".format(batchsize))\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "#Plain\n",
    "for i in range(len(batchsize_list)):\n",
    "    #training with different K with plain\n",
    "    batchsize = batchsize_list[i]\n",
    "    mu_0, omega_0, W_0, b_0, J_batch_plain, ypsilon_0, itr_0  = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N,tol, batchsize, \"plain\")\n",
    "    ax.plot(np.linspace(0,N,N), J_batch_plain/batchsize, label = r\"$batchsize ={}$\".format(batchsize))\n",
    "ax.set_title(r\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize_list = np.arange(I_0/4, I_0+1,I_0/4).astype(int) \n",
    "d_0 = p0_tilde.shape[0]      #dimension of layers for F(y)\n",
    "h = 0.1      #stepsize\n",
    "tau = 0.0001   #learning parameter\n",
    "\n",
    "\n",
    "#Adam\n",
    "filenames_batchsize_A = []\n",
    "filenames_batchsize_P = []\n",
    "for i in range(len(batchsize_list)):\n",
    "    batchsize = batchsize_list[i]\n",
    "    filenames_batchsize_A.append('trainingParams_A{}'.format(batchsize))\n",
    "    filenames_batchsize_P.append('trainingParams_P{}'.format(batchsize))\n",
    "    #training with different K with adam\n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_Abatch, ypsilon_A0, itr_A0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Abatch, itr_A0, filename = filenames_batchsize_A[i])\n",
    "    #training with different K with plain\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_Pbatch, ypsilon_P0, itr_P0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Pbatch, itr_P0, filename = filenames_batchsize_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "ax = axs[0]\n",
    "\n",
    "#plot adam\n",
    "for i in range(len(batchsize_list)):\n",
    "    batchsize = batchsize_list[i]\n",
    "    W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_A0, itr_A0 = readParams(K, d_0, batchsize, N, filename = filename_batchsize_A[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_Abatch/batchsize, label = r\"$batchsize ={}$\".format(batchsize))\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "#plot plain\n",
    "for i in range(len(batchsize_list)):\n",
    "    batchsize = batchsize_list[i]\n",
    "    W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_P0, itr_P0 = readParams(K, d_0, batchsize, N, filename = filename_batchsize_P[i])\n",
    "    ax.plot(np.linspace(0,N,N), J_Pbatch/batchsize, label = r\"$batchsize ={}$\".format(batchsize))\n",
    "ax.set_title(r\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Number of hidden layers, $K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_list = np.arange(25,101,25)      #number of hidden layers\n",
    "d_0 = p0_tilde.shape[0]      #dimension of layers for F(y)\n",
    "h = 0.1      #stepsize\n",
    "tau = 0.001   #learning parameter\n",
    "batchsize = I_0\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "ax = axs[0]\n",
    "#Adam\n",
    "for i in range(len(K_list)):\n",
    "    #training with different K with adam\n",
    "    K = K_list[i]\n",
    "    mu_0, omega_0, W_0, b_0, J_K, ypsilon_0, itr_0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol,batchsize, \"adam\")\n",
    "\n",
    "    ax.plot(np.linspace(0,N,N), J_K/batchsize, label = r\"$K ={}$\".format(K))\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "#Plain\n",
    "for i in range(len(K_list)):\n",
    "    #training with different K with plain\n",
    "    K = K_list[i]\n",
    "    mu_0, omega_0, W_0, b_0, J_K_plain, ypsilon_0, itr_0  = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N,tol, batchsize, \"plain\")\n",
    "    ax.plot(np.linspace(0,N,N), J_K_plain/batchsize, label = r\"$K ={}$\".format(K))\n",
    "ax.set_title(r\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y0_F = np.linspace(-2,2,I)\n",
    "Y0_F.resize(1,I)\n",
    "c_F = F(Y0_F)\n",
    "\n",
    "Y0_F_tilde, aY0_F, bY0_F = scale(Y0_F)\n",
    "c_F_tilde, ac_F, bc_F = scale(c_F)\n",
    "\n",
    "Y0_G = np.random.uniform(-2,2,(2,I))\n",
    "c_G = G(Y0_G[0],Y0_G[1])\n",
    "c_G.resize(1,I)\n",
    "\n",
    "Y0_G_tilde, aY0_G, bY0_G = scale(Y0_G)\n",
    "c_G_tilde, ac_G, bc_G = scale(c_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K_list = np.arange(25,101,25)      #number of hidden layers\n",
    "d_F = 2      #dimension of layers for F(y)\n",
    "h = 0.1      #stepsize\n",
    "tau = 0.001   #learning parameter\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "ax = axs[0]\n",
    "#Adam\n",
    "for i in range(len(K_list)):\n",
    "    #training with different K with adam\n",
    "    K = K_list[i]\n",
    "    mu_F, omega_F, W_F, b_F, J_K, ypsilon, itr = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I, \"adam\")\n",
    "                                                            \n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_K/I, label = r\"$K ={}$\".format(K))\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "#Plain\n",
    "for i in range(len(K_list)):\n",
    "    #training with different K with plain\n",
    "    K = K_list[i]\n",
    "    mu_F, omega_F, W_F, b_F, J_K_plain, ypsilon, itr = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I, \"plain\")\n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_K_plain/I, label = r\"$K ={}$\".format(K))\n",
    "ax.set_title(r\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Value of the learning parameter $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 50      #number of hidden layers\n",
    "d_0 = p0_tilde.shape[0]      #dimension of layers for F(y)\n",
    "h = 0.1      \n",
    "tau_list = np.linspace(0.001, 0.01, 4)   #learning parameter\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "ax = axs[0]\n",
    "#Adam\n",
    "for i in range(len(tau_list)):\n",
    "    #training with different tau with adam\n",
    "    tau = tau_list[i]\n",
    "    mu_0, omega_0, W_0, b_0, J_tau, ypsilon_0, itr_0  = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N,tol, batchsize, \"adam\")\n",
    "\n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_tau/I_0, label = r\"$\\tau =%.4f$\"%tau)\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "\n",
    "ax = axs[1]\n",
    "#Plain\n",
    "for i in range(len(tau_list)):\n",
    "    #training with different tau with plain\n",
    "    tau = tau_list[i]\n",
    "    mu_0, omega_0, W_0, b_0, J_tau_plain, ypsilon_0, itr_0  = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N,tol, batchsize, \"plain\")\n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_tau_plain/I, label = r\"$\\tau =%.4f$\"%tau)\n",
    "ax.set_title(\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 50      #number of hidden layers\n",
    "d_F = 2      #dimension of layers for F(y)\n",
    "h = 0.1      #stepsize\n",
    "tau_list = np.linspace(0.001, 0.01, 4)   #learning parameter\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "ax = axs[0]\n",
    "#Adam\n",
    "for i in range(len(tau_list)):\n",
    "    #training with different tau with adam\n",
    "    tau = tau_list[i]\n",
    "    mu_F, omega_F, W_F, b_F, J_tau, ypsilon, itr = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I, \"adam\")\n",
    "\n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_tau/I, label = r\"$\\tau =%.4f$\"%tau)\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "    \n",
    "\n",
    "ax = axs[1]\n",
    "#Plain\n",
    "for i in range(len(tau_list)):\n",
    "    #training with different tau with plain\n",
    "    tau = tau_list[i]\n",
    "    mu_F, omega_F, W_F, b_F, J_tau_plain, yspilon, itr = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I, \"plain\")\n",
    "    \n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_tau_plain/I, label = r\"$\\tau =%.4f$\"%tau)\n",
    "ax.set_title(\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 The dimension of the layers $d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 50      #number of hidden layers\n",
    "d0_list = np.array([1,2,3,4])      #dimension of layers for F(y)\n",
    "h = 0.1      #stepsize\n",
    "tau = 0.001   #learning parameter\n",
    "\n",
    "\n",
    "for i in range(len(d0_list)):\n",
    "    #training with different d_F with adam\n",
    "    d0 = d0_list[i]\n",
    "    mu_0, omega_0, W_0, b_0, J_d0, ypsilon_0, itr_0  = trainingAlgorithm(K, d0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N,tol, batchsize, \"adam\")\n",
    "    plt.plot(np.linspace(0,N,N), J_d0/I, label = r\"$d ={}$\".format(d_F))\n",
    "    \n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 50      #number of hidden layers\n",
    "d_F_list = np.array([1,2,3,4])      #dimension of layers for F(y)\n",
    "h = 0.1      #stepsize\n",
    "tau = 0.001   #learning parameter\n",
    "\n",
    "\n",
    "for i in range(len(d_F_list)):\n",
    "    #training with different d_F with adam\n",
    "    d_F = d_F_list[i]\n",
    "    mu_F, omega_F, W_F, b_F, J_dF, ypsilon, itr = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I, \"adam\")\n",
    "\n",
    "    plt.plot(np.linspace(0,N,N), J_dF/I, label = r\"$d ={}$\".format(d_F))\n",
    "    \n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 50      #number of hidden layers\n",
    "d_G_list = np.array([2,4,6,8])      #dimension of layers for G(y)\n",
    "h = 0.1      #stepsize\n",
    "tau = 0.001   #learning parameter\n",
    "\n",
    "for i in range(len(d_G_list)):\n",
    "    #training with different d_G with adam\n",
    "    d_G = d_G_list[i]\n",
    "    mu_G, omega_G, W_G, b_G, J_dG, ypsilon, itr = trainingAlgorithm(K, d_G, h, tau, Y0_G_tilde, c_G_tilde, eta, sigma, eta_div, sigma_div, N, tol, I, \"adam\")\n",
    "      \n",
    "    plt.plot(np.linspace(0,N,N), J_dG/I, label = r\"$d ={}$\".format(d_G))\n",
    "    \n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 The stepsize $h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 50      #number of hidden layers\n",
    "d0 = p0_tilde.shape[0]      #dimension of layers for F(y)\n",
    "h_list = np.array([0.05, 0.1, 0.15, 0.2])      #stepsize\n",
    "tau = 0.001   #learning parameter\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "ax = axs[0]\n",
    "#Adam\n",
    "for i in range(len(h_list)):\n",
    "    #training with different h with adam\n",
    "    h = h_list[i]\n",
    "    mu_0, omega_0, W_0, b_0, J_h, ypsilon_0, itr_0  = trainingAlgorithm(K, d0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N,tol, batchsize, \"adam\")\n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_h/I, label = r\"$h ={}$\".format(h))\n",
    "\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "ax = axs[1]\n",
    "#Plain\n",
    "for i in range(len(h_list)):\n",
    "    #training with different h with plain\n",
    "    h = h_list[i]\n",
    "    mu_0, omega_0, W_0, b_0, J_h_plain, ypsilon_0, itr_0  = trainingAlgorithm(K, d0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N,tol, batchsize, \"plain\")\n",
    "\n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_h_plain/I, label = r\"$h ={}$\".format(h))\n",
    "\n",
    "ax.set_title(\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 50      #number of hidden layers\n",
    "d_F = 2      #dimension of layers for F(y)\n",
    "h_list = np.array([0.05, 0.1, 0.15, 0.2])      #stepsize\n",
    "tau = 0.001   #learning parameter\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "ax = axs[0]\n",
    "#Adam\n",
    "for i in range(len(h_list)):\n",
    "    #training with different h with adam\n",
    "    h = h_list[i]\n",
    "    mu_F, omega_F, W_F, b_F, J_h, ypsilon, itr = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I, \"adam\")\n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_h/I, label = r\"$h ={}$\".format(h))\n",
    "\n",
    "ax.set_title(\"Adam\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "ax = axs[1]\n",
    "#Plain\n",
    "for i in range(len(h_list)):\n",
    "    #training with different h with plain\n",
    "    h = h_list[i]\n",
    "    mu_F, omega_F, W_F, b_F, J_h_plain, ypsilon, itr = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I, \"plain\")\n",
    "\n",
    "    \n",
    "    ax.plot(np.linspace(0,N,N), J_h_plain/I, label = r\"$h ={}$\".format(h))\n",
    "\n",
    "ax.set_title(\"Plain\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Testing with suggested functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines global variables\n",
    "h = 0.1       #stepsize\n",
    "tau = 0.0001   #learning parameter\n",
    "tol = 1e-5  #tolerance\n",
    "N = 5000      #number of training series\n",
    "K = 50        #number of hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1  F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(y):\n",
    "    return 1-np.cos(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_F = 4        #dimension of layers for F(y)\n",
    "I_F = 100     #input size \n",
    "\n",
    "#training with F(y)\n",
    "Y0_F = np.random.uniform(-2,2,(1,I_F))\n",
    "c_F = F(Y0_F)\n",
    "\n",
    "Y0_F_tilde, aY0_F, bY0_F = scale(Y0_F)\n",
    "c_F_tilde, ac_F, bc_F = scale(c_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_F, omega_F, W_F, b_F, J_F, ypsilon_F, itr_F = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_F, \"adam\")\n",
    "#writeParams(W_F, b_F, omega_F, mu_F,ypsilon_F, J_F, itr_F, filename = 'trainingParams_F.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_Fr,b_Fr,omega_Fr,mu_Fr,ypsilon_Fr, J_F, itr_F = readParams(K, d_F, I_F, N, filename='trainingParams_F.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(0,itr_F,itr_F), J_F[:itr_F]/I_F)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#testing with F(y)\n",
    "YTest_F = np.random.uniform(-2,2,(1,I_F))\n",
    "cTest_F = F(YTest_F)\n",
    "\n",
    "\n",
    "ypsilonTest_F = testing(YTest_F, cTest_F, W_F, b_F, omega_F, mu_F, K, d_F, I_F, h, sigma, eta)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(YTest_F[0], ypsilonTest_F, \"*\", label = \"test\")\n",
    "plt.plot(YTest_F[0], cTest_F[0], \"*\", label = \"fasit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def G(y1, y2):\n",
    "    return 1/2 *(y1**2 + y2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_G = 4        #dimension of layers for F(y)\n",
    "I_G = 1000     #input size \n",
    "\n",
    "#training with G(y)\n",
    "Y0_G = np.random.uniform(-2,2,(2,I_G))\n",
    "c_G = G(Y0_G[0],Y0_G[1])\n",
    "c_G.resize(1,I_G)\n",
    "\n",
    "Y0_G_tilde, aY0_G, bY0_G = scale(Y0_G)\n",
    "c_G_tilde, ac_G, bc_G = scale(c_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_G, omega_G, W_G, b_G, J_G, ypsilon_G, itr_G = trainingAlgorithm(K, d_G, h, tau, Y0_G_tilde, c_G_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_G, \"adam\")\n",
    "writeParams(W_G, b_G, omega_G, mu_G, ypsilon_G, J_G, itr_G, filename = 'trainingParams_G.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_Gr,b_Gr,omega_Gr,mu_Gr, ypsilon_Gr, J_G, itr_G = readParams(K, d_G, I_G, N, filename='trainingParams_G.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plotting of objective function\n",
    "plt.plot(np.linspace(0,itr_G,itr_G), J_G[:itr_G]/I_G)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing with G(y)\n",
    "YTest_G = np.random.uniform(-2,2,(2,I_G))\n",
    "cTest_G = G(YTest_G[0], YTest_G[1])\n",
    "cTest_G.resize(1,I_G)\n",
    "\n",
    "ypsilonTest_G = testing(YTest_G, cTest_G, W_G, b_G, omega_G, mu_G, K, d_G, I_G, h, sigma, eta)\n",
    "\n",
    "\n",
    "#plotting of G(y)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(YTest_G[0], YTest_G[1], ypsilonTest_G, label = \"test\", depthshade = True)\n",
    "ax.scatter(YTest_G[0], YTest_G[1], cTest_G[0], label = \"test\" , c=\"red\", depthshade = False)\n",
    "plt.show()\n",
    "\n",
    "x = np.arange(0, I_G)\n",
    "plt.plot(x, ypsilonTest_G, label =\"test\")\n",
    "plt.plot(x, cTest_G[0], label = \"fasit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4. Training with data given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_0 = 3\n",
    "N_0 = 3000\n",
    "K_0= 50\n",
    "\n",
    "\n",
    "batch0 = generate_data()\n",
    "\n",
    "p0_tilde, ap0,bp0 = scale(batch0['P'])\n",
    "T0_tilde, aT0, bT0 = scale(batch0['T'])\n",
    "T0_tilde.resize(1,T0_tilde.shape[0])\n",
    "\n",
    "I_0 = p0_tilde.shape[1]\n",
    "\n",
    "mu_0, omega_0, W_0, b_0, J_0, ypsilon_0, J_0 = trainingAlgorithm(K_0, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N_0,500, \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writeParams(W_0, b_0, omega_0, mu_0, ypsilon_0, filename = 'trainingParams_0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_0r,b_0r,omega_0r,mu_0r,ypsilon_0r = readParams(K_0, d_0, I_0, filename='trainingParams_0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plotting of objective function\n",
    "plt.plot(np.linspace(0,N_0,N_0), J_0)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(0, I_0)\n",
    "ypsilon0_scaled = inverseScale(ypsilon_0, aT0, bT0)\n",
    "T0 = batch0['T']\n",
    "\n",
    "print(successrate(ypsilon0_scaled, T0, 0.0035))\n",
    "\n",
    "plt.plot(x, ypsilon0_scaled, label =\"test\")\n",
    "plt.plot(x, T0[0], label = \"fasit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing with batch 1\n",
    "\n",
    "batch1 = generate_data(1)\n",
    "\n",
    "p1 = batch1['P']\n",
    "T1 =batch1['T']\n",
    "T1.resize(1,T1.shape[0])\n",
    "\n",
    "\n",
    "ypsilonTest_1 = testing(p1, T1, W_0, b_0, omega_0, mu_0, K_0, d_0, I_0, h, sigma, eta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(0, I_0)\n",
    "plt.plot(x, ypsilonTest_1, label =\"test\")\n",
    "plt.plot(x, T1[0], label = \"fasit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingBatch = concatenate(batchmin=0, batchmax=25)\n",
    "I_T = trainingBatch[\"P\"].shape[1]\n",
    "d_T = trainingBatch[\"P\"].shape[0]\n",
    "batchsize = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pT_tilde, apT, bpT = scale(trainingBatch[\"P\"])\n",
    "TT_tilde, aTT, bTT = scale(trainingBatch[\"T\"])\n",
    "TT_tilde.resize(1,TT_tilde.shape[0])\n",
    "\n",
    "mu_T, omega_T, W_T, b_T, J_T, ypsilon_T, itr_T = trainingAlgorithm(K, d_T, h, tau, pT_tilde, TT_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_T,b_T,omega_T,mu_T,ypsilon_T, J_T, itr_T = readParams(K, d_T, batchsize, N, filename='trainingParams_T.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,itr_T,itr_T), J_T[:itr_T]/batchsize)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### larger batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingBatch = concatenate(0,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dT = trainingBatch['P'].shape[0]\n",
    "IT = trainingBatch['P'].shape[1]\n",
    "batchsize = 50000\n",
    "\n",
    "pT_tilde = trainingBatch['P']\n",
    "TT_tilde, aTT, bTT = scale(trainingBatch['T'])\n",
    "TT_tilde.resize(1,TT_tilde.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_T, omega_T, W_T, b_T, J_T, ypsilon_T, itr_T = trainingAlgorithm(K, dT, h, tau, pT_tilde, TT_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "writeParams(W_T, b_T, omega_T, mu_T, ypsilon_T, J_T, itr_T, filename = 'trainingParams_T.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_T, b_T, omega_T, mu_T, ypsilon_T, J_T, itr_T = readParams(K, dT, batchsize, N, filename='trainingParams_T.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plotting of objective function\n",
    "plt.plot(np.linspace(0,itr_T,itr_T), J_T[:itr_T]/batchsize)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Computing the gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getZ_oneData(y,W,b,K,h,sigma):\n",
    "    d = y.shape[0]\n",
    "    I = y.shape[1]\n",
    "    Z = np.zeros((K,d,I))\n",
    "    Z[0] = y\n",
    "    \n",
    "    for k in range(1,K):\n",
    "        u = W[k-1]@Z[k-1] + b[k-1]\n",
    "        Z[k] = Z[k-1] + h*sigma(u)\n",
    "\n",
    "    return Z\n",
    "\n",
    "def gradF(y, omega, mu, W, b, K, sigma, eta_div, sigma_div):\n",
    "    Z = getZ_oneData(y, W, b, K, h, sigma)\n",
    "    A = eta_div(np.transpose(Z[K-1])@omega + mu)*omega \n",
    "    for k in range(K-1,0,-1): \n",
    "        B = h*sigma_div(W[k]@y+b[k])\n",
    "        u = B*A\n",
    "        A = A + W[k]@u\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Numerical methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def symplecticEuler(gradF, t, q0, p0, omega_V, mu_V, W_V, b_V, omega_T, mu_T, W_T, b_T, K, sigma, eta_div, sigma_div): \n",
    "\n",
    "    q = np.zeros((len(t), q0.shape[0], q0.shape[1]))\n",
    "    p = np.zeros((len(t), p0.shape[0], p0.shape[1]))\n",
    "    \n",
    "    q[0] = q0\n",
    "    p[0] = p0\n",
    "    \n",
    "    for n in range(len(t)-1):\n",
    "        delta_t = t[n+1]-t[n] \n",
    "\n",
    "        q[n+1] = q[n] + delta_t*gradF(p[n], omega_T, mu_T, W_T, b_T, K, sigma, eta_div, sigma_div) #dTdp\n",
    "        p[n+1] = p[n] - delta_t*gradF(q[n+1], omega_V, mu_V, W_V, b_V, K, sigma, eta_div, sigma_div) #dVdq\n",
    "    \n",
    "    return q,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Stromer_Verlet(gradF, t, q0, p0, omega_V, mu_V, W_V, b_V, omega_T, mu_T, W_T, b_T, K, sigma, eta_div, sigma_div): \n",
    "    \n",
    "    q = np.zeros((len(t), q0.shape[0], q0.shape[1]))\n",
    "    p = np.zeros((len(t), p0.shape[0], p0.shape[1]))\n",
    "    \n",
    "    q[0] = q0\n",
    "    p[0] = p0\n",
    "    \n",
    "    for n in range(len(t)-1):\n",
    "        delta_t = t[n+1]-t[n] \n",
    "        u = p[n] - delta_t/2*gradF(q[n], omega_V, mu_V, W_V, b_V, K, sigma, eta_div, sigma_div) #u = p_{n+1/2}, dVdq\n",
    "        q[n+1] = q[n] + delta_t*gradF(u, omega_T, mu_T, W_T, b_T, K, sigma, eta_div, sigma_div) #dTdp\n",
    "        p[n+1] = u - delta_t/2*gradF(q[n+1], omega_V, mu_V, W_V, b_V, K, sigma, eta_div, sigma_div) #dVdq\n",
    "    \n",
    "    return q,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Nonlinear pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = 10E-3\n",
    "l = 0.5\n",
    "g = 9.81\n",
    "\n",
    "def T_pend(p):\n",
    "    return 0.5*p**2\n",
    "\n",
    "def V_pend(q):\n",
    "    return m*g*l*(1-np.cos(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I_p = 1000\n",
    "d_p = 2\n",
    "\n",
    "q_p = np.random.uniform(0,2*np.pi,(1,I_p))\n",
    "V_p = V_pend(q_p)\n",
    "V_p_tilde, aVp, bVp = scale(V_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_Tpend,b_Tpend,omega_Tpend,mu_Tpend,ypsilon_Tpend, J_Tpend, itr_Tpend = readParams(K, d_p, I_p, N, filename='trainingParams_Tp.txt')\n",
    "W_Vpend,b_Vpend,omega_Vpend,mu_Vpend,ypsilon_Vpend, J_Vpend, itr_Vpend = readParams(K, d_p, I_p, N, filename='trainingParams_Vp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q0 = np.zeros((d_p,1))\n",
    "p0 = np.zeros((d_p,1))\n",
    "t = np.linspace(0,10,100)\n",
    "\n",
    "q_sympEuler, p_sympEuler = symplecticEuler(gradF, t, q0, p0, omega_Vpend, mu_Vpend, W_Vpend, b_Vpend, omega_Tpend, mu_Tpend, W_Tpend, b_Tpend, K, sigma, eta_div, sigma_div)\n",
    "\n",
    "q_StromerVerlet, p_StromerVerlet = Stromer_Verlet(gradF, t, q0, p0, omega_Vpend, mu_Vpend, W_Vpend, b_Vpend, omega_Tpend, mu_Tpend, W_Tpend, b_Tpend, K, sigma, eta_div, sigma_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_plot = np.append(np.flip(q_sympEuler[:,0,:]),q_sympEuler[:,1,:])\n",
    "\n",
    "q = np.linspace(-2,2,100)\n",
    "\n",
    "plt.plot(q, V_pend(q), label = \"fasit\")\n",
    "plt.plot(q_plot, V_pend(q_plot), \"--\", label = \"Symplectic Euler\")\n",
    "\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_plot = np.append(np.flip(q_StromerVerlet[:,0,:]),q_StromerVerlet[:,1,:])\n",
    "\n",
    "q = np.linspace(-2,2,100)\n",
    "\n",
    "plt.plot(q, V_pend(q), label = \"fasit\")\n",
    "plt.plot(q_plot, V_pend(q_plot), \"--\", label = \"Stromer Verlet\")\n",
    "\n",
    "\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
