{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "    <center>\n",
    "    TMA4215 Project 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "#### 1. Introduction\n",
    "#### 2. Algorithm \n",
    "\n",
    "2.1 Utility functions \n",
    "\n",
    "2.2 The training algorithm \n",
    "\n",
    "2.3 The testing algoritm and functions for analysis\n",
    "\n",
    "#### 3. Deciding parameters\n",
    "\n",
    "3.1 The batchsize \n",
    "\n",
    "3.2 The number of hidden layers, $K$\n",
    "\n",
    "3.3 The value of the learning parameter, $\\tau$\n",
    "\n",
    "3.4 The dimension of the input data in the hidden layers $d$\n",
    "\n",
    "3.5 The stepsize $h$\n",
    "\n",
    "3.6 Conclusion\n",
    "\n",
    "\n",
    "#### 4. Training and testing with suggested functions\n",
    "4.1 $F(y) = 1-\\cos(y)$\n",
    "\n",
    "4.2 $G(y_1, y_2) = \\frac{1}{2}(y_1^2 + y_2^2)$\n",
    "\n",
    "4.3 Known Hamiltonian\n",
    "\n",
    "4.3 Unknown Hamiltonian\n",
    "\n",
    "#### 5. Determine the gradient \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to use a neural network to train approximations of Hamiltonian function, derive and implement formulas for computing the gradient of the trained function and use those to implement symplectic Euler and the Størmer-Verlet method for the Hamiltonian function. \n",
    "\n",
    "This report starts with presenting the training algorithm and arguing for the choices of the parameters $K$, $\\tau$, $d$ ang $h$ used in this algorithm. Furthermore, the training prosedure is implemented for known functions and Hamiltonians and the efficienty of this training is determined by comparing the values obtained by using the trained weights and biases to the accurate function values. For unknown Hamiltonian.... The report brifly determine a formula for computing of gradients and define the symplectic Euler and Størmer-Verlet method. ...\n",
    "\n",
    "NB: We have chosen to use the inputdata batches from the new trajectories. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import isclose\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_2_data_acquisition import generate_data, concatenate\n",
    "from files import writeParams, readParams, writeScale, readScale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above are imported from separate files and are used to generate input data batches from comma seperated files and to write our trained values to a file as well as reading these values from a file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, there will be determined an algorithm to train inputdata to recognize Hamiltonians based on the principle of machine learning and an algorithm to calculate approximations of the Hamiltonian based on the trained weights and biases. .......\n",
    "\n",
    "\n",
    ". Principally, the training algorithm's parts is to transform input data between the layers in the network, decide the gradients of the objective function, $J = \\frac{1}{2} \\|Z-c\\| $, with respect to weights, biases, $\\omega$ and $\\mu$ and optimalize the network with respect to those. \n",
    "\n",
    "\n",
    "The code is separated into smaller codebrocks, with the pursose of making it easy to follow. In the next subsection there will be defined functions used in the algorithms.... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Functions used in the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getW(K,d):\n",
    "    w = np.random.randn(K,d,d)\n",
    "    return w\n",
    "\n",
    "def getb(K,d):\n",
    "    b = np.random.randn(K,d,1)\n",
    "    return b\n",
    "\n",
    "def getomega(d):\n",
    "    omega = np.random.randn(d,1)\n",
    "    return omega\n",
    "\n",
    "def getmu():\n",
    "    mu = np.random.randn(1)\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above generate random initial values, drawn from a standard normal distribution, to the weights, $W_k$, the biases, $b_k$, $\\omega$ and $\\mu$. These random values causes some marginal differences between runs of the same dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigma_div(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def eta(x):\n",
    "    return 1/2*(1+np.tanh(x/2))\n",
    "\n",
    "def eta_div(x):\n",
    "    return 1/(4*np.cosh(x/2)**2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the activating function $\\sigma$, which is used in the transformation $\\Phi_k: Z_k \\rightarrow Z_{k+1}$, and the function $\\eta$, used in projection from last layer on a scalar $z$, is defined. The derivatives of $\\sigma$ and $\\eta$ is also defined and will be used in calculations of the gradient of the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getZ(Y0, W, b, K, d, I, h, sigma):\n",
    "    #initialize Z, Z0=Y0\n",
    "    Z = np.zeros((K+1,d,I))\n",
    "    Z[0] = Y0\n",
    "\n",
    "    #finds Zk\n",
    "    for k in range(1,K+1):\n",
    "        u = W[k-1]@Z[k-1] + b[k-1]\n",
    "        Z[k] = Z[k-1] + h*sigma(u)\n",
    "\n",
    "    return Z\n",
    "\n",
    "def getP(Z, ypsilon, c, omega, mu, W, b, K, d, I, h, sigma_div, eta_div):\n",
    "    #initialize P\n",
    "    P = np.zeros((K+1,d,I))\n",
    "\n",
    "    #finds P_K\n",
    "    u = np.transpose(Z[-1])@omega + mu\n",
    "    P[-1]= omega@np.transpose((ypsilon-c)*eta_div(u))\n",
    "\n",
    "    #finds P_K-1 to P_0\n",
    "    for k in range(K-1,0,-1):\n",
    "        s = W[k]@Z[k] + b[k]\n",
    "        P[k]=P[k+1] + h*np.transpose(W[k])@(sigma_div(s)*P[k+1])\n",
    "        \n",
    "    return P\n",
    "\n",
    "\n",
    "def getYpsilon(Z, omega, mu, K, eta):\n",
    "    u = np.transpose(Z[K]) @ omega + mu\n",
    "    return eta(u)\n",
    "\n",
    "def getJ(ypsilon, c):\n",
    "    return 1/2 * np.linalg.norm(ypsilon-c)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Defines the functions getZ and getP, where getZ executes linear transformations $\\Phi_k: Z_k \\rightarrow Z_{k+1}$ based on the weights and biases and returns the matrix $Z$ and getP returns an utility matrix used in calulations of derivatives of the objective function with respect to the weight and biases. Furthermore, getYpsilon is defined and returns a vector of the function values in the last layer of the network and the objective funksjon $J$ is defined as a measure of the difference between the resulting trained values and the exact function values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getdJdmu(Z, ypsilon, c, omega, mu, K, eta_div):\n",
    "    u = np.transpose(Z[K])@omega + mu\n",
    "    dJdmu = np.transpose(eta_div(u))@(ypsilon-c)\n",
    "    return dJdmu\n",
    "\n",
    "def getdJdomega(Z, ypsilon, c, omega, mu, K, eta_div):\n",
    "    u = np.transpose(Z[K])@omega + mu\n",
    "    dJdOmega = Z[K] @ ((ypsilon - c) * eta_div(u))\n",
    "    return dJdOmega\n",
    "\n",
    "def getdJdW(P, Z, W, b, K, h, sigma_div):\n",
    "    dJdW = np.zeros_like(W)\n",
    "    for k in range(K):\n",
    "        u = W[k]@Z[k]+b[k]\n",
    "        dJdW[k] = h*(P[k+1]*sigma_div(u))@ np.transpose(Z[k])  \n",
    "    return dJdW\n",
    "\n",
    "def getdJdb(P, Z, W, b, K, h, sigma_div):\n",
    "    dJdb = np.zeros_like(b)\n",
    "    for k in range(K):\n",
    "        u = W[k]@Z[k]+b[k]\n",
    "        dJdb[k] = h*(P[k+1]*sigma_div(u))@np.ones((Z.shape[2],1))\n",
    "    return dJdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Functions to obtain the derivatives of the objective function with respect to weights and biased is defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plain(theta, tau, dJdtheta):\n",
    "    return theta - tau*dJdtheta\n",
    "\n",
    "def adam(theta,alpha,m,v,g,i):\n",
    "    #parameters\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 10E-8\n",
    "\n",
    "    #m,v\n",
    "    m = beta1*m + (1-beta1)*g\n",
    "    v = beta2*v + (1-beta2)*np.multiply(g,g)\n",
    "\n",
    "    m_hat = np.multiply(m,1/(1-beta1**i))\n",
    "    v_hat = np.multiply(v,1/(1-beta2**i))\n",
    "\n",
    "    #update\n",
    "    R = alpha*np.multiply(m_hat,1/(np.sqrt(v_hat)+epsilon))\n",
    "\n",
    "    return theta-R,m,v\n",
    "\n",
    "def getMandV(theta):\n",
    "    return np.zeros_like(theta),np.zeros_like(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above is used to optimize the weights, biases, $\\omega$ and $\\mu$ in different ways, which will be compared later on in this report.\n",
    "\n",
    "In plain vanilla gradient descent, one follows the gradient in decreasing direction and the learning parameter $\\tau$ determine how far one shall follow it. \n",
    "\n",
    "In Adam descent method, one follows the gradient with different length depending on different parameters. The parameters $m$ and $v$ represent respectively a kind of mean value and squared mean value of the previous iterations gradients. The function getMandV initialize $m$ and $v$ to be either zero, the zerovector or the zeromatrix depending on inputdata's dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale(x, alpha=0.2, beta=0.8):\n",
    "    a = np.amin(x)\n",
    "    b = np.amax(x)\n",
    "    \n",
    "    x_tilde = 1/(b-a)*((b-x)*alpha + (x-a)*beta)\n",
    "    return x_tilde, a, b\n",
    "\n",
    "def inverseScale(x_tilde, a, b, alpha=0.2, beta=0.8):\n",
    "    return 1/(beta-alpha)*((x_tilde-alpha)*b + (beta-x_tilde)*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to scale inputdata in terms of a min-max transformation, which guarantees that the data have all its components in the interval $[\\alpha, \\beta]$, is defined. This is desirable since some functions is acting component-wise on other functions....? We used scaling on yspsilon, and therefor also $c$ , such that c can be subtracted to ypsilon without conflicts. .... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 The Training Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainingAlgorithm(K, d, h, tau, Y0, c0, eta, sigma, eta_div, sigma_div, N, tol, chunksize, optimization):\n",
    "    #finds input shape\n",
    "    d0 = Y0.shape[0]\n",
    "    I0 = Y0.shape[1]\n",
    "    \n",
    "    #reshapes input to match dimension of layers\n",
    "    if d0 > d:\n",
    "        print(\"d must be larger than d0\")\n",
    "    \n",
    "    if d0 < d:\n",
    "        zero = np.zeros((d-d0,I0))\n",
    "        Y0 = np.vstack((Y0,zero))\n",
    "    \n",
    "    #gets initial weigths\n",
    "    omega = getomega(d)\n",
    "    mu = getmu()\n",
    "    W = getW(K,d)\n",
    "    b = getb(K,d)\n",
    "    \n",
    "    c0 = np.transpose(c0)\n",
    "  \n",
    "    if optimization == \"adam\":\n",
    "        #initial m,v for adam descent\n",
    "        mmu,vmu = getMandV(mu)\n",
    "        momega,vomega =getMandV(omega)\n",
    "        mW,vW = getMandV(W)\n",
    "        mb,vb = getMandV(b)\n",
    "    \n",
    "    #initializes vector to store objective function values\n",
    "    J = np.zeros(N)\n",
    "    ypsilon = np.zeros_like(c0)\n",
    "    \n",
    "    for i in range(N):\n",
    "               \n",
    "        #stochastic gradient descent\n",
    "        if I0 == chunksize:\n",
    "            Y0_chunk = Y0\n",
    "            c_chunk = c0\n",
    "        elif I0 > chunksize:\n",
    "            s = np.random.randint(0,I0-chunksize)\n",
    "            Y0_chunk = Y0[:,s:(s+chunksize)]\n",
    "            c_chunk=c0[s:(s+chunksize),:]\n",
    "        else:\n",
    "            print(\"chunksize must be smaller than I\")       \n",
    "        \n",
    "\n",
    "        #transformations between layers\n",
    "        Z = getZ(Y0_chunk, W, b, K, d, chunksize, h, sigma)\n",
    "        ypsilon = getYpsilon(Z, omega, mu, K, eta)\n",
    "        P = getP(Z, ypsilon, c_chunk, omega, mu, W, b, K, d, chunksize, h, sigma_div, eta_div)\n",
    "        \n",
    "        #objective function\n",
    "        J[i] = getJ(ypsilon, c_chunk)\n",
    "        \n",
    "        #finds gradients\n",
    "        dJdmu = getdJdmu(Z, ypsilon, c_chunk, omega, mu, K, eta_div)\n",
    "        dJdomega = getdJdomega(Z, ypsilon, c_chunk, omega, mu, K, eta_div)\n",
    "        dJdW = getdJdW(P, Z, W, b, K, h, sigma_div)\n",
    "        dJdb = getdJdb(P, Z, W, b, K, h, sigma_div)\n",
    "        \n",
    "        if optimization == \"plain\":\n",
    "            mu = plain(mu, tau, dJdmu)\n",
    "            omega = plain(omega, tau, dJdomega)\n",
    "            W = plain(W, tau, dJdW)\n",
    "            b = plain(b, tau, dJdb)\n",
    "        \n",
    "        elif optimization == \"adam\":\n",
    "            mu, mmu,vmu = adam(mu,tau,mmu,vmu,dJdmu,i+1)\n",
    "            omega, momega,vomega = adam(omega,tau,momega,vomega,dJdomega,i+1)\n",
    "            W, mW,vW = adam(W,tau,mW,vW,dJdW,i+1)\n",
    "            b, mb,vb = adam(b,tau,mb,vb,dJdb,i+1)\n",
    "        \n",
    "        if (J[i]/chunksize) < tol:\n",
    "            break\n",
    "        \n",
    "    return mu, omega, W, b, J, ypsilon, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training algorithm returns trained values of the weights, biases, $\\omega$ and $\\mu$ as well as the objective function and $\\Upsilon$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 The testing algorithms and functions used in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testingAlgorithm(yTest, W, b, omega, mu, K, d, I, h, sigma, eta, a_scale, b_scale):\n",
    "\n",
    "    #finds input shape\n",
    "    d0 = yTest.shape[0]\n",
    "    \n",
    "    if d > d0:\n",
    "        zero = np.zeros((d-d0,I))\n",
    "        yTest = np.vstack((yTest,zero))\n",
    "    \n",
    "    zTest = getZ(yTest, W, b, K, d, I, h, sigma)\n",
    "    ypsilonTilde = getYpsilon(zTest, omega, mu, K, eta)\n",
    "\n",
    "    #rescaling of input\n",
    "    ypsilonTest = inverseScale(ypsilonTilde, a_scale, b_scale)\n",
    "\n",
    "    return ypsilonTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing algorithm return a values of the $\\Upsilon$, corresponding the neutral networks approximants for each inputdata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def successrate(ypsilon, c, tol,I):\n",
    "    ypsilon.resize((1,I))\n",
    "    c.resize((1,I))\n",
    "    \n",
    "    s = 0\n",
    "    acc = 0\n",
    "    \n",
    "    for i in range(I):\n",
    "        d = np.abs(ypsilon[0,i]-c[0,i])\n",
    "        acc += d\n",
    "        if d <= tol:\n",
    "            s +=1\n",
    "    return s/I, acc/I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function successrate is used to determine how well the neural network trained the weights and biases by returning the proportion of values that is assorted as sufficently close to the exact value with respect the tolarated deviation. The function does also return a mean value of the deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plottingfunction of objective function\n",
    "def plotObjFnc(J, itr, batchsize, fnc_string):\n",
    "    plt.plot(np.linspace(0,itr,itr), J[:itr]/batchsize)\n",
    "    plt.title(r\"The objective function for {}\".format(fnc_string))\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Iterations, N')\n",
    "    plt.ylabel('Objective function, J')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deciding parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from decidingParameters import filenameList, plotParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch0 = generate_data(0)\n",
    "\n",
    "p0_tilde = batch0['P']\n",
    "T0_tilde, aT0, bT0 = scale(batch0['T'])\n",
    "T0_tilde.resize(1,T0_tilde.shape[0])\n",
    "\n",
    "N = 5000\n",
    "tol = 1E-10 \n",
    "\n",
    "I_0 = p0_tilde.shape[1] #I_0 = 2048\n",
    "\n",
    "#initial values\n",
    "batchsize = I_0 \n",
    "K = 50\n",
    "tau = 0.001\n",
    "d_0 = p0_tilde.shape[0]   #in this case 3  \n",
    "h = 0.1     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, optimal choices for the parameters $K$, $\\tau$, $d$ and $h$ will be investigated. All the following tests are based on the first batch of values, enclosed to the project description, for the unknown Hamiltonian function. The number for training series is chosen to be 5000 for all the tests, as it makes it possible to illustrate the development of each parameter's impact on the training process. The tolerance is set sufficiently small so that one can guarantee that the training algorithm does not obtain a tolerated value for the objective function within the 5000 training series. \n",
    "\n",
    "The initial values of the parameters is set to be $K = 50$, $\\tau = 0.001$, $d = 3$ and $h =0.1$. After each investigation the value of the current parameter will be updated to the investigated optimal choice and the updated value is used in the remaining tests.\n",
    "\n",
    "The investigations include graphs for both implementation of the adam descent and plain vanilla gradient descent, but our choices of parameters will in all cases be made based on Adam descent, knowing this method is more efficient than plain vanilla gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 The batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variables for this test\n",
    "batchsize_list = np.arange(I_0/4, I_0+1,I_0/4).astype(int) \n",
    "\n",
    "filenames_batchsize_A, filenames_batchsize_P = filenameList(batchsize_list, \"batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(batchsize_list)):\n",
    "    batchsize = batchsize_list[i]\n",
    "    #train for different batchsizes with adam descent \n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_Abatch, ypsilon_A0, itr_A0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Abatch, itr_A0, filename = filenames_batchsize_A[i])\n",
    "    \n",
    "    #train for different batchsizes with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_Pbatch, ypsilon_P0, itr_P0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Pbatch, itr_P0, filename = filenames_batchsize_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotParams(batchsize_list, \"batch\", K = K , d = d_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs illustrate that the use of stochastic gradient descent makes the objective function oscillates around a minimum. The smaller the batchsize, the more oscillations. Since a smaller batchsize will reduce the running time, the stochastic gradient descent will be used with about half of the total unputdatasize in cases with a lot of inputdata (e.g. inputdata from 25 batches). For full batchsize, the objective function converges to its minimum. This is favourable for the investigations of the parameters $K$, $\\tau$ and $d$ (since ??). Full batchsize will therefore be used in the investigations in this section. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 The number of hidden layers, $K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variables for this test   \n",
    "K_list = np.arange(25,101,25) \n",
    "\n",
    "#define lists of filenames where the trained parameters will be stored\n",
    "filenames_K_A, filenames_K_P = filenameList(K_list, \"K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(K_list)):\n",
    "    K = K_list[i]\n",
    "    #train for different K values with adam descent\n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_AK, ypsilon_A0, itr_A0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_AK, itr_A0, filename = filenames_K_A[i])\n",
    "    \n",
    "    #train for different K values with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_PK, ypsilon_P0, itr_P0 = trainingAlgorithm(K, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_PK, itr_P0, filename = filenames_K_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotParams(K_list, \"K\", d = d_0, batchsize = batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a large number of hidden layers will increase the running time significantly/considerably, we elliminate $K = 75$ and $K = 100$, even though $K = 75$ seems to be the most optimal choice by observing the graph. We choose $K = 25$ to be the optimal value in our case since the objective function seems to reach the value $10^{-5}$ after fewer iterations than the case where $K = 50$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 The value of the learning parameter $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variables for this test\n",
    "tau_list = np.array([0.001,0.004,0.007,0.01])   #learning parameter\n",
    "K_tau = 25\n",
    "\n",
    "#define lists of filenames where the trained parameters will be stored\n",
    "filenames_tau_A, filenames_tau_P = filenameList(tau_list, \"tau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(tau_list)):\n",
    "    tau = tau_list[i]\n",
    "\n",
    "    #train for different tau values with adam descent\n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_Atau, ypsilon_A0, itr_A0 = trainingAlgorithm(K_tau, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Atau, itr_A0, filename = filenames_tau_A[i])\n",
    "    \n",
    "    #train for different tau values with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_Ptau, ypsilon_P0, itr_P0 = trainingAlgorithm(K_tau, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Ptau, itr_P0, filename = filenames_tau_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotParams(tau_list, \"tau\", K = K_tau, d = d_0, batchsize = batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The optimal value of the learning parameter is chosen to be $\\tau = 0.004$, since the graph illustrates that it does not have that much spikes within ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 The dimension of the input data in the hidden layers $d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_0_list = np.array([3,4,5,6])  \n",
    "K_d = 25\n",
    "tau_d = 0.004\n",
    "\n",
    "#define lists of filenames where the trained parameters will be stored\n",
    "filenames_d_A, filenames_d_P = filenameList(d_0_list, \"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(d_0_list)):\n",
    "    d0 = d_0_list[i]\n",
    "    \n",
    "    #train for different values of d with adam descent\n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_Ad0, ypsilon_A0, itr_A0 = trainingAlgorithm(K_d, d0, h, tau_d, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Ad0, itr_A0, filename = filenames_d_A[i])\n",
    "    \n",
    "    #train for different values of d with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_Pd0, ypsilon_P0, itr_P0 = trainingAlgorithm(K_d, d0, h, tau_d, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Pd0, itr_P0, filename = filenames_d_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotParams(d_0_list, \"d\", K = K_d, batchsize = batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs illustrate that to .... the dimension of the inputdata in the hidden layer to a dimension greater then the original inputdata dimension is favourable. \n",
    "\n",
    "\n",
    "\n",
    "feil:\n",
    "n the cases where the dimension of the inputdata in the hidden layers is $d=3$, $d=5$ and $d=6$, and these spikes which can give values greater than the value for $d=4$. The dimension of the input data in the hidden layers is therefore chosen to be $d=4$ (where the input data originally has the dimension $d_0 = 3$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 The stepsize $h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_list = np.array([0.05, 0.1, 0.15, 0.2])\n",
    "K_h = 25\n",
    "tau_h = 0.004\n",
    "d_h = 5\n",
    "\n",
    "#define lists of filenames where the trained parameters will be stored\n",
    "filenames_h_A, filenames_h_P= filenameList(h_list, \"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(h_list)):\n",
    "    h = h_list[i]\n",
    "    #train for different stepsizes h with adam descent\n",
    "    mu_A0, omega_A0, W_A0, b_A0, J_Ah, ypsilon_A0, itr_A0 = trainingAlgorithm(K_h, d_h, h, tau_h, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "    writeParams(W_A0, b_A0, omega_A0, mu_A0, ypsilon_A0, J_Ah, itr_A0, filename = filenames_h_A[i])\n",
    "    \n",
    "    #train for different stepsizes h with plain vanilla gradient descent\n",
    "    mu_P0, omega_P0, W_P0, b_P0, J_Ph, ypsilon_P0, itr_P0 = trainingAlgorithm(K_h, d_h, h, tau_h, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"plain\")\n",
    "    writeParams(W_P0, b_P0, omega_P0, mu_P0, ypsilon_P0, J_Ph, itr_P0, filename = filenames_h_P[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotParams(h_list, \"h\", K = K_h, d = d_h, batchsize = batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-grafene viser at adam er bedre enn plain overalt\n",
    "\n",
    "-valg av verdier oppsumert\n",
    "\n",
    "-om man skulle vist den endelige objective function grafen med disse verdiene??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training and testing with suggested functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defines global variables for all the following tests\n",
    "h = 0.15       #stepsize\n",
    "tau = 0.004   #learning parameter\n",
    "tol = 1e-5  #tolerance\n",
    "N = 5000      #number of training series\n",
    "K = 25        #number of hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1  $F(y) = 1-\\cos{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F(y):\n",
    "    return 1-np.cos(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variable used for training with F(y)\n",
    "d_F = 2        \n",
    "I_F = 1500 \n",
    "\n",
    "#define and scale input data ....AND FASIT\n",
    "Y0_F = np.random.uniform(-2,2,(1,I_F))\n",
    "c_F = F(Y0_F)\n",
    "Y0_F_tilde = Y0_F\n",
    "c_F_tilde, ac_F, bc_F = scale(c_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training for F(y)\n",
    "mu_F, omega_F, W_F, b_F, J_F, ypsilon_F, itr_F = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_F, \"adam\")\n",
    "\n",
    "#stores the trained parameters in file\n",
    "writeParams(W_F, b_F, omega_F, mu_F,ypsilon_F, J_F, itr_F, filename = 'trainingParams_F.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read the stored trained parameters\n",
    "W_F,b_F,omega_F,mu_F,ypsilon_F, J_F, itr_F = readParams(K, d_F, I_F, N, filename='trainingParams_F.txt')\n",
    "ac_F,bc_F,ac_G,bc_G = readScale('scaleParamsFandG.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotObjFnc(J_F, itr_F, I_F, \"$F(y) = 1-\\cos{y}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with F(y)\n",
    "YTest_F = np.random.uniform(-2,2,(1,I_F))\n",
    "cTest_F = F(YTest_F)\n",
    "\n",
    "\n",
    "ypsilonTest_F = testingAlgorithm(YTest_F, W_F, b_F, omega_F, mu_F, K, d_F, I_F, h, sigma, eta, ac_F,bc_F)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(YTest_F[0], ypsilonTest_F, \"*\", label = \"test\")\n",
    "plt.plot(YTest_F[0], cTest_F[0], \"*\", label = \"fasit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successrate(ypsilonTest_F, cTest_F, (tol*I_F)**0.5, I_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 $G(y_1,y_2) = \\frac{1}{2} (y_1^2 + y_2^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def G(y1, y2):\n",
    "    return 1/2 *(y1**2 + y2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variable used for training with G(y)\n",
    "d_G = 4       \n",
    "I_G = 1500    \n",
    "\n",
    "#training with G(y)\n",
    "Y0_G = np.random.uniform(-2,2,(2,I_G))\n",
    "c_G = G(Y0_G[0],Y0_G[1])\n",
    "c_G.resize(1,I_G)\n",
    "\n",
    "Y0_G_tilde = Y0_G\n",
    "c_G_tilde, ac_G, bc_G = scale(c_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training for G(y)\n",
    "mu_G, omega_G, W_G, b_G, J_G, ypsilon_G, itr_G = trainingAlgorithm(K, d_G, h, tau, Y0_G_tilde, c_G_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_G, \"adam\")\n",
    "writeParams(W_G, b_G, omega_G, mu_G, ypsilon_G, J_G, itr_G, filename = 'trainingParams_G.txt')\n",
    "writeScale(ac_F,bc_F,ac_G,bc_G,'scaleParamsFandG.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_G,b_G,omega_G,mu_G, ypsilon_G, J_G, itr_G = readParams(K, d_G, I_G, N, filename='trainingParams_G.txt')\n",
    "ac_F,bc_F,ac_G,bc_G = readScale('scaleParamsFandG.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotObjFnc(J_G, itr_G, I_G, \"$G(y_1,y_2) = 0.5 (y_1^2 + y_2^2)$\") #frac greia for 1/2 funka ikke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with G(y)\n",
    "YTest_G = np.random.uniform(-2,2,(2,I_G))\n",
    "cTest_G = G(YTest_G[0], YTest_G[1])\n",
    "cTest_G.resize(1,I_G)\n",
    "\n",
    "ypsilonTest_G = testingAlgorithm(YTest_G, W_G, b_G, omega_G, mu_G, K, d_G, I_G, h, sigma, eta, ac_G, bc_G)\n",
    "\n",
    "\n",
    "#plotting of G(y)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(YTest_G[0], YTest_G[1], ypsilonTest_G, label = \"test\", depthshade = True)\n",
    "ax.scatter(YTest_G[0], YTest_G[1], cTest_G[0], label = \"test\" , c=\"red\", depthshade = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successrate(ypsilonTest_G, cTest_G, (tol*I_G)**0.5, I_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 4.3 Known Hamiltonian "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Pendel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = 10E-3\n",
    "l = 0.5\n",
    "g = 9.81\n",
    "\n",
    "def T_pend(p):\n",
    "    return 0.5*p**2\n",
    "\n",
    "def V_pend(q):\n",
    "    return m*g*l*(1-np.cos(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_p = 1500\n",
    "d_p = 2\n",
    "\n",
    "q_p = np.random.uniform(-2*np.pi,2*np.pi,(1,I_p))\n",
    "V_p = V_pend(q_p)\n",
    "V_p_tilde, aVp, bVp = scale(V_p)\n",
    "\n",
    "p_p = np.random.uniform(-10,10,(1,I_p))\n",
    "T_p = T_pend(p_p)\n",
    "T_p_tilde, aTp, bTp = scale(T_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_p, omega_p, W_p, b_p, J_p, ypsilon_p, itr_p = trainingAlgorithm(K, d_p, h, tau, q_p, V_p_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_p, \"adam\")\n",
    "writeParams(W_p, b_p, omega_p, mu_p, ypsilon_p, J_p, itr_p, filename = 'trainingParams_Vpend.txt')\n",
    "mu_pp, omega_pp, W_pp, b_pp, J_pp, ypsilon_pp, itr_pp = trainingAlgorithm(K, d_p, h, tau, p_p, T_p_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_p, \"adam\")\n",
    "writeParams(W_pp, b_pp, omega_pp, mu_pp, ypsilon_pp, J_pp, itr_pp, filename = 'trainingParams_Tpend.txt')\n",
    "writeScale(aVp,bVp,aTp,bTp,'scaleParamsPend.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Tpend,b_Tpend,omega_Tpend,mu_Tpend, ypsilon_Tpend, J_Tpend, itr_Tpend = readParams(K, d_p, I_p, N, filename='trainingParams_Tpend.txt')\n",
    "W_Vpend,b_Vpend,omega_Vpend,mu_Vpend, ypsilon_Vpend, J_Vpend, itr_Vpend = readParams(K, d_p, I_p, N, filename='trainingParams_Vpend.txt')\n",
    "aVp,bVp,aTp,bTp = readScale('scaleParamsPend.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotObjFnc(J_Tpend, itr_Tpend, I_p, \"$T(p) = 0.5p^2$\")\n",
    "plotObjFnc(J_Vpend, itr_Vpend, I_p, \"$V(q) = mgl\\cdot(1-\\cos(q))$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IKKE TESTA ENDA\n",
    "\n",
    "successrate(ypsilonTest_G, cTest_G, (tol*I_G)**0.5, I_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Kepler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def T_Kepler(p):\n",
    "    T_p = np.zeros((1,p.shape[1]))\n",
    "    for i in range(p.shape[1]):\n",
    "        T_p[:,i] = 1/2*np.transpose(p[:,i])@p[:,i]\n",
    "    return T_p\n",
    "\n",
    "def V_Kepler(q1,q2):\n",
    "    return -1/np.sqrt(q1**2+q2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I_K = 1500\n",
    "d_K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_K = np.random.uniform(-10,10, (2,I_K))\n",
    "T_K = T_Kepler(p_K)\n",
    "T_K_tilde, aTK, bTK = scale(T_K)\n",
    "\n",
    "q_K = np.random.uniform(-10,10, (2,I_K))\n",
    "V_K = V_Kepler(q_K[0],q_K[1])\n",
    "V_K.resize(1,I_K)\n",
    "V_K_tilde, aVK, bVK = scale(V_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training T\n",
    "mu_TK, omega_TK, W_TK, b_TK, J_TK, ypsilon_TK, itr_TK = trainingAlgorithm(K, d_K, h, tau, p_K, T_K_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_K, \"adam\")\n",
    "writeParams(W_TK, b_TK, omega_TK, mu_TK, ypsilon_TK, J_TK, itr_TK, filename = 'trainingParams_TKepler.txt')\n",
    "\n",
    "#Training V\n",
    "mu_VK, omega_VK, W_VK, b_VK, J_VK, ypsilon_VK, itr_VK = trainingAlgorithm(K, d_K, h, tau, q_K, V_K_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_K, \"adam\")\n",
    "writeParams(W_VK, b_VK, omega_VK, mu_VK, ypsilon_VK, J_VK, itr_VK, filename = 'trainingParams_VKepler.txt')\n",
    "\n",
    "#saving scale parameters\n",
    "writeScale(aVK,bVK,aTK,bTK,'scaleParamsKepler.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_TK,b_TK,omega_TK,mu_TK, ypsilon_TK, J_TK, itr_TK = readParams(K, d_K, I_K, N, filename='trainingParams_TKepler.txt')\n",
    "W_VK,b_VK,omega_VK,mu_VK, ypsilon_VK, J_VK, itr_VK = readParams(K, d_K, I_K, N, filename='trainingParams_VKepler.txt')\n",
    "aVK,bVK,aTK,bTK = readScale('scaleParamsPend.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting of objective function\n",
    "plotObjFnc(J_TK, itr_TK, I_K, '$T(p) = 0.5 p^T p$')\n",
    "plotObjFnc(J_VK, itr_VK, I_K, '$V(q_1, q_2)= - 1 / (q_1^2 + q_2^2)^{1/2}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Unknown Hamiltonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingBatch = concatenate(0,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I_T = trainingBatch[\"P\"].shape[1]\n",
    "d_T = 5\n",
    "batchsize = 50000\n",
    "\n",
    "\n",
    "pT_tilde = trainingBatch[\"P\"]\n",
    "TT_tilde, aTT, bTT = scale(trainingBatch[\"T\"])\n",
    "TT_tilde.resize(1,TT_tilde.shape[0])\n",
    "\n",
    "\n",
    "qT_tilde = trainingBatch[\"Q\"]\n",
    "VT_tilde, aVT, bVT = scale(trainingBatch[\"V\"])\n",
    "VT_tilde.resize(1,VT_tilde.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_TT, omega_TT, W_TT, b_TT, J_TT, ypsilon_TT, itr_TT = trainingAlgorithm(K, d_T, h, tau, pT_tilde, TT_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "writeParams(W_TT, b_TT, omega_TT, mu_TT, ypsilon_TT, J_TT, itr_TT, \"trainingParams_TUnknown.txt\")\n",
    "\n",
    "mu_VT, omega_VT, W_VT, b_VT, J_VT, ypsilon_VT, itr_VT = trainingAlgorithm(K, d_T, h, tau, qT_tilde, VT_tilde, eta, sigma, eta_div, sigma_div, N, tol, batchsize, \"adam\")\n",
    "writeParams(W_VT, b_VT, omega_VT, mu_VT, ypsilon_VT, J_VT, itr_VT, \"trainingParams_VUnknown.txt\")\n",
    "\n",
    "writeScale(aVT, bVT, aTT, bTT, \"scaleParamsUnknown.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_TT,b_TT,omega_TT,mu_TT, ypsilon_TT, J_TT, itr_TT = readParams(K, d_T, batchsize, N, filename='trainingParams_TUnknown.txt')\n",
    "\n",
    "W_VT,b_VT,omega_VT,mu_VT, ypsilon_VT, J_VT, itr_VT = readParams(K, d_T, batchsize, N, filename='trainingParams_VUnknown.txt')\n",
    "\n",
    "readScale(\"scaleParamsUnknown.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting of objective function\n",
    "plotObjFnc(J_TT, itr_TT, batchsize, \"the unknown Hamiltonians T(p)\")\n",
    "plotObjFnc(J_VT, itr_VT, batchsize, \"the unknown Hamiltonians V(q)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Computing the gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getZ_oneData(y,W,b,K,h,sigma):\n",
    "    d = y.shape[0]\n",
    "    I = y.shape[1]\n",
    "    Z = np.zeros((K,d,I))\n",
    "    Z[0] = y\n",
    "    \n",
    "    for k in range(1,K):\n",
    "        u = W[k-1]@Z[k-1] + b[k-1]\n",
    "        Z[k] = Z[k-1] + h*sigma(u)\n",
    "\n",
    "    return Z\n",
    "\n",
    "def gradF(y, omega, mu, W, b, K, sigma, eta_div, sigma_div):\n",
    "    Z = getZ_oneData(y, W, b, K, h, sigma)\n",
    "    A = eta_div(np.transpose(Z[K-1])@omega + mu)*omega \n",
    "    for k in range(K-1,0,-1): \n",
    "        B = h*sigma_div(W[k]@y+b[k])\n",
    "        u = B*A\n",
    "        A = A + W[k]@u\n",
    "    return A"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sjekk om det er forskjeller mellom getZ_oneData og getZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Numerical methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def symplecticEuler(gradF, t, q0, p0, omega_V, mu_V, W_V, b_V, omega_T, mu_T, W_T, b_T, K, sigma, eta_div, sigma_div): \n",
    "\n",
    "    q = np.zeros((len(t), q0.shape[0], q0.shape[1]))\n",
    "    p = np.zeros((len(t), p0.shape[0], p0.shape[1]))\n",
    "    \n",
    "    q[0] = q0\n",
    "    p[0] = p0\n",
    "    \n",
    "    for n in range(len(t)-1):\n",
    "        delta_t = t[n+1]-t[n] \n",
    "\n",
    "        q[n+1] = q[n] + delta_t*gradF(p[n], omega_T, mu_T, W_T, b_T, K, sigma, eta_div, sigma_div) #dTdp\n",
    "        p[n+1] = p[n] - delta_t*gradF(q[n+1], omega_V, mu_V, W_V, b_V, K, sigma, eta_div, sigma_div) #dVdq\n",
    "    \n",
    "    return q,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Stromer_Verlet(gradF, t, q0, p0, omega_V, mu_V, W_V, b_V, omega_T, mu_T, W_T, b_T, K, sigma, eta_div, sigma_div): \n",
    "    \n",
    "    q = np.zeros((len(t), q0.shape[0], q0.shape[1]))\n",
    "    p = np.zeros((len(t), p0.shape[0], p0.shape[1]))\n",
    "    \n",
    "    q[0] = q0\n",
    "    p[0] = p0\n",
    "    \n",
    "    for n in range(len(t)-1):\n",
    "        delta_t = t[n+1]-t[n]\n",
    "        \n",
    "        u = p[n] - delta_t/2*gradF(q[n], omega_V, mu_V, W_V, b_V, K, sigma, eta_div, sigma_div) #u = p_{n+1/2}, dVdq\n",
    "        q[n+1] = q[n] + delta_t*gradF(u, omega_T, mu_T, W_T, b_T, K, sigma, eta_div, sigma_div) #dTdp\n",
    "        p[n+1] = u - delta_t/2*gradF(q[n+1], omega_V, mu_V, W_V, b_V, K, sigma, eta_div, sigma_div) #dVdq\n",
    "    \n",
    "    return q,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Nonlinear pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I_p = 1000\n",
    "d_p = 2\n",
    "\n",
    "q_p = np.random.uniform(0,2*np.pi,(1,I_p))\n",
    "V_p = V_pend(q_p)\n",
    "V_p_tilde, aVp, bVp = scale(V_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_Tpend,b_Tpend,omega_Tpend,mu_Tpend,ypsilon_Tpend, J_Tpend, itr_Tpend = readParams(K, d_p, I_p, N, filename='trainingParams_Tp.txt')\n",
    "W_Vpend,b_Vpend,omega_Vpend,mu_Vpend,ypsilon_Vpend, J_Vpend, itr_Vpend = readParams(K, d_p, I_p, N, filename='trainingParams_Vp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q0 = np.zeros((d_p,1))\n",
    "p0 = np.zeros((d_p,1))\n",
    "t = np.linspace(0,10,100)\n",
    "\n",
    "q_sympEuler, p_sympEuler = symplecticEuler(gradF, t, q0, p0, omega_Vpend, mu_Vpend, W_Vpend, b_Vpend, omega_Tpend, mu_Tpend, W_Tpend, b_Tpend, K, sigma, eta_div, sigma_div)\n",
    "\n",
    "q_StromerVerlet, p_StromerVerlet = Stromer_Verlet(gradF, t, q0, p0, omega_Vpend, mu_Vpend, W_Vpend, b_Vpend, omega_Tpend, mu_Tpend, W_Tpend, b_Tpend, K, sigma, eta_div, sigma_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_plot = np.append(np.flip(q_sympEuler[:,0,:]),q_sympEuler[:,1,:])\n",
    "\n",
    "q = np.linspace(-2,2,100)\n",
    "\n",
    "plt.plot(q, V_pend(q), label = \"fasit\")\n",
    "plt.plot(q_plot, V_pend(q_plot), \"--\", label = \"Symplectic Euler\")\n",
    "\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_plot = np.append(np.flip(q_StromerVerlet[:,0,:]),q_StromerVerlet[:,1,:])\n",
    "\n",
    "q = np.linspace(-2,2,100)\n",
    "\n",
    "plt.plot(q, V_pend(q), label = \"fasit\")\n",
    "plt.plot(q_plot, V_pend(q_plot), \"--\", label = \"Stromer Verlet\")\n",
    "\n",
    "\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
