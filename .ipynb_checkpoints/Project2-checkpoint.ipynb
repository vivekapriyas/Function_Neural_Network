{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "    <center>\n",
    "    TMA4215 Project 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "#### 1. Introduction\n",
    "#### 2. Algorithm \n",
    "#### 3. Testing on suggested functions\n",
    "3.1 $F(y) = 1-\\cos(y)$\n",
    "\n",
    "3.2 $G(y) = \\frac{1}{2}(y_1^2 + y_2^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to use a neural network to train approximations of Hamiltonian function, derive and implement formulas for computing the gradient of the trained function and use those to implement symplectic Euler and the StÃ¸rmer-Verlet method for the Hamiltonian function. \n",
    "\n",
    "This report present the algorithm, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "collapsed": true
   },

   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import isclose\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "collapsed": true
   },

   "outputs": [],
   "source": [
    "from project_2_data_acquisition import generate_data, concatenate\n",
    "from files import writeParams, readParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above are imported from separate files and are used to generate input data batches from comma seperated files and to write our trained values to a file as well as reading these values from a file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is divided into several smaller codeblocks with the purpose of making the code easy to follow. Principally, the algorithm's parts is to transform input data between the layers in the network, decide the gradients of the objective function, $J = \\frac{1}{2} \\|Z-c\\| $, with respect to weights, biases, $\\omega$ and $\\mu$ and optimalize the network with respect to those. Additionally, several utility functions is defined to be used in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "collapsed": true
   },

   "outputs": [],
   "source": [
    "def getW(K,d):\n",
    "    w = np.random.randn(K,d,d)\n",
    "    return w\n",
    "\n",
    "def getb(K,d):\n",
    "    b = np.random.randn(K,d,1)\n",
    "    return b\n",
    "\n",
    "def getomega(d):\n",
    "    omega = np.random.randn(d,1)\n",
    "    return omega\n",
    "\n",
    "def getmu():\n",
    "    mu = np.random.randn(1)\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above generate random initial values drawn from a standard normal distribution to the weights, $W_k$, the biases, $b_k$, $\\omega$ and $\\mu$. These random values causes some marginal differences between runs of the same dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "collapsed": true
   },

   "outputs": [],
   "source": [
    "def getZ(Y0, W, b, K, d, I, h, sigma):\n",
    "    #initialize Z, Z0=Y0\n",
    "    Z = np.zeros((K,d,I))\n",
    "    Z[0] = Y0\n",
    "\n",
    "    #finds Zk\n",
    "    for k in range(1,K):\n",
    "        u = W[k-1]@Z[k-1] + b[k-1]\n",
    "        Z[k] = Z[k-1] + h*sigma(u)\n",
    "\n",
    "    return Z\n",
    "\n",
    "def getP(Z, ypsilon, c, omega, mu, W, b, K, d, I, h, sigma_div, eta_div):\n",
    "    #initialize P\n",
    "    P = np.zeros((K+1,d,I))\n",
    "\n",
    "    #finds P_K\n",
    "    u = np.transpose(Z[-1])@omega + mu\n",
    "    P[-1]= omega@np.transpose((ypsilon-c)*eta_div(u))\n",
    "\n",
    "    #finds P_K-1 to P_0\n",
    "    for k in range(K-1,0,-1):\n",
    "        s = W[k]@Z[k] + b[k]\n",
    "        P[k]=P[k+1] + h*np.transpose(W[k])@(sigma_div(s)*P[k+1])\n",
    "        \n",
    "    return P\n",
    "\n",
    "\n",
    "def getYpsilon(Z, omega, mu, K, eta):\n",
    "    u = np.transpose(Z[K-1]) @ omega + mu\n",
    "    return eta(u)\n",
    "\n",
    "def getJ(ypsilon, c):\n",
    "    return 1/2 * np.linalg.norm(ypsilon-c)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Defines the functions getZ and getP, where getZ executes linear transformations $\\Phi_k: Z_k \\rightarrow Z_{k+1}$ based on the weights and biases and returns the matrix $Z$ and getP returns an utility matrix used in calulations of derivatives of the objective function with respect to the weight and biases. Furthermore, getYpsilon is defined and returns a vector of the function values in the last layer in the network and the objective funksjon $J$ is defined as a measure of the difference between the resulting trained values and the exact function values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "collapsed": true
   },

   "outputs": [],
   "source": [
    "def getdJdmu(Z, ypsilon, c, omega, mu, K, eta_div):\n",
    "    u = np.transpose(Z[K-1])@omega + mu\n",
    "    dJdmu = np.transpose(eta_div(u))@(ypsilon-c)\n",
    "    return dJdmu\n",
    "\n",
    "def getdJdomega(Z, ypsilon, c, omega, mu, K, eta_div):\n",
    "    u = np.transpose(Z[K-1])@omega + mu\n",
    "    dJdOmega = Z[K-1] @ ((ypsilon - c) * eta_div(u))\n",
    "    return dJdOmega\n",
    "\n",
    "def getdJdW(P, Z, W, b, K, h, sigma_div):\n",
    "    dJdW = np.zeros_like(W)\n",
    "    for k in range(K):\n",
    "        u = W[k]@Z[k]+b[k]\n",
    "        dJdW[k] = h*(P[k+1]*sigma_div(u))@ np.transpose(Z[k])  \n",
    "    return dJdW\n",
    "\n",
    "def getdJdb(P, Z, W, b, K, h, sigma_div):\n",
    "    dJdb = np.zeros_like(b)\n",
    "    for k in range(K):\n",
    "        u = W[k]@Z[k]+b[k]\n",
    "        dJdb[k] = h*(P[k+1]*sigma_div(u))@np.ones((Z.shape[2],1))\n",
    "    return dJdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Functions to obtain the derivatives of the objective function with respect to weights and biased is defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "collapsed": true
   },

   "outputs": [],
   "source": [
    "def getMandV(theta):\n",
    "    return np.zeros_like(theta),np.zeros_like(theta)\n",
    "\n",
    "def adam(alpha,m,v,g,i):\n",
    "    #parameters\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 10E-8\n",
    "\n",
    "    #m,v\n",
    "    m = beta1*m + (1-beta1)*g\n",
    "    v = beta2*v + (1-beta2)*np.multiply(g,g)\n",
    "\n",
    "    m_hat = np.multiply(m,1/(1-beta1**i))\n",
    "    v_hat = np.multiply(v,1/(1-beta2**i))\n",
    "\n",
    "    #update\n",
    "    R = alpha*np.multiply(m_hat,1/(np.sqrt(v_hat)+epsilon))\n",
    "\n",
    "    return m,v,R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above is used to optimize the weights, biases, $\\omega$ and $\\mu$ in different ways, which will be compared later on in this report.\n",
    "\n",
    "In plain vanilla gradient descent, one follows the gradient in decreasing direction and the learning parameter $\\tau$ determine how far one shall follow it. \n",
    "\n",
    "In Adam descent method, one follows the gradient with different length depending on different parameters. The parameters $m$ and $v$ represent respectively a kind of mean value and squared mean value of the previous iterations gradients. The function getMandV initialize $m$ and $v$ to be either zer, the zerovector or the zeromatrix depending on inputdata's dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "collapsed": true
   },

   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigma_div(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def eta(x):\n",
    "    return 1/2*(1+np.tanh(x/2))\n",
    "\n",
    "def eta_div(x):\n",
    "    return 1/(4*np.cosh(x/2)**2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the activating function $\\sigma$, which is used in the transformation $\\Phi_k: Z_k \\rightarrow Z_{k+1}$, and the function $\\eta$, used in projection from last layer on a scalar $z$, is defined. The derivatives of $\\sigma$ and $\\eta$ is also defined and will be used in calculations of the gradient of the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "collapsed": true
   },

   "outputs": [],
   "source": [
    "def scale(x, alpha=0.2, beta=0.8):\n",
    "    a = np.amin(x)\n",
    "    b = np.amax(x)\n",
    "    \n",
    "    x_tilde = 1/(b-a)*((b-x)*alpha + (x-a)*beta)\n",
    "    return x_tilde, a, b\n",
    "\n",
    "def inverseScale(x_tilde, a, b, alpha=0.2, beta=0.8):\n",
    "    return 1/(beta-alpha)*((x_tilde-alpha)*b + (beta-x_tilde)*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to scale data in terms of a min-max transformation that guaranteed that the data have all its components in the interval $[\\alpha, \\beta]$ is defined. This is desirable since some functions is acting component-wise on other functions....?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "collapsed": true
   },

   "outputs": [],
   "source": [
    "def successrate(ypsilon, c, tol):\n",
    "    sum = 0\n",
    "    for i in range(ypsilon.shape[0]):\n",
    "        if isclose(ypsilon[i][0], c[0][i], abs_tol = tol):\n",
    "            sum +=1\n",
    "    return sum/ypsilon.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si noe fornuftig her etterhver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingAlgorithm(K, d, h, tau, Y0, c0, eta, sigma, eta_div, sigma_div, N, tol, chunksize, optimization):\n",
    "    #finds input shape\n",
    "    d0 = Y0.shape[0]\n",
    "    I0 = Y0.shape[1]\n",
    "    \n",
    "    #reshapes input to match dimension of layers\n",
    "    if d0 > d:\n",
    "        return \"d must be larger than d0\"\n",
    "    \n",
    "    if d0 < d:\n",
    "        zero = np.zeros((d-d0,I0))\n",
    "        Y0 = np.vstack((Y0,zero))\n",
    "    \n",
    "    #gets initial weigths\n",
    "    omega = getomega(d)\n",
    "    mu = getmu()\n",
    "    W = getW(K,d)\n",
    "    b = getb(K,d)\n",
    "    \n",
    "    c0 = np.transpose(c0)\n",
    "  \n",
    "    if optimization == \"adam\":\n",
    "        #initial m,v for adam descent\n",
    "        mmu,vmu = getMandV(mu)\n",
    "        momega,vomega =getMandV(omega)\n",
    "        mW,vW = getMandV(W)\n",
    "        mb,vb = getMandV(b)\n",
    "    \n",
    "    #initializes vector to store objective function values\n",
    "    J = np.zeros(N)\n",
    "    \n",
    "    \n",
    "    for i in range(N):\n",
    "        if (J[i-1]/chunksize)< tol:\n",
    "            break\n",
    "               \n",
    "        #stochastic gradient descent\n",
    "        if I0 == chunksize:\n",
    "            Y0_chunk = Y0\n",
    "            c_chunk = c0\n",
    "        elif I0 > chunksize:\n",
    "            s = np.random.randint(0,I0-chunksize)\n",
    "            Y0_chunk = Y0[:,s:(s+chunksize)]\n",
    "            c_chunk=c0[s:(s+chunksize),:]\n",
    "        else:\n",
    "            return \"chunksize must be smaller than I\"        \n",
    "        \n",
    "\n",
    "        #transformations between layers\n",
    "        Z = getZ(Y0_chunk, W, b, K, d, chunksize, h, sigma)\n",
    "        ypsilon = getYpsilon(Z, omega, mu, K, eta)\n",
    "        P = getP(Z, ypsilon, c_chunk, omega, mu, W, b, K, d, chunksize, h, sigma_div, eta_div)\n",
    "        \n",
    "        #objective function\n",
    "        J[i] = getJ(ypsilon, c_chunk)\n",
    "        \n",
    "        #finds gradients\n",
    "        dJdmu = getdJdmu(Z, ypsilon, c_chunk, omega, mu, K, eta_div)\n",
    "        dJdomega = getdJdomega(Z, ypsilon, c_chunk, omega, mu, K, eta_div)\n",
    "        dJdW = getdJdW(P, Z, W, b, K, h, sigma_div)\n",
    "        dJdb = getdJdb(P, Z, W, b, K, h, sigma_div)\n",
    "        \n",
    "        if optimization == \"plain\":\n",
    "            mu = mu - tau*dJdmu\n",
    "            omega = omega - tau*dJdomega\n",
    "            W = W - tau*dJdW\n",
    "            b = b - tau*dJdb\n",
    "        \n",
    "        elif optimization == \"adam\":\n",
    "            mmu,vmu, Rmu = adam(tau,mmu,vmu,dJdmu,i+1)\n",
    "            momega,vomega, Romega = adam(tau,momega,vomega,dJdomega,i+1)\n",
    "            mW,vW, RW = adam(tau,mW,vW,dJdW,i+1)\n",
    "            mb,vb, Rb = adam(tau,mb,vb,dJdb,i+1)\n",
    "\n",
    "            #updates weights and biases\n",
    "            mu = mu-Rmu\n",
    "            omega = omega - Romega\n",
    "            W = W - RW\n",
    "            b = b - Rb\n",
    "        \n",
    "    return mu, omega, W, b, J, ypsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "collapsed": true
   },

   "outputs": [],
   "source": [
    "def testing(yTest, cTest, W, b, omega, mu, K, d, I, h, sigma, eta):\n",
    "\n",
    "    #scaling of input\n",
    "    yTilde, aY, bY = scale(yTest)\n",
    "    cTilde, ac, bc = scale(cTest)\n",
    "\n",
    "    #finds input shape\n",
    "    d0 = yTest.shape[0]\n",
    "    \n",
    "    if d > d0:\n",
    "        zero = np.zeros((d-d0,I))\n",
    "        yTilde = np.vstack((yTilde,zero))\n",
    "    \n",
    "    zTest = getZ(yTilde, W, b, K, d, I, h, sigma)\n",
    "    ypsilonTilde = getYpsilon(zTest, omega, mu, K, eta)\n",
    "\n",
    "    #rescaling of input\n",
    "    ypsilonTest = inverseScale(ypsilonTilde, ac, bc)\n",
    "\n",
    "    return ypsilonTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training algorithm returns trained values of the weights, biases, $\\omega$ and $\\mu$ as well as the objective function and $\\Upsilon$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Testing with suggested functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "collapsed": true
   },

   "outputs": [],
   "source": [
    "#defines global variables\n",
    "h = 0.1       #stepsize\n",
    "tau = 0.001   #learning parameter\n",
    "tol = 0.5e-4  #tolerance\n",
    "N = 5000      #number of training series\n",
    "K = 50        #number of hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1  F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "collapsed": true
   },

   "outputs": [],
   "source": [
    "def F(y):\n",
    "    return 1-np.cos(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "collapsed": true
   },

   "outputs": [],
   "source": [
    "d_F = 4        #dimension of layers for F(y)\n",
    "I_F = 1000     #input size \n",
    "\n",
    "#training with F(y)\n",
    "Y0_F = np.random.uniform(-2,2,(1,I_F))\n",
    "c_F = F(Y0_F)\n",
    "\n",
    "Y0_F_tilde, aY0_F, bY0_F = scale(Y0_F)\n",
    "c_F_tilde, ac_F, bc_F = scale(c_F)\n",
    "\n",
    "mu_F, omega_F, W_F, b_F, J_F, ypsilon_F = trainingAlgorithm(K, d_F, h, tau, Y0_F_tilde, c_F_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_F, \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeParams(W_F, b_F, omega_F, mu_F,ypsilon_F, J_F, filename = 'trainingParams_F.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Fr,b_Fr,omega_Fr,mu_Fr,ypsilon_Fr, J_F = readParams(K, d_F, I_F, filename='trainingParams_F.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,N,N), J_F)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with F(y)\n",
    "YTest_F = np.random.uniform(-2,2,(1,I_F))\n",
    "cTest_F = F(YTest_F)\n",
    "\n",
    "\n",
    "ypsilonTest_F = testing(YTest_F, cTest_F, W_F, b_F, omega_F, mu_F, K, d_F, I_F, h, sigma, eta)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(YTest_F[0], ypsilonTest_F, \"*\", label = \"test\")\n",
    "plt.plot(YTest_F[0], cTest_F[0], \"*\", label = \"fasit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "collapsed": true
   },

   "outputs": [],
   "source": [
    "def G(y1, y2):\n",
    "    return 1/2 *(y1**2 + y2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "collapsed": true
   },

   "outputs": [],
   "source": [
    "d_G = 4        #dimension of layers for F(y)\n",
    "I_G = 1000     #input size \n",
    "\n",
    "#training with G(y)\n",
    "Y0_G = np.random.uniform(-2,2,(2,I_G))\n",
    "c_G = G(Y0_G[0],Y0_G[1])\n",
    "c_G.resize(1,I_G)\n",
    "\n",
    "Y0_G_tilde, aY0_G, bY0_G = scale(Y0_G)\n",
    "c_G_tilde, ac_G, bc_G = scale(c_G)\n",
    "\n",
    "mu_G, omega_G, W_G, b_G, J_G, ypsilon_G = trainingAlgorithm(K, d_G, h, tau, Y0_G_tilde, c_G_tilde, eta, sigma, eta_div, sigma_div, N, tol, I_G, \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeParams(W_G, b_G, omega_G, mu_G, ypsilon_G, J_G, filename = 'trainingParams_G.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Gr,b_Gr,omega_Gr,mu_Gr, ypsilon_Gr, J_G = readParams(K, d_G, I_G, filename='trainingParams_G.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting of objective function\n",
    "plt.plot(np.linspace(0,N,N), J_G)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with G(y)\n",
    "YTest_G = np.random.uniform(-2,2,(2,I_G))\n",
    "cTest_G = G(YTest_G[0], YTest_G[1])\n",
    "cTest_G.resize(1,I_G)\n",
    "\n",
    "ypsilonTest_G = testing(YTest_G, cTest_G, W_G, b_G, omega_G, mu_G, K, d_G, I_G, h, sigma, eta)\n",
    "\n",
    "\n",
    "#plotting of G(y)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(YTest_G[0], YTest_G[1], ypsilonTest_G, label = \"test\", depthshade = False)\n",
    "ax.scatter(YTest_G[0], YTest_G[1], cTest_G[0], label = \"test\" , c=\"red\", depthshade = False)\n",
    "plt.show()\n",
    "\n",
    "x = np.arange(0, I_G)\n",
    "plt.plot(x, ypsilonTest_G, label =\"test\")\n",
    "plt.plot(x, cTest_G[0], label = \"fasit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4. Training with data given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "collapsed": true
   },

   "outputs": [],
   "source": [
    "d_0 = 3\n",
    "N_0 = 3000\n",
    "K_0= 50\n",
    "\n",
    "\n",
    "batch0 = generate_data()\n",
    "\n",
    "p0_tilde, ap0,bp0 = scale(batch0['P'])\n",
    "T0_tilde, aT0, bT0 = scale(batch0['T'])\n",
    "T0_tilde.resize(1,T0_tilde.shape[0])\n",
    "\n",
    "I_0 = p0_tilde.shape[1]\n",
    "\n",
    "mu_0, omega_0, W_0, b_0, J_0, ypsilon_0, J_0 = trainingAlgorithm(K_0, d_0, h, tau, p0_tilde, T0_tilde, eta, sigma, eta_div, sigma_div, N_0,500, \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeParams(W_0, b_0, omega_0, mu_0, ypsilon_0, filename = 'trainingParams_0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_0r,b_0r,omega_0r,mu_0r,ypsilon_0r = readParams(K_0, d_0, I_0, filename='trainingParams_0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting of objective function\n",
    "plt.plot(np.linspace(0,N_0,N_0), J_0)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "collapsed": true
   },

   "outputs": [],
   "source": [
    "x = np.arange(0, I_0)\n",
    "ypsilon0_scaled = inverseScale(ypsilon_0, aT0, bT0)\n",
    "T0 = batch0['T']\n",
    "\n",
    "print(successrate(ypsilon0_scaled, T0, 0.0035))\n",
    "\n",
    "plt.plot(x, ypsilon0_scaled, label =\"test\")\n",
    "plt.plot(x, T0[0], label = \"fasit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "collapsed": true
   },

   "outputs": [],
   "source": [
    "#testing with batch 1\n",
    "\n",
    "batch1 = generate_data(1)\n",
    "\n",
    "p1 = batch1['P']\n",
    "T1 =batch1['T']\n",
    "T1.resize(1,T1.shape[0])\n",
    "\n",
    "\n",
    "ypsilonTest_1 = testing(p1, T1, W_0, b_0, omega_0, mu_0, K_0, d_0, I_0, h, sigma, eta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, I_0)\n",
    "plt.plot(x, ypsilonTest_1, label =\"test\")\n",
    "plt.plot(x, T1[0], label = \"fasit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",

   "version": "3.6.1"

  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
